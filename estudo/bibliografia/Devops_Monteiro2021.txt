SOLUÇÕES

EDUCACIONAIS

INTEGRADAS 



Revisão técnica: 

Fernando Sérgio Soares Fagonde 
Especialista em Gerência e Segurança de Redes 
Bacharel em Sistemas de Informação 

Cristiani de Oliveira Dias 
Doutora em Informática na Educação 
Mestra em Educação 

D511 | DevOps [recurso eletrônico] / Eduarda Rodrigues Monteiro 
[fetal]; revisão técnica: Fernando Sérgio Soares 

Fagonde e Cristiani de Oliveira Dias — Porto Alegre 

SAGAH, 2021. 
ISBN 978-65-5690-172-5 
1, Ciência da computação — Tecnologia da informação. 

| Monteiro, Eduarda Rodrigues. 

CDU 004.01/9 

Catalogação na publicação: Mônica Ballejo Canto — CRB 10/1023 



DEVOPS 

Eduarda Rodrigues Monteiro 
Doutora em Ciência da Computação 
Mestra em Ciência da Computação 
Marcos Vinicius Bião Cerqueira 
Mestre em Computação Aplicada 

Engenheiro da Computação 

Matheus da Silva Serpa 
Mestre em Computação 
Nicolli Souza Rios Alves 
Doutora em Ciência da Computação 
Mestra em Sistemas e Computação 
Priscila Gonçalves 
Mestra em Administração 
Bacharela em Sistemas de Informação 

Rafael Queiroz Gonçalves 
Doutor em Ciência da Computação 

Lucas Gonçalves Correia 
Especialista em Segurança 
da Informação 
Graduado em Redes 
de Computadores 

Marcel Santos Silva 
Mestre em Ciência da Informação 
Bacharel em Análise de Sistemas 

Maristela Regina Weinfurter 
Teixeira 
Mestra em Ciência da Computação 
Especialista em Informática 

Renê de Ávila Mendes 
Mestre em Engenharia Elétrica 
Graduado em Sistemas 
de Informação 

Porto Alegre, 
2021 

sagah' 



O Grupo A Educação S.A,, 2021 

Gerente editorial: Arysinha Affonso 

Colaboraram nesta edição: 
Editora: Fernanda Anflor 
Preparação de originais: Marquieli Oliveira 
Editoração: Ledur Serviços Editoriais Ltda. 

Os links para sites da web fornecidos neste livro foram todos testados, e seu funciona-
mento foi comprovado no momento da publicação do material. No entanto, a rede 
é extremamente dinâmica; suas páginas estão constantemente mudando de local 
e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade sobre 
qualidade, precisão ou integralidade das informações referidas em tais links. 

Reservados todos os direitos de publicação ao GRUPO A EDUCAÇÃO S.A. 
(Sagah é um selo editorial do GRUPO A EDUCAÇÃO S.A.) 

Rua Ernesto Alves, 150 — Bairro Floresta 
90220-190 Porto Alegre RS 
Fone: (51) 3027-7000 

SAC 0800 703-3444 -www.grupoa.com.br 

É proibida a duplicação ou reprodução deste volume, no todo ou em parte, 
sob quaisquer formas ou por quaisquer meios (eletrônico, mecânico, gravação, 
fotocópia, distribuição na web e outros), sem permissão expressa da Editora. 



APRESENTAÇÃO 

Arecente evolução das tecnologias digitais e a consolidação da internet 
modificaram tanto as relações na sociedade quanto as noções de espaço 
e tempo. Se antes levávamos dias ou até semanas para saber de aconte-
cimentos e eventos distantes, hoje temos a informação de maneira quase 
instantânea. Essa realidade possibilita a ampliação do conhecimento. No 
entanto, é necessário pensar cada vez mais em formas de aproximar os 
estudantes de conteúdos relevantes e de qualidade. Assim, para atender 
às necessidades tanto dos alunos de graduação quanto das instituições 
de ensino, desenvolvemos livros que buscam essa aproximação por meio 
de uma linguagem dialógica e de uma abordagem didática e funcional, 
e que apresentam os principais conceitos dos temas propostos em cada 
capítulo de maneira simples e concisa. 

Nestes livros, foram desenvolvidas seções de discussão para reflexão, 
de maneira a complementar o aprendizado do aluno, além de exemplos 
e dicas que facilitam o entendimento sobre o tema a ser estudado. 

Ao iniciar um capítulo, você, leitor, será apresentado aos objetivos 
de aprendizagem e às habilidades a serem desenvolvidas no capítulo, 

seguidos da introdução e dos conceitos básicos para que você possa 
dar continuidade à leitura. 
Ao longo do livro, você vai encontrar hipertextos que lhe auxiliarão 
no processo de compreensão do tema. Esses hipertextos estão clas-
sificados como: 

Saiba mais 

Traz dicas e informações extras sobre o assunto tratado na seção. 


Fique atento 

Alerta sobre alguma informação não explicitada no texto ou acrescenta dados sobre 
determinado assunto. 

» 

Exemplo 

Mostra um exemplo sobre o tema estudado, para que você possa compreendê-lo 
de maneira mais eficaz. 

, 

Todas essas facilidades vão contribuir para um ambiente de apren-
dizagem dinâmico e produtivo, conectando alunos e professores no 
processo do conhecimento. 

Bons estudos! 



PREFÁCIO 

À medida que aumenta a utilização de soluções de software para 
os mais variados problemas, aumenta também a complexidade das 
tecnologias que envolvem as camadas necessárias para que um sis-
tema esteja disponível para o usuário. Uma das etapas mais críticas 
durante a execução de um projeto de software é a entrega: quanto 
mais integrações e ferramentas são utilizadas, maiores são a comple-
xidade e os riscos. 

A etapa de entrega era geralmente executada pelo setor de ope-
rações de tecnologia da informação, o chamado sysadmin, ou pela 
própria equipe de desenvolvimento, em um processo demorado e 
criterioso. O ritmo imposto pelo incremento das demandas fez com 
que essa manutenção humana se tornasse um gargalo. Para resolver 
esse problema, uma série de técnicas, ferramentas e metodologias 
foram criadas e implementadas para aproximar as duas áreas: desen-
volvimento e operações. 

Surgiu aí o nome DevOps, que busca a otimização da comunicação 
entre as duas áreas, melhorando a colaboração e a integração, criando 
soluções para monitoramento, teste, alteração e recuperação dos 
artefatos de software. Essa aproximação ajuda a garantir a integridade, 
qualidade e segurança dos sistemas. 

Neste livro, serão abordadas técnicas e ferramentas utilizadas em 
DevOps, suas aplicações, vantagens e configurações, e outros assuntos 
essenciais para estudantes e profissionais que desejam trabalhar com 
essa cultura da engenharia de software. 

Fernando Sérgio Soares Fagonde 



Esta página foi deixada em branco intencionalmente. 



SUMÁRIO 

Introdução ao DevOps 

Marcos Vinicius Bião Cerqueira 
A cultura DevOps 
Conceitos básicos. 
Integração e entrega contínuas. 

Tipos de controles de versão e verbetes... 

Marcel Santos Silva 
Controle de versão ... 
Operações em controle de versõe: 
Versionamentos distribuído e centralizado. 

Criação de branches e commits com o Git... 

Lucas Gonçalves Correia 
Conhecendo a ferramenta Gt ... 
Estrutura de versionamento do Gi 
Utilizando o Git com os arquivos e repositórios... 

Integração da equipe de Devs com o Git.. 

René de Ávila Mendes 
Gitflow: organizando o verstonamento de código-fonte 
Trabalhando em equipe: verstonando o código-fonte 
Trabalhando em equipe: resolvendo conflitos. 

Configuração e manutenção de contêineres 
usando o Docker.. 
Eduarda Rodrigues Monteiro 

Introdução ao Docker.. 
Incorporando o Docker. 

Dockerfile... 

Uso do Docker para microsserviços ..... 

Rafael Queiroz Gonçalves 
Os microsserviços. 
Criando microsserviços com o apoio da ferramenta Docker. 
Gerenciando o ciclo de vida de contêineres... 


10 || Sumário 

Prática de microsserviços 

Rafael Queiroz Gonçalves 
Aplicações de microsserviços, : a 
Microsserviços e arquitetura monolítica. principais diferenças. 
Benefícios da utilização de microsserviços. 

Ferramenta de orquestração de contêineres.. 

Nicolli Souza Rios Alves 
Orquestração de contêineres 
Kubernetes ... 
Gerenciamento de objetos Kubernetes .. 

Cluster Kubernetes 
Eduarda Rodrigues Monteiro 
Introdução ao Kubernetes 

Manipulação, gerenciamento e escalonament 

Aplicações stateless e stateful.. 

Fundamentos do Jenkins ............smmeesenesssmees 
Nicolli Souza Rios Alves 
Jenkins mm 

Pipeline para entrega contínu: 

Plugins 

Pipeline de integração e entrega contínuas. 

Marcos Vinicius Bião Cerqueira 

Criação de um projeto. 
Gráficos e logs 
Trigger build. 

Identificação e planejamento da configuração .. 
Priscila Gonçalves 

A importância do gerenciamentode configuração. 

Itens de configuração 
Etapas de identificação e planejamento da configuração ... 

Gerenciamento de entregas... 
Priscila Gonçalves 
Integração, entrega e implantação contínuas. 

Blue-green deployment. 

Provisionamento em ambientes distribuídos 

203 
204 
mm 206 

2 


Sumário || 1 


Ferramentas de gerenciamento de configuração 
de software . 
Maristela Regina Weinfurter Teixeira 

Função... 
Ferramentas, cenários, planejamento e funcionalidade: 
Estudo de caso... 

Comunicação e colaboração na cultura DevOps. 

Matheusda Silva Serpa 
Mantendo a equipe alinhada durante o process: 236 
Importância da cultura DevOps 239 
Comunicação como pilar estratégico ....... 243 

Cloud computing e DevOps..... 249 
Matheus da Silva Serpa 
Cloud computing. 249 

Soluções DevOps na nuvem... 254 
Comparando soluções DevOps na nuvem 259 


Esta página foi deixada em branco intencionalmente. 



Introdução ao DevOps 

-Objetivos de aprendizagem 
Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Descrever os principais aspectos da cultura de DevOps. 

E Definir os conceitos básicos relativos à infraestrutura ágil. 

E Demonstrar os efeitos da aplicação de integração e de entrega 

contínuas. 

Introdução 

A ideia central do DevOps está na organização, simplificação e automa-

ção de processos, abrangendo desde as etapas de desenvolvimento 

de software, passando pelo ciclo de subida do código ao servidor até 

o estágio de configuração do servidor. Essa é uma prática que ainda é 
pouco utilizada pelas empresas, porém ela possui a capacidade de elevar a 
qualidade do sistema produzido e alcançar um maior controle do código. 
Neste capítulo, você conhecerá o DevOps, bem como os diversos 
conceitos que o cercam e as metodologias que contribuem para a sua 
continuidade. 

1 A cultura DevOps 

Antigamente, grandes empresas possuíam equipes especializadas para cada 
uma das etapas do desenvolvimento de um software. A equipe de desenvolvedo-
res era responsável pelo planejamento arquitetural, o cronograma das atividades 
ea codificação do projeto. O grupo de testes, por sua vez, era incumbido de 
analisar o software em busca de erros. Por fim, o setor de infraestrutura ficava 
encarregado dos servidores, tanto dos updates das atualizações quanto da 
configuração dos servidores em si. 



14) | Introdução ao DevOps 
Cada uma dessas equipes conseguia trabalhar de forma fluida, e os seus 
membros contribuíam uns com os outros, a fim de elucidar os problemas 
que surgiam, aprender novas tecnologias e pensar em formas inovadoras de 
realizar o seu trabalho. Obviamente, há organizações em que os profissionais 
da mesma equipe não se auxiliam, porém será considerada uma situação em 
que isso não ocorre. O problema maior está na comunicação entre as equipes, 
visto que, em parte, os colaboradores visam a facilitar e reduzir o seu próprio 
trabalho, o que acaba tornando o serviço do outro mais complexo. 

Por exemplo, imagine que a equipe de desenvolvedores realiza entregas 
semanais, sendo que cada programador lida com um arquivo diferente do 
projeto, a fim de evitar problemas de versões. Foi gerado um documento com 
todos os requisitos, entretanto, não foram documentadas todas as funções que 
surgiram durante o andamento do projeto. Assim, o grupo de testes recebe o 
projeto no pen drive, sem saber com exatidão as alterações que foram realizadas 
na semana, podendo, assim, deixar trechos importantes sem serem testados. 
Por fim, o sistema é colocado no servidor de produção, pois não apresentava 
mais erros. A quantidade de problemas que podem surgir nessa situação é 
alarmante, os quais, muitas vezes, acabam gerando retrabalho para corrigir 
falhas que poderiam ter sido evitadas. 

ique atento 

A periodicidade de entregas varia de acordo com a metodologia de desenvolvimento 
utilizada na empresa. Contudo, independentemente da frequência, os mesmos pro-
blemas podem vir a ocorrer. 

O termo DevOps surgiu com a junção das áreas de desenvolvimento e 
operação (FARROCHA, 2014). Contudo, isso não significa que o conhecimento 
fica restrito a esses dois campos, uma vez que diversas áreas acabam sendo 
correlacionadas com as práticas dessa metodologia. 



Introdução ao DevOps 15 

Há muito tempo se ouve falar sobre desenvolvimento ágil e, em 2001, houve 
uma elaboração de um documento por parte de diversos especialistas da área, 
chamado de Manifesto Ágil, que contém os princípios que fundamentam o 
desenvolvimento de software (MATTIOLI et al., 2009). O evento Agile 2008 
abriu caminhos para o DevOps, com discussões e debates acerca de infraes-
trutura ágil. Entretanto, o termo DevOps só surgiu em 2009, com o objetivo 
de unir administradores de infraestrutura e programadores, promovendo 
automação nos processos de integração e entrega. 

Fique atento 

Ao contrário da metodologia ágil, o DevOps não possui um documento similar ao 
Manifesto Ágil. Dessa forma, mesmo sendo conhecido mundialmente, cada empresa 
adapta o conceito de acordo com as suas necessidades. 

A ideia central por trás do DevOps é ter processos bem definidos, auto-
matizar ações e possibilitar cooperação mútua. Dessa forma, os profissionais 
ficam livres para focar as suas atividades em soluções inteligentes (MUELLER, 
2016). Coma liderança e o uso dos software corretos, é possível viabilizar um 
ambiente onde desenvolvedores e administradores de infraestrutura trabalhem 
em harmonia. A eficiência da cultura DevOps pode ser percebida quando a 
organização compreende o seu funcionamento. 

A Figura 1, a seguir, apresenta as áreas que se relacionam, formando o 
DevOps. Logicamente, cada uma dessas áreas sofre influências de outras 
disciplinas, o que enriquece ainda mais as práticas utilizadas. 



16 || Introduçãoao DevOps 

Desenvolvimento do 
projeto Garantia da 
qualidade 

Operações 

Figura 1. Áreas correlacionadas que formam o DevOps. 

Fonte: Adaptada de Traversin (2017) 
Na J 

Cada uma dessas áreas possui suas próprias metodologias, problemas e 
processos, de modo que, independentemente do setor, os profissionais devem 
estar aptos a resolver as problemáticas que surgem no cotidiano. Vale lembrar 
que a intenção do DevOps é criar formas de interação entre as equipes, para 
que a passagem de atividades sejam simplificadas. 

2 Conceitos básicos 

O DevOps é um movimento profissional que objetiva o apoio, a comunicação 
ea integração entre as pessoas envolvidas no processo de desenvolvimento de 
um projeto (TRAVERSIN, 2017). Nesse sentido, a padronização e a automação 
das ações são imprescindíveis para a manutenção do trabalho. 

Conceitualmente, o DevOps é uma metodologia de desenvolvimento 
de software que interage diretamente com profissionais de infraestrutura. 
Diversas ideias giram em torno dessa cultura, propiciando o bom trabalho dos 
profissionais, sendo as principais ideias apresentadas a seguir. 



Introdu ao DevOps 17 

Automação 

Muitas empresas possuem um ritmo de entrega acelerado e, a cada atualização, 
é necessário realizar diferentes ações para garantir a qualidade do software 
produzido. Os processos realizados manualmente dificultam a entrega mais 
rápida e baixam a produtividade dos colaboradores por estarem presos àquela 
atividade (TRAVERSIN, 2017), de modo que a automação dos processos é 
indispensável para o DevOps. 

Quando os processos são automatizados, eles acabam ficando repetitivos, 

o que permite os entender mais facilmente, descomplica os processos de audi-
torias e possibilita melhorá-los sem grandes problemas (TR AVERSIN, 2017). 
Qualidade 

De modo geral, por meio do software, as empresas buscam propiciar infor-
mações, valores e experiências para os usuários. Muitas vezes, o software é o 
ponto-chave de uma empresa, gerando resultados e benefícios para os clientes. 
Portanto, a qualidade do sistema deve ser primordial na sua construção, o que, 
frequentemente, é difícil de ser alcançado, pois existem muitas barreiras a 
serem superadas (FORRESTER, 2016). 

Em algumas situações, qualidade e agilidade são fatores inversamente 
proporcionais, entretanto, o DevOps faz essas duas características andarem 
lado a lado. A ideia em torno desse aspecto está em reduzir o tamanho das 
entregas, para que elas sejam realizadas mais rapidamente, com maior controle 
e menor risco associado, criando, assim, um bom nível de qualidade em toda 
a cadeia de entrega do software (MICRO FOCUS, 2018). 

No entanto, existe um impacto negativo na qualidade quando a expectativa 
do usuário não é alcançada. De acordo com Micro Focus (2018), os requisitos 
do usuário devem estar sincronizados com os testes, para garantir que todas 
as exigências dos usuários sejam atendidas. 

Produtividade 

Em alguns casos, é possível observar alguns pontos que, inicialmente, parecerão 
falar do mesmo conteúdo, mas, na verdade, tratam de aspectos diferentes, 
ou até mesmo de causa e efeito. Por exemplo, ao automatizar processos, tem-se 
uma menor carga de trabalho e mais tempo para realizar outras atividades. 
Como consequência, é possível aumentar a produtividade. 



18) | Introduç ao DevOps 
Com a eliminação de trabalhos cansativos, repetitivos e enfadonhos, 
os colaboradores conseguem otimizar o período de trabalho. Dessa forma, 
é possível utilizar esse tempo para dar continuidade ao desenvolvimento do 
projeto, reduzir o prazo de entrega, realizar intervenções proativas ou estudar 
uma nova tecnologia. 

Segurança 

Um dos pontos principais de qualquer tecnologia é a segurança, visto que, sem 

ela, a retenção dos usuários torna-se 
necessário manter ações que analisem 
retroceder para momentos estáveis do 

A possibilidade de monitoramento, 
funções do software contribui para o 
velocidade da resolução de problemas e 
que a equipe responsável realize mais 

uma tarefa 
possíveis 
software, 

árdua. Desse modo, faz-se 
falhas no código, bem como 
se houver erros graves. 

registro e análise em tempo real das 
diagnóstico de erros. Isso aumenta a 
possibilita entrega contínua, permitindo 

testes. 

As técnicas utilizadas no DevOps permitem definir ações para serem exe-
cutadas ao fim do processo de produção. Após a criação do software, testes são 
realizados, garantindo, assim, a segurança operacional e a satisfação do cliente. 

Colaboração 

Quando uma organização opta por trabalhar com metodologia DevOps, o fluxo 
de trabalho torna-se mais constante. Isso ocorre em virtude de os processos 

serem bem definidos e porque cada equipe conhece as responsabilidades das 
outras. Desse modo, na definição dos processos, tudo é pensado de forma a 
facilitar o trabalho do próximo. 

Fique atento 

É importante que todas as pessoas envolvidas na organização tenham conhecimento 
das etapas de criação, envio e execução da aplicação. Contudo, isso não significa que 
os desenvolvedores sejam bons operadores, e vice-versa, mas sim que eles devem 
entender as atividades do outro e colaborar para concluir cada novo ciclo do software. 


Introdução ao DevOps 19 

3 Integração e entrega contínuas 

A integração contínua (Cl, continuous integration) e a entrega contínua (CD, 
continuous delivery) são práticas da engenharia de software que visam a testar 
e disponibilizar o software de forma automatizada, contínua e consistente 
(DESTRO; FRANÇA, 2019). Essas ações são aplicadas a projetos de desen-
volvimento de software, com o intuito de manter o nível de qualidade elevado, 
a estabilidade da aplicação e propiciar uma menor quantidade de erros. 

Essas práticas são fixadas em um conjunto, intitulado pipeline, e organi-
zadas de forma lógica e sequencial, conforme a Figura 2. 

D=" -==3> 

*Taopesca eTemespagares eferamemado eTeseagomótco eferanemande + Neniteramento

de operações consiução e Testes deacetação implantação « a

Respondendo condições 

+ Apoonaintegação dousuário deco 
«Testes de unciade 
Figura 2. Abstração do pipeline DevOps. 

Fonte: Adaptada de Souza Neto (2018). 

O pipeline pode variar de acordo com o projeto, considerando-se que cada 
sistema suporta tecnologias distintas e possui ferramentas específicas para 
realizar diferentes ações (DESTRO; FRANÇA, 2019). A metodologia ágil 
recomenda que o processo de desenvolvimento de software seja executado 
com maior frequência de entregas. Como resultado, o trabalho necessário para 
coletar, integrar e testar todo o código desenvolvido pela equipe no repositório 
central se tornou mais frequente. 

Assim, a CI foi introduzida para limitar o trabalho manual que era neces-
sário para o processo de integração do sistema. Além disso, acrescentou-se 

o processo de testes automatizados, garantindo o funcionamento do sistema 
após cada atualização. Para adotar essa prática, é essencial utilizar ferramen-
tas de controle de versão, como GitHub e GitLab, a fim de obter garantia do 
controle entre as versões dos códigos, possibilidade de retorno para versões 
anteriores e viabilidade de diferentes programadores trabalharem no mesmo 
código através de branches distintas. 

20 || Introdução ao DevOps 

Antes de falar sobre CD, é preciso entender os tipos de servidores. O pri-
meiro tipo é o servidor de teste, que possui recursos limitados e é utilizado 
para realizar os testes de forma mais controlada. O segundo é o servidor de 
homologação, que é basicamente uma cópia do servidor de produção, pos-
suindo integração com todas as APIs (application programming interface; ou 
interface de programação de aplicações, em português) utilizadas. Com isso, 
os testes são realizados de forma mais fidedigna. Por fim, tem-se o servidor 
de produção, que se refere ao ambiente final que é acessado pelos usuários. 

A CD é uma evolução da CI. Com ela, além das ações apresentadas ante-
riormente, é possível garantir que o código esteja pronto para ir ao servidor de 
produção. Para tanto, existe um deploy automatizado que sobe o código para o 
servidor de homologação, realiza os testes pré-programados e, caso não ocorra 
nenhum erro, encaminha o software para o servidor de produção. A Figura 3, 
a seguir, apresenta um detalhamento da divisão de tarefas e responsabilidades 
das práticas DevOps. 

OREVIEW 
Bo &
STAGING PRODUCTION! 
auto UNIT INTEGRATIONTESTS TESTS 
CAPIPELINE 
o o 
CO PIPELINE 
o 

RELATED CODE 

Figura 3. Detalhamento dos pipelines Cle CD 
Fonte: Marcio (2019, documento on-line). 

Para os profissionais, trabalhar com CI e CD implica diminuição do tra-
balho, redução de erros e entregas rápidas. Já para a organização, o tempo 
necessário para lançar uma atualização é encurtado. Além disso, os usuário 
recebem novos updates com maior frequência. 



Introdução ao 

Referências 

DESTRO, G. A.; FRANÇA, B. B. N. Avaliação do nível da aplicação de práticas de entrega 
e integração contínua em repositórios de código aberto. 2019. Projeto Final (Graduação) 

— Universidade Estadual de Campinas, Campinas, 2019. Disponível em: https://Awww. 
icunicamp.br/-reltech/PFG/2019/PFG-19-31.pdf. Acesso em: 31 jul. 2020. 
FORRESTER, A. Accelerate your DevOps to awesome: learn how to take your DevOps to. 
the next level. London: Forrester, 2016. 

MARCIO. CI/CD: continuous integration and continuous delivery. [S. |]: Medium, 2019. 
Disponível em: https://medium.com/tecnologia-e-afins/ci-cd-continuous-integration-
-and-continuous-delivery-fb5dOaed4bf5. Acesso em: 31 jul. 2020. 

MATTIOLI, F. E. R. et al. Uma proposta para o desenvolvimento ágil de ambientes 
virtuais. /n: WORKSHOP DE REALIDADE VIRTUALE AUMENTADA, 6., 2009, Santos. Anais 

[.J. Santos: SBC, 2009. 

MICRO FOCUS. Measuring DevOps success: how do you know DevOps is working? Watch 
these KPIs. Berkshire: Micro Focus, 2018. Disponível em: http://files.asset.microfocus. 
com/42a6-3036/en/4aa6-3036.pdf. Acesso em: 9 jul. 2020. 

SOUZA NETO, P.J. Integração e entrega contínua para aplicações móveis desenvolvidas em 
React Native. 2018. Trabalho de Conclusão de Curso (Graduação) -Universidade Federal 
de Pernambuco, Recife, 2018. Disponível em: https:/Awww.cin.ufpe.br/-tg/2018-2/ 
TG. Sl/pjsn.pdf. Acesso em: 31 jul. 2020. 

TRAVERSIN, G. DevOps: habilidades e capacidades necessárias para sua utilização. 2017. 
Monografia (Especialização) -Universidade de Taubaté, Taubaté, 2017. Disponível em: 
http://repositorio.unitau.br:8080/jspui/bitstream/20.500.11874/1246/1/GUSTAVO%20 
TRAVERSIN.pdf. Acesso em: 31 jul. 2020. 

Leituras recomendadas 

BOAGLIO, F. Jenkins: automatize tudo sem complicações. 2. ed. São Paulo: Casa do 
Código, 2019. 

KIM, G. etal. Manual de DevOps: como obter agilidade, confiabilidade e segurança em 
organizações tecnológicas. Rio de Janeiro: Alta Books, 2018. 

MORAES, G. Caixa de ferramentas DevOps: um guia para construção, administração e 
arquitetura de sistemas modernos. São Paulo: Casa do Código, 2015. 

DevOps || 21 
EN 


22 || Introdução ao DevOps 

Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 


Tipos de controles de 
versão e verbetes 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Listar os sistemas mais utilizados de controles de versão de código-
-fonte. 

m Demonstrar as principais operações utilizadas nos sistemas de versio-
namento. 

E Diferenciar sistemas de versionamento distribuídos de sistemas de 
versionamento centralizados. 

Introdução 

Cada vez mais há necessidade de agilidade no fluxo de trabalho de 
desenvolvimento de software. Em virtude da enorme quantidade de 
linhas de código e dos inúmeros projetos em desenvolvimento, tornou-se 
comum a utilização de ferramentas e padrões que viabilizam um maior 
controle no ambiente DevOps. Para Kim et al. (2018), no DevOps ideal, 
os programadores recebem feedback rápido e frequente com relação ao 
seu trabalho, o que possibilita que eles possam implementar, integrar 
e validar o seu código-fonte com maior rapidez. Mas como é possível 
controlar todo um processo de desenvolvimento de software sem medo 
de perder informações ou sobrescrever atualizações? A resposta para essa 
questão é a utilização de um controle de versão. 

Um sistema de controle de versão, também conhecido como VCS 
(do inglês Version Control System) ou SCM (do inglês Source Code Mana-
gement) na função prática da computação e da engenharia de software, 
tem como finalidade gerenciar várias versões no desenvolvimento de um 
documento qualquer. Isso garante a gestão de forma mais inteligente e 
eficaz para organizar qualquer projeto, possibilitando o acompanhamento 
de históricos de desenvolvimento, atividades paralelas, customização de 
uma determinada versão e finalidades específicas sem a necessidade de 



24 ) (Tipos de controles de versão e verbetes 
se alterar O projeto principal ou até mesmo recuperar uma versão anterior, 
caso tenha ocorrido perda de alguma informação. 

Os sistemas de controle de versão são muito utilizados no desenvol-
vimento de sistemas pelos iniciantes na área, que, ao serem inseridos em 
uma equipe de programadores, não conhecem as ferramentas utilizadas 
para que se consiga trabalhar em paralelo de forma eficiente, excluindo-
-se as possibilidades de sobreposição de alterações, utilização de versão 
errada, entre outras. 

Neste capítulo, você conhecerá os principais tipos de controle de 
versão, também conhecido como versionamento. Além disso, verá quais 
são as principais operações utilizadas nos sistemas de versionamento. 
Por fim, conhecerá a diferença entre sistemas de versionamento distri-
buídos e centralizados. 

1 Controle de versão 

De maneira bem simples, os arquivos de um projeto são armazenados em um 
repositório, no qual o histórico de suas versões é registrado. Os programadores 
podem acessar e recuperar a última versão disponível e realizar uma cópia 
local, que possibilitará fazer alterações sobre ela. É possível submeter cada 
alteração executada ao repositório e recuperar as atualizações feitas pelos 
outros membros da equipe. 

Dentro dessa arquitetura, é importante ressaltar que o controle de versão 
suporta o desenvolvimento em algumas formas, apresentadas a seguir. 

E Registrando o histórico: armazena toda a evolução do projeto, ou seja, 
toda alteração realizada é registrada no repositório, o que possibilita a 
identificação de autor, data e origem das alterações. Uma característica 
importante é a possibilidade de reconstrução de determinada revisão 
específica do código-fonte, sempre que necessário. 
= Colaborando concorrentemente: permite que mais de um programa-
dor realize alterações em paralelo sobre um mesmo código-fonte, sem 
sobrescrever as modificações de outro membro da equipe. 
E Variações no projeto: proporciona a manutenção de versões diferentes 
de evolução do mesmo projeto. Ou seja, a versão 1.0 é a oficial, enquanto 
se prepara a versão 2.0. 



Tipos de de versão e verbetes 25

controles )[ 

Os controles de versões são compostos, basicamente, por duas partes: 

o repositório (servidor), que armazena todo o histórico de ajustes do projeto, 
registrando todas as alterações realizadas nos itens versionados; e a estação de 
trabalho, que possui uma cópia dos arquivos do projeto vinculada ao servidor 
para a identificação de possíveis modificações (Figura 1). Cada estação de 
trabalho é considerada individual e isolada das demais. 
N 
SERVIDOR 

Estação de trabalho 1 Estação de trabalho 

Estação de trabalho 

Figura 1. Arquitetura básica do controle de versão. 
No J 

O processo de sincronização entre a estação de trabalho e o repositório 
é realizado por meio dos comandos commit e update. O comando com-
mit submete um pacote com as modificações feitas pela estação de trabalho 
(origem) ao repositório (destino). Já o comando update realiza o processo 
inverso, ou seja, disponibiliza as alterações submetidas pelas demais estações 
de trabalho ao repositório (origem) para a estação de trabalho (destino), que 
deseja atualizar o projeto. 



26) [ Tipos de controles de versão e verbetes 
É importante ressaltar que todo commit cria uma revisão no servidor 
contendo as alterações, a data e o usuário responsável. O conceito de revisão 
pode ser ilustrado como uma fotografia de todos os arquivos e diretórios em 
um momento específico do projeto. Os registros antigos são guardados e podem 
ser recuperados assim que houver necessidade, e o conjunto dessas revisões 
é especificamente o histórico de alterações do projeto. 

Existem diversas ferramentas para controle de versão no mercado, sendo 
algumas gratuitas e outras proprietárias. A seguir, serão apresentados os 
principais sistemas utilizados pelas equipes de desenvolvimento de software 
para controle de versionamento. 

GIT 

A ferramenta Git (Figura 2) é um dos sistemas de controle de versão e de 
controle de código-fonte distribuído mais conhecidos e utilizados pelas equi-
pes de desenvolvimento de software. O Git foi criado por Linus Torvalds 
especificamente para o desenvolvimento do kernel Linux, porém tornou-se 
popular e foi adotado para outros projetos. 

git 

Figura 2. Logotipo do Git. 
Fonte:Git (2020, documento on-line). 

Uma das vantagens do Git é o fato de ele ser livre e de código aberto, além 
de ter sido projetado para lidar com todo tipo de projeto, desde pequenos a 
enormes, com eficiência e rapidez. Algumas características interessantes do 
Git são a garantia de dados, área de teste, ramificação e fusão. 

Para o gerenciamento dos repositórios na web, os desenvolvedores utilizam, 
em sua maioria, o github ou o bitbucket. 



Tipos de controles de versãoverbetes )[

e 27 

Redmine 

A ferramenta Redmine (Figura 3) consiste em uma aplicação web desenvolvida 
por meio do uso do framework Ruby on Rails, além de ser multiplataforma e 
utilizar um banco de dados cruzado. 

£*% REDMINE 

flexible project management 

Figura 3. Logotipo do Redmine 
Fonte: Redmine (2020, documento on-line), 

Dentre as características do Redmine, destacam-se as apresentadas a seguir: 

suporte para múltiplos projetos; 
controle flexível de acesso baseado em função; 
sistema de rastreamento de problemas flexíveis; 
rastreamento do tempo; 
suporte a vários bancos de dados; 
integração com diversos repositórios (SVN, CVS, Git, Mercurial e 
Bazaar). 

IBM Rational ClearCase 

O IBM Rational ClearCase (Figura 4) disponibiliza acesso controlado aos ativos 
de software, incluindo código, requisitos, documentos de projeto, modelos, 
planos e resultados de testes. Além disso, ele oferece suporte a desenvolvimento 
paralelo, gerenciamento automatizado do espaço de trabalho, gerenciamento 
de linha de base, gerenciamento seguro de versões, auditoria confiável de 
compilação e acesso flexível. 



28) (ripos de controles de versão e verbetes 
Figura 4. Logotipo do IBM Rational ClearCase. 
Fonte: IBM (2020, documento on-line) 

Dentre as características do IBM Rational ClearCase, destacam-se as 
apresentadas a seguir: 

gerenciamento automatizado do espaço de trabalho; 
acesso transparente e em tempo real a arquivos e diretórios; 
suporte a desenvolvimento paralelo; 
gerenciamento avançado de criação e lançamento; 
acesso local, remoto e de cliente à web. 

Subversion 

O Subversion (Figura 5) é um sistema de controle de versão de código aberto, 
centralizado e caracterizado, em virtude de sua confiabilidade, como um repo-
sitório seguro para projetos e dados importantes. Alguns pontos de destaque 
são: a simplicidade do modelo e seu fácil uso e a capacidade de suportar as 
demandas de uma variedade de usuários e projetos para operações corporativas 
de grande escala. 



Tipos de controles de versão e verbetes ) [29 

SUBVERSION 

Figura 5. Logotipo do Subversion. 
Fonte: IBM (2020, documento on-line). 

Dentre as características do Subversion, destacam-se as apresentadas a 
seguir: 

versionamento de diretórios; 
registro de cópias, exclusões e renomeação; 
metadados versionados de forma livre; 
commits únicos e individuais; 
bloqueio de arquivos; 
resolução de conflitos interativos. 

Mercurial 

O Mercurial (Figura 6) é um sistema de controle de versão distribuído e 
gratuito que gerencia de forma eficaz projetos de qualquer tamanho, além de 
oferecer uma interface fácil e intuitiva. Cada cópia possui todo o histórico 
do projeto, com isso, a maioria das ações é realizada localmente, de forma 
rápida e conveniente. 



30 ) (Tipos de controles de versão e verbetes 
o) 

mercurial 

Figura 6. Logotipo do Mercurial 

Fonte: Mercurial (2020, documento on-line), 

Dentre as características do Mercurial, destacam-se as apresentadas a 
seguir: 

suporte a vários fluxos de trabalho; 
arquitetura distribuída; 
facilidade de uso; 
extensível; 
código aberto; 
plataforma independente. 

Os principais objetivos do desenvolvimento do Mercurial incluem alta 
performance e escalabilidade, descentralização, desenvolvimento colabora-
tivo distribuído, controle de arquivos textuais e binários de forma robusta e 
operações avançadas de ramos e mesclagem. Além disso, o Mercurial inclui, 
de forma integrada, um sistema de visualização dos repositórios via web e 
facilitação na transição de usuários do Subversion. 



Tipos de controles de e verbetes )[

versão E 

Darcs 

O Darcs (Figura 7) é um sistema de controle de versão de código aberto 
e gratuito e de plataforma cruzada, como Git, Mercurial ou Subversion. 
No entanto, uma característica diferenciada é o seu foco em mudanças, em vez 
de em snapshots. O Dares permite, ainda, uma forma de trabalho mais livre e 
interface de usuário simplista. Outro ponto a ser considerado como diferencial 
é que ele não exige um servidor centralizado, funcionando perfeitamente no 
modo off-line. 

(w darcs 

Figura 7. Logotipo do Darcs 

Fonte: Darcs (2020, documento on-line) 

Dentre as características do Darcs, destacam-se as apresentadas a seguir: 

modo off-line; 

preparação local; 

fácil ramificação e fusão; 

fácil colaboração por e-mail; 

desenvolvimento paralelo; 

interatividade; 

hospedagem própria. 



32) (Tipos de controles de versão e verbetes 
BAZAAR 
O Bazaar (Figura 8) é um sistema de controle de versão 
de rastrear o histórico do projeto ao longo do tempo, 
colaboração entre desenvolvedores. 
que 
além 
tem 
de 
a capacidade 
possibilitar a 
Cor 

Figura 8. Logotipo do Bazaar 

Fonte: Bazaar (2020, documento on-line) 
A 

Dentre as características do Bazaar, destacam-se as apresentadas a seguir: 

trabalho off-line; 

suporte a diversos fluxos de trabalho; 

suporte multiplataforma; 

rastreamento de renomeação e mesclagem inteligente; 

alta eficiência e velocidade de armazenamento. 

O Git e o Mercurial são muito flexíveis em relação aos fluxos de traba-
lho, porém o Bazaar é a única dessa ferramenta que suporta ramificações 
vinculadas, sendo uma forma mais fácil e segura de implementar um fluxo 
de trabalho centralizado. 



Tipos de controles e verbetes (3

de versão 

2 Operações em controle de versões 

Para melhor compreensão do funcionamento dos sistemas de controle de versão, 
é importante conhecer as principais operações que os envolvem. A seguir, 
são apresentados os elementos mais utilizados pela maioria dos controles de 
versionamento: 

m Commit (Checkin): criação de uma nova versão do projeto. 
Em Checkout: recuperação de uma versão específica do projeto ou arquivo. 
m Revert: possibilita ao desenvolvedor descartar as mudanças realizadas 
em estação local, recuperando a mesma versão do repositório. 
E Diff: garante a possibilidade de comparação do arquivo na estação local 
com qualquer outra versão do repositório. 
E Delete: permite a exclusão de um arquivo do repositório. Quando as 
demais estações de trabalho realizarem um update, o arquivo será 
efetivamente excluído do repositório. 
E Lock: possibilita o travamento de determinado arquivo, de forma que 
nenhum outro usuário o modifique. 

Saiba mais 

Uma tag é simplesmente uma representação mais simples de se lembrar, para seres 
humanos, de uma determinada versão do código. 

Confira, a seguir, alguns conceitos relevantes com relação aos controles 
de versões. 

= Repositório de versões: armazenamento de todas as versões dos ar-
quivos sob o controle versões. 

= Repositório de versões centralizado: cada estação de trabalho local 
contém apenas uma versão específica da árvore de versões do reposi-
tório central, e todos os usuários realizam as operações de controle de 
versões nesse repositório. 

m Repositório de versões distribuído: cada estação local possui um reposi-
tório acoplado, de modo que o usuário possui um repositório próprio para 
realizaro controle de versões. As operações realizadas sobre os arquivos 



34 ) ( Tipos de controles de versão e verbetes 
são feitas no repositório local do usuário, e operações específicas dos repo-
sitórios distribuídos são utilizadas para sincronizar repositórios diferentes. 

m Árvore de revisões ou de versões: estrutura lógica que mapeia todas 
as versões armazenadas no repositório para determinado arquivo ou 
conjunto de arquivos. 

Fique atento 

É importante ressaltar que o repositório evita a perda de dados, porém o que está na 
estação local não pode ser recuperado se houver um problema com o computador. 
É possível, ainda, ignorar arquivos para que eles não sejam comitados nem apareçam 
na lista de arquivos passíveis de commit. 

Para cada um dos tipos de controle de versão, há um conjunto de operações 
básicas, conforme o Quadro 1, a seguir. 

á o 
Quadro 1. Operações básicas dos controles de versão centralizado e distribuído 

Controle de versão | Controle de versão 
D É centralizado distribuído 

Criação de estação de CHECKOUT CLONE 

trabalho ou repositório. 

Envio de modificações para COMMIT COMMIT 

o repositório, gerando uma 
revisão. 
Alteração da estação de UPDATE UPDATE 

trabalho em uma revisão. 

Importação de revisões feitas PULL 

em outro repositório. 

Envio de revisões locais para PUSH 
outro repositório. 


Tipos de controles de versão e verbetes ) ( 35 

Uma característica importante dos controles de versão é a possibilidade de 
separar modificações em um caminho diferente para cada desenvolvimento, 
conforme a Figura 9. Esse caminho, conhecido como branch (seta verde), 
é utilizado especialmente para a implementação de novas funcionalidades, 
sem comprometer o caminho principal da implementação, denominado trunk, 
(seta azul), com erros de compilação e bugs. A branch só será integrada ao 
trunk quando ela se tornar estável. Há também a possibilidade de congelamento 
de revisão, denominada tag (seta cor de laranja), isto é, é um estado fixo do 
produto que possui um conjunto de funcionalidades estáveis que não sofrerão 
mais nenhuma alteração. 

— a 
1204 116 1205146 
14 

1.20. BRANCH (Correção de bugs) 
TRUNK (Novas funcionalidades) 

121 BRANCH (Correção

de bugs) 

1219746 

Figura 9. Diferenças entre trunk, branch e tag 

Fonte: Adaptada de Guelfi (2015). 

Versionamentos distribuído e centralizado 

Existem dois tipos de sistemas de controles de versão: centralizado e distri-
buído. Ambos possuem repositórios e estações de trabalho, porém a diferença 
entre eles está em como cada um está estruturado e organizado. A seguir, são 
apresentados os detalhes e as diferenças entre esses dois tipos de versionamento. 



36) [ Tipos de controles de versão e verbetes 
Versionamento centralizado 

O sistema de controle de versão centralizado é composto por um único servidor 
central e várias estações de trabalho, com base no conceito de arquitetura 
cliente-servidor. Por ser um sistema centralizado, as estações de trabalho 
necessitam consultar o servidor para a realização da comunicação. Esse mo-
delo atende à maioria das equipes de desenvolvimento de sistemas de médio 
porte para a realização de implementação por meio de uma rede local, além 
de não necessitar de velocidade para o envio e o recebimento dos dados. Um 
dos sistemas mais comuns com esse tipo de controle de versão centralizado 
é o Subversion. 

Os sistemas de controle de versão centralizados possuem uma topologia 
em forma de estrela, ou seja, há um único repositório central com diversas 
estações de trabalho, uma para cada desenvolvedor. A comunicação entre 
as estações de trabalho passa, obrigatoriamente, pelo repositório central, 
conforme a Figura 10. 

REPOSITÓRIO CENTRAL 

Estação de trabalho | | Estação de trabalho 

Estação de trabalho 

Figura 10. Sistema de controle de versão centralizado. 
A J 


Tipos de controles de versão e verbetes ) ( 37 

Algumas vantagens do uso do versionamento centralizado são: maior 
controle do projeto, imposição de segurança de acesso com facilidade e possibi-
lidade de bloqueio de arquivos específicos, sendo ideal para equipes pequenas. 
Com relação às desvantagens, pode-se considerar: baixa escalabilidade e 
necessidade constante de conexão com a internet. 

Versionamento distribuído 

Os sistemas de controle de versão distribuídos podem ser definidos como 
diversos repositórios autônomos e independentes, um para cada programador. 
Cada repositório tem a sua estação de trabalho acoplada, onde as operações 
commit e update ocorrem localmente, conforme a Figura 11. 

Essa arquitetura é recomendada para equipes com uma grande quantidade 
de desenvolvedores que estão remotamente distantes. O funcionamento do 
sistema de controle de versão distribuído ocorre da seguinte forma: cada 
estação de trabalho possui o seu próprio repositório, ou seja, as operações de 
checkin e checkout são realizadas de forma local. Ao contrário da arquitetura 
centralizada, essas estações de trabalho podem se comunicar entre si, porém é 
recomendável que se utilize um servidor responsável pelo envio dos arquivos 
para que se organize o fluxo e se evite ramificações do projeto e a perda do 
controle. Na maioria das vezes, o sistema oferece um servidor remoto para 
que o projeto seja hospedado. O processo de comunicação entre o servidor 
principal e as estações de trabalho funciona por meio de duas operações: uma 
para atualizar (puxar) e outra para mesclar o projeto (empurrar), conhecidas 
como pull e push, respectivamente. 

Considerando-se que o processo é local, o sistema de controle distribuído 
possui maior rapidez, porém exige um maior conhecimento da ferramenta 
e, inicialmente, pode confundir o programador. Por exemplo, o sistema de 
mesclagem em alterações concorrentes torna-se distinto por trabalhar em um 
sistema de arquivos binários, que, em determinadas situações, não possibilita 
a comparação entre essas atualizações ao mesmo tempo. Já os sistemas de 
controle de versão centralizados utilizam arquivos de texto, o que permite 
a comparação em alterações concorrentes, apresentando ao programador a 
possibilidade de escolher a solução ideal. 



38 ) (ricos de controles de versão e verbetes 
4 D 
REPOSITÓRIO 

4 

i

!

i

L] ! 
L. 

Estação de r 
trabalho 1 Í 

Figura 11. Sistema de controle de versão distribuído. 
NU J 

Com relação ao sistema de controle de versão distribuído, algumas das 
vantagens do seu uso são replicação do repositório, maior rapidez e autonomia, 
pois ele permite a realização de alteração off-line. No que se refere às desvan-
tagens, pode-se citar a complexidade do fluxo de trabalho e a dificuldade em 
bloqueio de arquivos específicos. 

Por fim, observa-se que os sistemas de controle de versão resolvem mui-

tos problemas relacionados diretamente ao desenvolvimento de software. 
Atualmente, é prática comum a utilização dessa forma de trabalho, e exis-
tem inúmeras ferramentas disponíveis no mercado, conforme apresentado. 
É importante ressaltar que, antes de escolher qual sistema utilizar, deve-se 
analisar as opções e identificar a solução que melhor atende às necessidades 
da equipe de desenvolvimento. 



Tipos de controles de versão e verbetes ) ( 39 
Referências 
BAZAAR. [Site]. [2020]. Disponível em: https://bazaar.canonical.com/en/. Acesso em: 
26 ago. 2020. 
DARCS. [Site]. [2020]. Disponível em: http://darcs.net/ Acesso em: 26 ago. 2020. 
GIT. [Site]. [2020]. Disponível em: https://git-scm.com/. Acesso em: 26 ago. 2020. 
GUELFI, E. Conceitos do controle de versão: criando branchs e tags utilizando Tortoise SVN. 
2015. Disponível em: https://tsdn.tecnospeed.com.br/blog-do-desenvolvimento-tec-
nospeed/post/conceitos-do-controle-de-versao-criando-branchs-e-tags-utilizando-
-tortoise-svnit-:text=Logo%20de%20caraw20podemos%20dizer funcionalidades%20 
que%20n%C3%A30%20ser%C3%A30%20mais. Acesso em: 26 ago. 2020. 
IBM. What can IBM Rational ClearCase do for my business? [2020]. Disponível em: https:// 
www.ibm.com/us-en/marketplace/rational-clearcase. Acesso em: 26 ago. 2020. 
KIM, G. etal. Manualde DevOps: como obter agilidade, confiabilidade e segurança em 
organizações tecnológicas. Rio de Janeiro: Alta Books, 2018. 
MERCURIAL. [Site]. [2020]. Disponível em: https://Awww.mercurial-scm.org/. Acesso 
em: 26 ago. 2020. 
REDMINE. [Site]. [2020]. Disponível em: https://www.redmine.org/projects/redmine/ 
wiki/Logo. Acesso em: 26 ago. 2020. 
Leituras recomendadas 
ARUNDEL, J.; DOMINGUS, J. DevOps nativo de nuvem com Kubernetes: como construir, 
implantar e escalar aplicações modernas na nuvem. São Paulo: Novatec, 2019. 
MUNIZ, A. et al. Jornada DevOps: unindo cultura ágil, Lean e tecnologia para entregar 
software com qualidade. 2. ed. Rio de Janeiro: Brasport, 2020. 
Fique atento 
Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 
» 


Esta página foi deixada em branco intencionalmente. 



Criação de branches e 
commits com o Git 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Reconhecer o uso da ferramenta Git para versionamento de código-
fonte. 
m Descrever as vantagens do Git em relação aos sistemas de versiona-
mento distribuídos ou centralizados e sua estrutura de funcionamento 
de branches e staging area. 
m Demonstrar a criação de repositórios e a manipulação de arquivos 
no sistema de versionamento do Git. 

Introdução 

Para tornar o processo de controle de versão de software eficiente, con-
templando o fluxo de trabalho das equipes de desenvolvimento, como 
as alterações realizadas no código-fonte ao longo do projeto, as identi-
ficações das versões lançadas, a estrutura de múltiplas frentes paralelas 
de execução de testes, a homologação, o controle de qualidade e a 
execução em produção, a ferramenta de gerenciamento de controle 
de versão deve prover mecanismos que atendam a esses requisitos de 
forma dinâmica, simples e independente para as pessoas envolvidas no 
projeto. O conhecimento a respeito da forma como é realizado o controle 
de versão de software por meio de uma ferramenta de gerenciamento 
é cada vez mais importante no mercado, visto que grande parte das 
equipes de projetos de software na internet, comunidades responsáveis 
por um determinado repositório público de aplicativo ou mesmo grupos 
de trabalho colaborativo têm como prática comum a utilização de um 
sistema de versionamento, a fim de garantir uma gestão mais eficiente 
das diversas alterações realizadas no código e nos arquivos. 



42 ) (criação de branches e commits com o Git 
Neste capítulo, você estudará sobre o Git, uma das ferramentas de 
controle de versionamento mais populares entre os desenvolvedores 
de sistema. Além disso, conhecerá as principais vantagens do formato 
de controle de versão do Git em relação às outras ferramentas corres-
pondentes, bem como a estrutura em que os repositórios são dispostos, 

o formato de branches e a manipulação dos arquivos na denominada 
staging area. Por fim, verá os principais modos de criação e gerenciamento 
dos repositórios e arquivos de projeto dentro do sistema do Git. 
1 Conhecendo a ferramenta Git 

Criado em 2005, o Git é uma ferramenta de controle de versão de código-fonte 
disponibilizada em formato de código aberto, utilizada pela grande maioria das 
equipes de projetos de desenvolvimento de sistemas na atualidade. Conforme 
Silverman (2013), o Git utiliza um formato de estrutura de projeto com base 
no modo como a ferramenta é utilizada na prática. 

Por meio do uso do Git, é possível criar o histórico de edições realizadas 
no código-fonte de um projeto, facilitando o processo de consulta do status 
de um arquivo editado e seu conteúdo em um determinado ponto no tempo. 
Além disso, essa ferramenta facilita a edição paralela entre arquivos, de forma 
simultânea, entre os vários desenvolvedores em um mesmo projeto, por meio 
do controle de fluxo de novas funcionalidades com ferramentas para análise 
e resolução de conflitos das modificações realizadas. Com isso, é possível 
desenvolver projetos nos quais uma série de pessoas podem contribuir simul-
tancamente nos mesmos arquivos, da forma mais segura e confiável possível, 

o que permite que esses conteúdos possam existir sem o risco de suas alterações 
serem sobrescritas. 
Saiba mais 

Criador do kernel Linux, o finlandês Linus Torvalds também é o responsável pela 
ferramenta Git (GANTACROCE, 2015). A ferramenta de controle de versão surgiu a partir 
de uma necessidade da comunidade de desenvolvedores do sistema operacional Linux 
de manter o compartilhamento das mudanças de código-fonte do kernel. A mudança 
para Git se da após a ferramenta de sistema de controle de versão utilizada até aquele 
momento pelo projeto tornar-se paga (CHACON; STRAUB, 2014). 


Criação de branches e commits com o Git ) (a3 

Sempre que os desenvolvedores criam um novo projeto, os arquivos relacio-
nados ao código-base de um sistema são constantemente modificados, mesmo 
após o lançamento do projeto, por meio de atualização de versões, correção de 
falhas, adição de novas ferramentas, entre outros aspectos. Conforme Chacon 
e Straub (2014), em comparação com outros controladores de versão, o Git 
é categorizado como um sistema de controle de versão distribuído, pelo 
fato de permitir alterações independentes de versões, sem a necessidade de 
um servidor central para o armazenamento do banco de dados de controle. 

O Git tem como principal objetivo auxiliar no acompanhamento das mu-
danças feitas nos arquivos ao longo do tempo, identificando o registro de quem 
efetuou uma alteração. Dentre as características que o controle de versiona-
mento com o Git possibilita, destacam-se a possibilidade de restauração de 
porções de código removido ou previamente modificado, a manutenção da 
organização do código-fonte entre a equipe de desenvolvimento, a criação de 
históricos de funcionalidades e o backup do código utilizado em si. 

O Quadro 1, a seguir, apresenta alguns dos principais termos associados 
ao Git, os quais são necessários para entender o funcionamento da ferramenta. 

á N 

Quadro 1. Principais termos do Git 

Termo Descrição 

Repositório | O repositório é o diretório onde os arquivos do projeto 
ficam armazenados, também denominado diretório 
Git, o qual pode estar presente de forma local ou 
remota (em outro servidor; CHACON; STRAUB, 2014). 

Commit O commit é o ponto no histórico do projeto que indica 
um conjunto de modificações realizadas em um ou mais 
arquivos do projeto naquele momento (CHACON; STRAUB, 
2014). Além disso, uma descrição é utilizada para identificar 
as alterações realizadas nesse determinado ponto. 

Diretório de | Diretório onde os arquivos de uma determinada 

trabalho versão do repositório do projeto ficam disponíveis para 
acesso e manipulação por parte dos usuários (CHACON; 
STRAUB, 2014). Esse conteúdo é, na verdade, uma simples 
cópia dos arquivos contidos no diretório Git. 


44 )[ Criação de branches e commus com o Gt 

Uma das principais características de descentralização do controle de 
versão com o Git é a capacidade de gerenciamento utilizando, em sua maioria, 
Os recursos locais da máquina onde está hospedado o repositório do projeto. 
Com isso, é possível ter uma velocidade de operação maior nas operações de 
modificação e consulta de histórico dos arquivos do projeto, ou até mesmo 
manter a manipulação dos arquivos em um ambiente sem conectividade de rede. 

Em um conceito de sistema de controle de versão distribuído, cada máquina 
na qual estão hospedados os arquivos do projeto se torna independente, com 
sua própria estrutura de repositório. Segundo Humble e Farley (2014), a grande 
diferença de um sistema distribuído para outros formatos de controle de versão é 
areal utilização concorrente dos repositórios por múltiplos usuários. Entretanto, 
existe a possibilidade de troca entre os arquivos de repositórios diferentes, 
utilizando-se um servidor remoto para centralizar o conteúdo das máquinas 
nas quais estão armazenados os arquivos do projeto. Conforme Santacroce 
(2015), as máquinas passam a se comunicar com o servidor para a atualização 
dos arquivos e a versão do projeto por meio de operações conhecidas como 
push (realizar o envio dos arquivos locais para a versão do projeto no servidor 
remoto) e pull (baixar os arquivos do servidor na máquina local e atualizar 
com os arquivos existentes). 

Um dos métodos mais conhecidos para a centralização de repositórios Git em um 
ambiente remoto é a utilização do serviço em nuvem conhecido como GitHub. Essa 

plataforma atua como um servidor remoto de controle de versão e hospedagem de 

repositórios de projetos, com os quais é possível interagir por meio do Git. 

Segundo Chacon e Straub (2014), para garantir a integridade dos arquivos 
nos repositórios de projetos, o Git utiliza o conceito de checksum, por meio 
do qual um hash do tipo SHA-l (i.e. uma sequência de 40 caracteres em 
formato hexadecimal) é sempre gerado para identificar a estrutura de um 
determinado arquivo no banco de dados da ferramenta de controle de versão. 
Assim, ao realizar qualquer modificação no arquivo presente no repositório, 

o Git modificará o valor da sequência de hash armazenado. 

Criação de branches e commits com o Gt) ( as 

Os status dos arquivos do projeto durante o gerenciamento de versão 
com o Git podem ser atribuídos em três formas principais, por meio de uma 
sequência lógica: 

E Em um primeiro momento, os desenvolvedores realizam as alterações 
nos arquivos dentro do repositório do projeto. Com isso, o Git identifica 
as alterações nos arquivos, porém os conteúdos modificados ainda não 
sofreram o processo de commit no banco de dados de controle de versão, 
sendo marcados com o status modified. 

E Em seguida, os arquivos modificados podem ser marcados no Git, 
a fim de que sejam preparados para fazer parte do próximo processo 
de commit de versão, sendo marcados com o status staged. 

E Por fim, os arquivos que foram preparados e marcados para serem 
adicionados são gravados, de forma definitiva, no banco de dados 
de controle de versão (localizado no diretório Git). Esse é o status 
relacionado diretamente com o commit dos arquivos no projeto, sendo 
marcado com o status committed. 

O Git é uma ferramenta multiplataforma, com versão disponível para 
instalação nos principais sistemas operacionais do mercado, como Linux, 
Windows e Mac OS. Para a sua utilização, a ferramenta possui o gerenciamento 
de versão de código-fonte tanto por meio da linha de comando quanto pelo 
formato de aplicação via interface gráfica. 

Fique atento 

Para instalar o Git em uma determinada plataforma, recomenda-se seguiras instruções. 
da documentação, disponíveis no site oficial do projeto. 


46) (ração de branches e commts com o Git 
2 Estrutura de versionamento do Git 

Uma das principais diferenças do Git em relação às demais ferramentas de 
controle de versão é a sua estrutura de gerenciamento dos dados dos arquivos 
do projeto. Em geral, um sistema de versionamento de código-fonte trata 
somente das mudanças realizadas nos arquivos, armazenando as alterações 
que foram feitas, e não o arquivo inteiro. Uma das vantagens da estrutura 
de versionamento do Git é que a ferramenta trata os dados com um conceito 
denominado snapshot, ou seja, ao realizar um commit, é como se o sistema 
tirasse uma foto dos arquivos existentes, mantendo um valor de hash que faz 
referência à versão anterior. A eficiência no armazenamento se dá pelo fato 
de que, ao manter essa referência, caso não existam alterações realizadas nos 
arquivos do repositório, o Git não gravará o arquivo novamente, realizando 

apenas um apontamento para o conteúdo existente. 

A Figura 1, a seguir, apresenta a forma como o Git trabalha com o fluxo 

geral dos status dos arquivos em sua estrutura de projeto (modified, staged 

e commited). 

Working Staging «git directory 
Directory Area (Repository) 

Checkout the project 

Stage Fixes 

Figura 1. Fluxo geral dos arquivos na estrutura de projeto. 

Fonte: Git (20103, documento on-line). 


Criação de branc nes e commis como Gu) 47 

Como visto na Figura 1, o Git checará se existem alterações nos arqui-
vos do projeto, os quais são editados pelos usuários no diretório de trabalho 
(Working Directory). Ao verificar os arquivos que foram identificados com o 
status modified, estes podem ser encaminhados e preparados para fazer parte 
do próximo commit do projeto. Para isso, os arquivos alterados no projeto são 
marcados o status staged, passando a ser listados em um controle interno do 
Git, denominado Staging Area (SANTACROCE, 2015). Essa área de controle 
é, basicamente, um arquivo de índice presente no diretório do repositório 
(diretório Git), no qual está presente a relação de arquivos do diretório de 
trabalho que estarão no próximo commit. Ao efetivar o processo de commit, 
os arquivos passam a ser identificados com o status committed, sendo arma-
zenados no diretório Git. 

A partir do momento em que o commit é realizado, os arquivos são incluídos 
no snapshot feito pelo Git. Todos os arquivos de projeto presentes no snapshot 
gerado são tratados pelo Git com um controle interno, que marca esses arquivos 
como rastreados. Conforme Chacon e Straub (2014), todos os outros arquivos 
do diretório de trabalho que não estão incluídos nesta última “foto” ou que não 
constam com o status staged são considerados pela ferramenta como arquivos 
não rastreados (untracked). 

Saiba mais 

Segundo Chacon e Straub (2014), o Git trata como arquivo não rastreado todo novo 
arquivo criado no diretório de trabalho de um repositório existente. Além disso, quando 

um novo repositório de projeto é criado ou iniciado, todos os arquivos presentes no 

diretório de trabalho são considerados como não rastreados pela ferramenta. 

O uso de branches no Git 

As ferramentas de controle de versão de código-fonte para o gerenciamento de 
arquivos de um projeto possuem um conceito denominado branch. Segundo 
Silverman (2013), um branch é uma cópia de todo o conteúdo de uma versão 
específica do projeto criada para ser acessada por outras pessoas ou equipes, 
sem afetar a estrutura principal do repositório. 



as ) ( Criação de branches e commits com o Git 

Assim, dentro do projeto, é criada uma ramificação idêntica do conteúdo 
dos arquivos do projeto, para que possam ser acessados, modificados e rece-
ber commits de forma independente do conteúdo original. Segundo Chacon 
e Straub (2014), todo branch dentro do Git funciona como um ponteiro de 
controle, apontando para o último commit realizado no conteúdo. A partir do 
momento em que for executado um novo commit, esse ponteiro moverá, de 
forma automática, a exibição do histórico de controle para esse novo registro. 

Lembre-se de que todo commit realizado no Git cria um snapshot do conte-
údo no momento em que foi armazenado. Ao se manipular os mesmos arquivos 
de um repositório em diferentes branches, cada modificação é realizada de 
forma isolada, embora seja feita no mesmo conteúdo. Com isso, durante um 
fluxo de trabalho do projeto, as alterações realizadas pela equipe em um branch 
adicional podem ser mescladas, posteriormente, com o branch principal do 
controle de versão do projeto. A Figura 2, a seguir, apresenta um exemplo 
simples de como uma estrutura de branches pode ser utilizada em um projeto. 

a a 

Figura 2. Exemplo de utilização de branches em um projeto. 

Fonte: Atlassian (2020, documento on-line). 
A x 


Criação de branches e commits com o st) ( 49 

O exemplo da Figura 2 mostra como o Git pode ser utilizado com diferentes 
fluxos de trabalho dentro de um mesmo projeto, por meio das ramificações 
dos históricos registrados das atividades com um determinado código. Neste 
caso, o branch Master é responsável pelos registros das versões específicas 
do projeto ao longo do tempo, contendo, assim, todos os commits realizados 
nos lançamentos oficiais do sistema em questão. Ao mesmo tempo, o branch 
identificado como Develop mostra o histórico dos commits realizados pela 
equipe de desenvolvimento do código ao longo do tempo, como, por exemplo, 
a adição de funcionalidades ao sistema ou a correção do código existente. 
No modelo citado, essa organização de branches possibilita que o time res-
ponsável pelo desenvolvimento tenha acesso para baixar e sincronizar todo 

o conteúdo do repositório, a fim de manter os arquivos rastreados dentro do 
branch Develop. 
Fique atento 

Por padrão, em qualquer repositório criado no Git, é adicionado um branch denominado 
master. 

A forma como o Git trabalha torna mais simplificada a utilização de di-
ferentes branches ao longo do projeto. Durante a utilização da ferramenta, 

o branch que está sendo utilizado pelo usuário para a realização dos commits 
é identificado com um ponteiro interno especial no Git, denominado HEAD. 
Desse modo, ao criar um novo branch, é possível identificar se ele está sendo 
utilizado para o registro dos commits naquele momento ao identificar a presença 
do ponteiro HEAD em seu status. 

50 ) (ração de branches e commits com o Git 
3 Utilizando o Git com os arquivos e 
repositórios 

Após a instalação da ferramenta de controle de versão de código-fonte em um 
ambiente de trabalho por meio da linha de comando (ou por meio da interface 
gráfica, conforme a preferência de instalação), o Git pode realizar diversos 
métodos de gerenciamento de conteúdo, como a criação de repositórios e a 
manipulação de arquivos, commits e branches. 

Antes de executar propriamente os comandos, faz-se necessário entender 
como os arquivos são manipulados no diretório de trabalho em relação ao 
fluxo de controle através do Git. A Figura 3, a seguir, apresenta um exemplo 
de como os arquivos do projeto e seus respectivos status dentro do controle 
de versão do Git são referenciados. 

Untracked 

Add the file 

Edit the file 
Stage the file 

Remove the file 

Figura 3. Controle dos status dos arquivos dentro do Gtt. 

Fonte: Git (2020b, documento on-line) 

Conforme observado na Figura 3, os arquivos não rastreados dentro do 
diretório de trabalho (p. ex., arquivos novos no diretório ou que não estavam 
no último commit) podem ser adicionados como preparados para o próximo 
commit, recebendo o status staged. Ao realizar o commit, os arquivos são 
armazenados no Git, passando a ser monitorados em relação às modificações 
em seu conteúdo. Se o arquivo rastreado tiver novas alterações, o seu status 
fica como modified, sendo necessária a adição desse arquivo como preparado 
para o próximo commit; se não houver nenhuma modificação, o arquivo fica 
disponível no diretório de trabalho, inclusive para ser removido do conteúdo 
do projeto. 



branches o

Criação de e commits comG1 ) [ 51 

Com a divisão de um projeto no Git entre equipes diferentes, uma das 
vantagens do controle de versão dos arquivos é a utilização do conceito de 
mesclagem de conteúdo (definido como merging). Por exemplo, um deter-
minado código-fonte pode ser alterado pela equipe de desenvolvimento em 
um branch antes de entrar em produção. Para que o arquivo alterado seja 
carregado no branch responsável por deixar o código em produção, a equipe 
responsável pelo branch de produção realiza uma mesclagem com o conteúdo 
alterado no branch de desenvolvimento, mantendo o arquivo final com as 
últimas alterações. 

Em virtude da possibilidade de os usuários atuarem em branches diferentes 
para modificações em paralelo nos arquivos de um projeto, pode ocorrer uma 
alteração concorrente em um mesmo conteúdo. Por exemplo, um desenvolvedor 
adiciona uma linha de código em um arquivo index. html no branch mas-
ter, porém outro desenvolvedor, realizando modificações no branch testing, 
apaga uma linha de código no mesmo arquivo do repositório. O Git tentará 
identificar e realizar de forma automática a resolução de conflitos entre as 
versões ao executar o comando para mesclagem entre um branch e outro, 
ogit merge. No entanto, podem ocorrer situações em que esse conflito não 
é resolvido diretamente, sendo necessário identificar e corrigir manualmente 
as modificações. 

Na linha de comando, o exemplo de conflito no arquivo index. html 
poderia ser diagnosticado ao se tentar utilizar o comando git merge a 
partir do branch master para mesclar as alterações realizadas no conteúdo 
do branch testing: 

4 git merge testing 

A Figura 4, a seguir, apresenta a mensagem de erro será mostrada. 

F gt m t

erging is not possible se you have unmerged files 

fatal: Exiting beca an unreso 

Figura 4. Saída do comando git merge. 


52) 

Nessa situação, é possível verificar o status dos arquivos para detectar 
qual conteúdo está sendo afetado pelo conflito de versões entre os branches. 
Para isso, pode-se utilizar o comando para a verificação do status, o git 
status, da seguinte forma: 

* git status 
A Figura 5, a seguir, apresenta a mensagem sobre o conflito entre dois 
arquivos modificados (em ambos os branches). 

the merge) 

resolution) 

ges added to commit (use "“git add” and/or " 

Figura 5. Mensagem de detecção de conflito entre conteúdo. 

Neste caso, faz-se necessário intervir manualmente no conteúdo, identi-
ficando onde está o conflito, a fim de gerar a correção adequada. Para isso, 
é possível utilizar o comando git diff no arquivo indicado (neste caso, 

o index.html)c analisar quais informações estão diferentes entre as versões 
modificadas: 
4 git diff index.html 

A Figura 6, a seguir, apresenta a saída do comando por meio do qual o 
Git insere uma marcação de detecção de conflito para as versões em cada 
branch, representada pelos caracteres “<<<<<<<” e “>>>>>>>". As alterações 
detectadas são separadas pelos caracteres “ 



rootQlpidevops:f git diff index.html

diff --cc index.html 

c29f1c7,15980

ndex. html 

<body> 
<hl>Teste de HTML</hl> 

</bod) 

Figura 6. Detecção de conflito como git diff. 

> 

Ao lado dos caracteres “<<<<<<<” é identificado o ponteiro HEAD, 

referente ao branch atual (neste caso, o master). Todo o conteúdo HTML 
(HyperText Markup Language; ou Linguagem de Marcação de Hipertexto, 
em português) do arquivo mostrado até os caracteres “= =” são as 
informações a mais (em verde) que a versão do index. html nesse branch 
traz em relação à versão mesclada com o branch testing. Lembre-se de que, 
no exemplo citado, no branch testing, o desenvolvedor havia removido uma 
linha (provavelmente a indicada no código HTML com o título para o site). 

A partir dos caracteres “>>>>>>>", é indicado o conteúdo no branch testing 
(em branco) que é idêntico ao encontrado no mesmo arquivo no branch master. 
Com a identificação da informação que está em conflito, o desenvolvedor 

pode editar manualmente o arquivo index . html no branch mastere verificar 
se a linha citada deve ser mantida ou removida para resolver o problema da 
mesclagem. 

Fique atento 

Ao editar novamente o arquivo para a correção manual de conflitos no Git após definir 

o conteúdo da mesclagem, faz-se necessário remover os caracteres de marcação 
(<ee<e<<", =" €">>>>>>>". 

54) (ciação de branches e commitscom o Git 
Comandos básicos do Git 

A primeira coisa a ser feita com a ferramenta é iniciar o uso de um repositório 
Git. Para isso, faz-se necessário definir o diretório de trabalho, na máquina 
local, para a utilização dos arquivos. 

ue atento 

Os comandos internos do Git funcionam em qualquer uma das plataformas suportadas 
pela ferramenta. Nos exemplos adotados neste capítulo, serão feitas referências à 
execução do Git em um ambiente com Linux. 

Para iniciar um novo repositório vazio, utiliza-se o comando git init. 
Em um diretório de trabalho definido na máquina em /home/user/pro-
jeto, deve-se executar o seguinte comando: 

4 cd /home/user/projeto 

4 git init 

A Figura 7, a seguir, apresenta o resultado do comando. 

Figura 7. Saída do comando git init 

Essa sequência de comandos criará o repositório Git dentro do diretório de 
trabalho, com toda a estrutura para iniciar o controle de versão. O diretório 
Git será identificado como um subdiretório, denominado .git, dentro de 
/home/user/projeto (no caso do exemplo citado). 



pm o Git )(ss 

Em vez de iniciar um novo repositório vazio, outra forma seria baixar a 
cópia de um repositório existente em um servidor remoto. Se essa for a opção 
escolhida, o comando a ser utilizado no diretório de trabalho criado é o gi t 
clone, indicando a URL remota do repositório. Com isso, a estrutura do 
repositório remoto (arquivos do projeto e o diretório de controle .git) será 
copiada para a máquina local. Para isso, é só executar o seguinte comando: 

£ git clone <url remota do projeto> 

Assim, tanto por meio da criação de uma nova estrutura de repositório com 
ogit init como por meio da cópia de um projeto existente com o git 
clone, o diretório de trabalho possui uma estrutura para versionamento dos 
arquivos existentes em seu conteúdo. Para verificar os status dos arquivos do 
diretório de trabalho dentro do Git, pode-se utilizar o comando git status 
da seguinte forma: 

£ git status 

A Figura 8, a seguir, apresenta o resultado do comando com o status. 

bn 
tQlpidevops:
branch ma: 
/home/user/projetor git status 
No commits yet 
Juntracked file:
(use "git add <fi --" to include in what will be committed) 

commit but untracked files pres se "git add" to track) 
Figura 8. Saída do comando git status. 


56) ( Criação de branches e commits com o Git 
Esse comando exibirá todos os arquivos não rastreados, modificados e 
preparados para o próximo commit. Para adicionar novos arquivos criados ou 
editados no diretório de trabalho para serem rastreados pelo Git, utiliza-se o 
comando git add, indicando o(s) nome(s) do(s) arquivo(s) ou diretório(s) 
a serem preparados para o próximo commit. Assim, é só executar o seguinte 
comando: 

4 git add README.txt 
4 git add *.html 

Para organizar a visualização das versões, é interessante utilizar marcações 
e identificadores ao longo do histórico de commits, adicionando uma tag. 
Conforme Santacroce (2015), as tags fazem referência a alterações que podem 
ser consultadas futuramente, servindo como uma documentação das versões 
disponibilizadas ao longo do projeto. O comando git tag é responsável por 
adicionar ou consultar as tags utilizadas ao longo dos commits. Para consultar 
as tags existentes, basta executar o seguinte comando: 

* git tag 
Para criar uma nova tag, o comando citado deve ser acompanhado com 
as opções -a (criar uma nova tag de versão) e -m (comentário adicionado à 
tag a ser criada): 

4 git tag -a v1.0 -m “Versão 1.0 do sistema” 

Os arquivos referenciados com o git add estão marcados como pre-
parados para o próximo commit dentro do projeto (status staged). Com isso, 
pode-se utilizar o comando git commit para armazenar os arquivos no 
diretório Git. Para isso, é só executar o seguinte comando: 

£ git commit -m “Primeiro commit do projeto” 

A opção -m indica um comentário a ser realizado durante a realização do 
commit. Isso é importante para o registro do histórico das modificações ao 
longo do projeto, pois indica o que foi realizado naquele momento. 



Criação de branchescommits com o Git )(

e 57 

Fique atento 

Pode ocorrer uma situação em que um arquivo tenha sido preparado via comando 
git add para fazer parte do próximo commit, porém, antes da execuçãodo comando 
git commit, ele tenha sido novamente modificado. Assim, o status do arquivo para 
O Git estará como modified, sendo necessário realizar novamente o comando git. 
add com o mesmo arquivo,a fim de que as novas alterações sejam armazenadas no 
próximo commit. 

Se um arquivo for removido do diretório de trabalho, no status do diretório 
Git, ele irá aparecer como um conteúdo a ser excluído do repositório. Para que o 
arquivo seja removido do repositório no próximo commit, utiliza-se o comando 
git rm, indicando o(s) nome(s) do(s) arquivo(s) a ser(em) removido(s). Para 
isso, basta executar o seguinte comando: 

£ git rm robots.txt 

Assim, no próximo git commit, o arquivo referenciado será removido do 
repositório do projeto. Além disso, é possível retirar o status staged de arquivos 
preparados para o próximo commit, como, por exemplo, em uma situação em 
que um arquivo modificado no diretório de trabalho foi adicionado de forma 
incorreta via git add para fazer parte do próximo commit. Para isso, é só 
utilizar o comando git reset, indicando o ponteiro de controle do Git para 

o branch atual e o nome do arquivo que será necessário retirar da staging area. 
No caso de um arquivo index. html que foi adicionado acidentalmente 
à staging area viagit add, ele deve ser removido via git reset da lista 
de arquivos preparados para o próximo commit. Para isso, basta utilizar o 
seguinte comando: 

git reset HEAD index.html 



58) (criação de branches e commts com o Git 
Referências 

ATLASSIAN. Gitflow workflow. [2020]. Disponível em: https:/Awww atlassian.com/git/ 
tutorials/comparing-workflows/gitflow-workflow. Acesso em: 27 ago. 2020. 

CHACON, S.; STRAUB, B. Pro Git: everything you need to know about Git. 2nd ed. New 
York: Apress, 2014. 

GIT. 1.3 getting started: what is Git? [2020a]. Disponível em: https://git-sem.com/book/ 
en/v2/Getting-Started-What-is-Git%3F. Acesso em: 27 ago. 2020. 

GIT. 2.2 Git basics: ecording changes to the repository. [2020b]. Disponível em: https:// 

git-scm.com/book/en/v2/Git-Basics-Recording-Changes-to-the-Repository. Acesso 

em: 27 ago. 2020. 

HUMBLE, J.; FARLEY, D. Entrega contínua: como entregar software de forma rápida e 
confiável. Porto Alegre: Bookman, 2014. 

SILVERMAN, E. Git: guia prático. São Paulo: Novatec, 2013. 

SANTACROCE, F. Git essentials. Birmingham: Packt Publishing, 2015. 

Leituras recomendadas 

NAREBSKI, J. Mastering Git. Birmingham: Packt Publishing, 2016. 

PIPINELLIS, A. GitHub essentials. Birmingham: Packt Publishing, 2015. 

Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 

J 


Integração da equipe 
de Devs com o Git 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Identificar a estratégia de organização do versionamento do código-
-fonte por meio do Gitflow. 
m. Ilustraro método de trabalho conjunto de uma equipe com o versio-
namento de código-fonte. 
Em Demonstrar a resolução de conflitos de versionamento. 

Introdução 

A realidade de trabalho de um desenvolvedor de software requer cons-
tante cooperação, seja com profissionais de infraestrutura e de projeto, 
seja com seus pares de desenvolvimento. São comuns as situações em 
que um desenvolvedor é contratado para atuar em uma equipe que está 
fisicamente separada, exigindo desse profissional que seu código-fonte 
se encaixe perfeitamente ao dos demais desenvolvedores, como se todos. 
estivessem trabalhando lado a lado. 

Neste capítulo, você verá como organizar a estrutura de versiona-
mento do código-fonte de uma aplicação. Além disso, verá como uma 
equipe de trabalho pode atuar nessa estrutura. Por fim, conhecerá os 
principais problemas que podem ocorrer no versionamento de código-
-fonte quando se trabalha em equipe. 



60) ( Integração da equipe de Devs com o Git 
1 Gitflow: organizando o versionamento 
de código-fonte 

O desenvolvimento de um software, de um website ou mesmo de uma aplicação 
para smartphones é uma atividade complexa, pois envolve pessoas, tecnologias 
e processos. Ao mesmo tempo que cria oportunidades para o surgimento de 
novos sistemas e aplicativos, o avanço da ciência computacional aumenta a 
variedade de linguagens e de tecnologias envolvidas no desenvolvimento 
desses sistemas. 

Metodologias de desenvolvimento de software começaram a surgir na 
década de 1960 para estruturar o desenvolvimento de software, organizando 
a formação do time de desenvolvimento, o planejamento e a execução da 
fase de desenvolvimento, bem como a organização da comunicação entre os 
desenvolvedores e com os times externos. Dependendo da metodologia apli-
cada no desenvolvimento de um software, a quantidade e os nomes das etapas 
podem variar, mas ainda assim seguirão uma sequência lógica e organizada. 
Em um contexto de desenvolvimento de aplicações, é provável se deparar com 
nomes como metodologia em cascata, metodologia de prototipação, desenvol-
vimento rápido, DSDM (dynamic system development model; ou metodologia 
de desenvolvimento de sistemas dinâmicos, em português), metodologia ágil, 
modelo em espiral, XP (extreme programming; ou programação extrema, em 
português), RUP (rational unified process; ou processe unificado rational), 
Scrum, Lean, entre outros (WAZLAWICK, 2013). 

No entanto, em todas as metodologias, é comum a preocupação com o 
desenvolvimento e a integração do código-fonte, gerado por equipes que podem 
estar em um mesmo departamento ou até mesmo em continentes diferentes. 
A metodologia de desenvolvimento de software adotada organiza o que será 
desenvolvido e quando será desenvolvido. Mas como desenvolver uma apli-
cação em equipe sem que seu trabalho sobreponha o do seu par, cumprindo 
Os prazos e os objetivos da entrega? 

O sistema de controle de versionamento distribuído Git fornece recursos 
tecnológicos que permitem a codificação e o versionamento por equipes; 
no entanto, por si só, ele não garante a organização do trabalho dessas equipes. 
Assim, faz-se necessário um fluxo de trabalho simples, bem estabelecido e que 
seja facilmente compreendido pela equipe de desenvolvimento. Além disso, 
esse fluxo deve atender às demandas por integração contínua, presentes nas 
práticas DevOps (MUNIZ et al., 2020). 



Integração da equipe de Devs com o Git ) (e 

Controlando versões no processo de 
integração contínua 

O processo de integração contínua é fundamental na filosofia DevOps, e pode 
ser entendido como a prática de submeter pequenas peças de código para o uso 
em produção. A ideia da integração contínua é parte da metodologia de de-
senvolvimento XP, proposta inicialmente por Kent Beck (COPELAND, 2001). 

No processo de integração contínua, os códigos desenvolvidos individual e 
localmente por cada desenvolvedor são testados automaticamente e validados 
antes de serem disponibilizados para o uso. Esse processo de integração requer 
liberações de código com frequências diárias, a fim de evitar grandes pacotes 
de atualização que causem maiores impactos na aplicação e que requeiram 
uma grande janela de tempo para a sua implantação em produção. 

Segundo Muniz et al. (2020), existem três conceitos principais do processo 
de integração de versões: 

E Linha principal, trunk ou master: local onde ficará armazenado o 
código principal da aplicação, a ser posteriormente disponibilizado 
para produção. 
EB Ramificação (branching): os ramos (branchs) são as cópias do trunk 
em um determinado momento, os quais permitem que linhas indepen-
dentes de desenvolvimento sejam mantidas e, posteriormente, fundidas 
(merging) com a linha principal. 
m Marcação (tagging): o estado de um código deve ser identificado por 
uma marca (tag), que geralmente identifica a versão do código. Por 
exemplo: a tag vl.1.14. 

A Figura 1, a seguir, ilustra o controle de versionamento de um código. 

Ao concluir um desenvolvimento realizado em um ramo (branch), um de-
senvolvedor precisará integrá-lo ao restante do código. Esse processo consiste 
na sincronização das modificações realizadas em um determinado ramo com 

o código fonte do ramo principal. Essa ação é chamada de fusão, integração 
ou merge, e é muito comum em projetos com muitos branches. 
A complexidade do processo de merge dependerá, entre outras variáveis, 
do tempo que o código não está sendo integrado ao ramo principal. A quanti-
dade de alterações sendo simultaneamente desenvolvidas, em ramos distintos, 
aumentará a complexidade do processo de merge, com a ocorrência de conflitos 
de versionamento e a possibilidade da introdução de bugs (MUNIZ et al., 2020). 



62) ( Integração da equipe de Devs com o Git 
N 

master | Últimos ajustes 

Continuação desenvolvimento... 

novo-branch | Commit de um novo branch em paralelo 

hotfix/1.0.1 || 1.01 | 8ug 1.90 finalmente resolvido 

Tentando resolver problema 1.0.1 

Melhorias para a próxima versão 

1.0.0 | Primeira versão para reprodução 
Primeiro commit 

Figura 1. Controle de versionamento, com destaque para master, branches e tag. 
Fonte: Adaptada de Muniz et al (2020) ) 

O modelo Gitflow 

Para responder ao questionamento de como desenvolver em equipe com o 
Git, Driessen (2010) apresentou uma estratégia de ramificação (branching) 
e um modelo de gerenciamento de versão, chamado posteriormente de Gitflow, 
amplamente adotado por equipes locais ou distribuídas. O Gitflow baseia-
-se nos conceitos de repositório, ramificação (branching) e fusão (merging) 
apresentados no Git (CHACON; STRAUB, 2014), definindo uma convenção 
para nomes de ramos (branchs) e estabelecendo regras para a ramificação e a 
fusão entre eles. O Quadro 1, a seguir, apresenta os tipos de ramos e as regras 
definidas no Gitflow. 



(onunuoo) 

doTensp Jeqseu doTsAsp Jsqseu -

opônpoid opônpoid 
QUA [ONUBSSp. eJed opesag|| Jas esed eJed opesagj| Jas sed 
WD J9AS9 Quenbu3 ouugul quoud efajsa anb ay ojuold efaisa anb ay Ouuu] 

»-XTI30U

',-9SPoTOI'doTenap '1º3sew

OJ2oxa “SAM -«-SseoTaI *-XT330U E 

eumany asDajas eum aspajas | (una) oouon 10d 
eJed no euuixgud e eLuIXgId ejad Jesadsa opbayuoo Lusa 
eJed 'opipjuatuajduu asbajas euuIxoId opinpod apod ogu anb 'o2nu> -uWe| “opônpoid 
esou eu | eesedesando eJed euanbo 6nq un ap opóauos tu B3sa anb O 

[2207 (|2nU92) utbuo (Jequao) usôuo j220| (jenuao) utbuo 

MOINI9 OU SEppajageisa selbau 2 soe! ap sodi| *| ospend 

A 

Integração da equipe de Devs com o Gt) (es 

Clique aqui para visualizar este conteúdo na horizontal. 



sa) (Integração da equipe de Devs com o Git 

EN 

doTsasp 
(ermoa)) UJOD 'OgUA “9 TSJs eu doTsAsp ui 
opÍejuaua|dtui enou UJOD OJtatuyd *ogjua 'a O SPU OD | oesIBA BAOU UN 
epeo eJed owes wir) -OpIpuny1as 34d] | Ouawnd opipunyias ansg | pJeob ogsny epes 

= -CL SELL PULL 
aspatoradoTaasp eseoToI 
doTarsp 'szngeez doTsaspo 1egsew adoTsAsp'193seu -

“OLOZ)

UaSSaUg

ap

opeidepy

:aquoy

MO!

OU

Sepioajageisa

sesba)

à

sowes

ap

sodi|

“|

ospend

Clique aqui para visualizar este conteúdo na horizontal. 



Integração da equipe de Devs com o st) [ss 

2 Trabalhando em equipe: versionando 

o código-fonte 
Após conhecer os tipos de ramos e as regras que limitam o seu uso, pode-se 

pensar em como o desenvolvimento do código-fonte se organiza no Gitflow. 
Os tipos de ramos apresentados no Quadro 1 não são diferentes ou especiais, 
mas se diferenciam pela forma como são utilizados. A seguir, cada tipo de 
ramo é apresentado em mais detalhes. 

Ramos fundamentais 

No padrão Gitflow, os ramos fundamentais são apenas dois: master e de-
velop. O ramo master existe apenas no repositório central, identificado 
por origin (consulte o comando git remote, na documentação do Git), 
e sempre conterá o código-fonte que foi liberado para uso em produção. 
A versão disponível no ramo master é necessariamente identificada por uma 
tag, que define em qual versão (release) o código-fonte está. Por exemplo, 
a release do código-fonte do ramo master pode ser 1.1.14. Sempre que o 
ramo master for fundido com mudanças que vierem dos ramos release 
ou hotfix, a sua versão necessariamente deverá ser alterada. Por exemplo, 
a versão do ramo master é alterada para 1.1.15 ao receber um hotfix, ao passo 
que é alterada para 1.2 ao receber a release identificada por release-1.2. 

O ramo develop, por sua vez, é criado inicialmente como um ramo 
(branch) do ramo master, e receberá daí em diante os códigos-fonte dos 
desenvolvimentos e as correções dos bugs que forem produzidos nos ramos 
feature. Todas as implementações e correções de bug que estiverem no 
ramo develop farão parte da próxima release a ser liberada para produção. 

O ramo develop é limpo após o seu conteúdo ser liberado para o ramo 
release, e não receberá códigos-fonte dos ramos feature até que o conte-
údo do ramo release seja liberado para produção, isto é, seja fundido com o 
ramomaster. É importante ressaltar que, antes de começar a receber as novas 
implementações e correções de bug, o ramo develop deve receber (merge) 

o conteúdo do ramo release, para que as implementações que acabaram 
de ser liberadas para produção se reflitam nos próximos desenvolvimentos e 
correções de bug (DRIESSEN, 2010). 

66 ) (Imegração da equipe de Devs como Git 
Exemplo 

Para adotar o Gitflow, comece pela criação do ramo develop, a partir do ramo 
master, no repositório origin: 

$ git checkout -b develop master 

Switched to a new branch 'develop" 

Ramo feature 

É um tipo de ramo criado a partir do ramo develop, com a finalidade especi-
fica de receber o desenvolvimento de uma nova funcionalidade para o software 
ou aplicação, a ser incorporada à próxima versão (release) específica, ou a uma 
versão futura. Esse tipo de ramo é utilizado também para o desenvolvimento 
de correções de erros (bugs) previamente documentados, cuja liberação para 
produção possa ocorrer em uma release futura (DRIESSEN, 2010). 

Uma característica que diferencia o ramo feature dos demais é o fato 

de que podem existir diversos ramos desse tipo simultaneamente, um para 

cada nova funcionalidade ou para cada bug a ser corrigido. Cada ramo recebe 

o código-fonte sendo produzido por um desenvolvedor ou por uma equipe de 
desenvolvedores. O nome do ramo deve seguir um padrão de nomenclatura 
que identifique unicamente a funcionalidade ou bug, e jamais pode receber 
o nome dos ramos fundamentais (master e develop) ou adotar o nome 
de uma release ou de um hotfix. Por exemplo, um ramo (branch) do tipo 
feature pode ter o nome feature-7, ao passo que outro ramo pode se 
chamar bug-10. 
Fique atento 

Se uma equipe de desenvolvedores trabalhar na mesma funcionalidade ou no mesmo 
bug, os desenvolvedores devem configurar seu Git para apontar para o repositório de 
seus pares, a fim de acompanhar o desenvolvimento. 


Integraçãoda equipede Devs com o Git )[ 67 

Um ramo do tipo feature durará enquanto a funcionalidade ou correção 

de bug estiver em desenvolvimento, sendo excluído após ser fundido com o 
ramo develop ou se for rejeitado por ser reprovado em testes (DRIESSEN, 
2010). 

Ramo release 

Assim como os ramos do tipo feature, os ramos do tipo release são 
criados a partir do ramo develop, mas com finalidade e efeitos diferentes. 
Ao contrário de um ramo feature, um ramo release é um tipo de ramo 
utilizado especificamente para preparar uma nova versão da aplicação ou do 
software a ser liberada para produção (DRIESSEN, 2010). 

Quando um ramo do tipo release é criado, ele recebe uma tag que 
identifica a sua versão (release). No momento de criação desse ramo, 

o ramo develop deve ser limpo, para que não receba novas funcionalidades. 
As funcionalidades ou correções de bug que ainda estejam em desenvolvimento 
e que não fazem parte do ramo release atual devem aguardar a liberação 
do ramo develop. 
O ramo release é identificado pelo padrão release-*, em que 0 * 
corresponde à sua versão, a mesma atribuída pela tag. Por exemplo, um ramo 
do tipo release pode ser criado com o nome release-1.2 e receber a 
tag v1.2. O ramo release é excluído após ser fundido aos ramos master 
edevelop, para que futuros desenvolvimentos incluam o que já foi liberado 
para produção. 

Ramo hotfix 

Criado exclusivamente a partir do ramo master, esse tipo de ramo não é 
planejado, pois é criado para conter código-fonte de correção de um bug crítico 
do software que está em produção. Um bug crítico em produção não pode 
esperar a liberação de um novo release, pois requer ação de correção imediata. 

De maneira semelhante a um ramo do tipo feature, mais de um ramo 
de hotfix pode ser criado simultaneamente, cada um para conter códigos-
-fonte em desenvolvimento para a correção de um bug crítico específico. Para 
diferenciar entre os ramos do tipo hot fi x, adota-se o padrão de nomenclatura 
hotfix-+, identificando unicamente o hotfix. Por exemplo, um ramo hot fix 
pode se chamar hot fix-05. 



68) ( Integração da equipe de Devs com o Git 
O ramo hot fix é excluído após ser fundido (merge) aos ramos master 

e develop, para garantir que a correção esteja também na próxima release 

(DRIESSEN, 2010). 

Exemplo 

Quando um ramo release é fundido ao ramo master, um novo commité gerado 
no ramo master, e uma tag de versão é atribuída a ele: 

$ git checkout master 

Switched to branch 'master! 

S$ git merge --no-ff release-1.1 

Merge made by the 'recursive! strategy. 
arquivo3.c | 0 
1 file changed, O insertions(+), O deletions(-) 

create mode 100644 arquivo3.c 

$ git tag -a vl.l -m "versao vl.1" 
a AM 

3 Trabalhando em equipe: resolvendo conflitos 

O desenvolvedor deverá adotar alguns cuidados ao trabalhar em equipe, 
os quais, quando respeitados, diminuirão a chance de problemas com seus pares 
de desenvolvimento. Como boas práticas de versionamento, o desenvolvedor 
deve (CHACON; STRAUB, 2014): 

m Garantir que suas submissões não contenham erros de espaço em branco 
(trailing whitespace). Esses erros ocorrem quando o código-fonte contém 
linhas compostas apenas de espaços em branco, ou quando a indentação 
de uma linha contém um espaço em branco antes de um caractere TAB. 

Assim, deve-se utilizar o comando git diff --check antes de 

um comando commit para identificar a presença de erros de espaço 
em branco. 



Integração da equipe de Devs com o Git ) (eo 

m Garantir que cada commit contenha uma unidade logicamente separá-
vel, isto é, contenha apenas uma nova funcionalidade ou apenas uma 
correção de bug. 
m Adotar o hábito de incluir mensagens de qualidade para um commit. 
Recomenda-se que a mensagem inicie com uma linha de, no máximo, 
50 caracteres que descreva a implementação de maneira concisa, 
seguida por uma linha em branco e, então, por um texto mais completo 
sobre a implementação. Nesse texto, deve-se incluir a motivação para o 
desenvolvimento e descrever em que essa implementação se diferencia 
da anterior. 

A adoção de boas práticas de versionamento melhora o relacionamento entre 
os desenvolvedores e aumenta a qualidade do trabalho, mas não os poupa dos 
conflitos de versionamento. Esse tipo de conflito ocorrerá frequentemente ao 
se submeter um objeto a um repositório compartilhado (origin ou repositório 
do par) ou ao se fundir (merge) um desenvolvimento a um ramo. 

Fowler (2006) já apontava para a ocorrência de conflitos de versão de 
código-fonte ao apresentar seus princípios de integração contínua. O autor não 
somente cita a possibilidade desses conflitos, mas também delineia o método 
a ser utilizado no Git em caso de conflitos: 

Com uma boa compilação, posso pensar em confirmar minhas alterações no 
repositório. A reviravolta, é claro, é que outras pessoas podem, e geralmente 
fizeram, alterações na linha principal antes que eu tenha chance de confir-
mar. Então, primeiro atualizo minha cópia de trabalho com as alterações e 
reconstruo. Se as alterações deles colidirem com as minhas alterações, ele 
se manifestará como uma falha na compilação ou nos testes. Nesse caso, 
é minha responsabilidade corrigir isso e repetir até que eu possa criar uma 

cópia de trabalho que esteja sincronizada corretamente com a linha principal. 

(FOWLER, 2006, documento on-line). 

Para ilustrar o procedimento básico de resolução de conflitos de versio-
namento, confira, a seguir, a adaptação de um dos cenários propostos por 
Chacon e Straub (2014). 



70 ) ( Integração da equipe de Devs com o Git 
Exemplo 1 

Nesse cenário, dois desenvolvedores, DEV A e DEVB, trabalharão separada-
mente no desenvolvimento de uma nova implementação, seguindo o padrão do 
Gitflow. Essa nova implementação requererá a inclusão de dois novos arquivos 
e receberá o nome de feature-15. Cada arquivo dessa implementação será 
codificado por um desenvolvedor em seu repositório local do Git. Observe que, 
inicialmente, cada desenvolvedor clonará para a sua máquina A

(DEVLOCAL 
eDEVLOCAL) o repositório central (REPOSITORIO):

B 

$ git clone REPOSITORIO DEV A LOCAL 
Cloning into 'DEV A JLOCAL'... 
done. 

$ git clone REPOSITORIO DEV B LOCAL 

Cloning into 'DEV B LOCAL'... 

done. 

Ao clonar o repositório central, o ramo develop também foi clonado, 
incluindo todos os arquivos que faziam parte desse ramo no momento do clone. 
Os arquivos que foram clonados do ramo develop do repositório central 
devem existir também no ramo develop do repositório local do desenvolve-
dor, para que ele possa codificar e testar a sua parte da nova funcionalidade. 

No próximo passo, o desenvolvedor A (DEVA) criará um novo ramo a 
partir do ramo local develop, para, então, codificar a sua parte da nova 
implementação, que consistirá no arquivo arquivo2 . c. Confira esses passos 
a seguir: 

$ git branch feature-15 
$ git checkout feature-l5 

Switched to branch 'feature-15! 

$ vi arquivo2.c 

$ git add arquivo2.c 
warning: LF will be replaced by CRLF in arquivo2.c. 



Integração da equipe de Devs com o st) ( n 

The file will have its original line endings in your working 
directory 

S$ git commit -a -m "feature-15 -adiciona arquivo2.c" 

[feature-15 c87847£] feature-15 -adiciona arquivo2.c 

1 file changed, 1 insertion(+) create mode 100644 arquivo2.c 

Observe que, após criar a sua parte da nova implementação, o desenvolvedor 
A(DEVA) adiciona o novo arquivo ao Git(git add) cria um novo commit. 
Da mesma forma, o desenvolvedor B (DEVB) trabalhará em sua parte da 
implementação, criando um arquivo com seu código-fonte (arquivo3.c). 
Para isso, o desenvolvedor criará um ramo a partir do ramo develop do seu 
repositório local e, então, criará e adicionará (git add) o novo arquivo ao 
repositório Git. Confira esses passos a seguir: 

$ git branch feature-15 
$ git checkout feature-15 
Switched to branch 'feature-15! 

$ vi arquivo3.c 

$ git add arquivo3.c 
warning: LF will be replaced by CRLF in arquivo3.c. 
The file will have its original line endings in your working 
directory 

$ git commit -a -m "feature-15 -adiciona arquivo3.c" 

[feature-15 aal2e87] feature-15 -adiciona arquivo3.c 
1 file changed, 1 insertion(+) 
create mode 100644 arquivo3.c 

$ git checkout develop 
Switched to branch 'develop' 
Your branch is up to date with 'origin/develop"'. 

$ git merge feature-15 
Updating 845cfd9..aal2e87 

Fast-forward 



72 ) ( Integração da equipe de Devs com o Git 
arquivo3.c | 1 + 
1 file changed, 1 insertion(+) 

create mode 100644 arquivo3.c 

S git push origin develop 
Enumerating objects: 4, done. 
Counting objects: 100% (4/4), done. 
Delta compression using up to 4 threads 
Compressing objects: 100% (2/2), done. 
Writing objects: 100% (3/3), 333 bytes | 333.00 KiB/s, done. 
Total 3 (delta 0), reused O (delta 0), pack-reused 0 
To /REPOSITORIO 

845cfd9..aal2e87 develop -> develop 

Observe que, após fazer o commit do que foi desenvolvido, o desenvolve-
dor B faz o check out para o ramo develop local e funde (merge) o ramo 
feature-15 ao ramo develop. Então, a sua versão do ramo develop é 
movida (push) para o ramo develop do repositório central (representado pela 
conexão remota origin). Essa ação finaliza a atividade de desenvolvimento 
do desenvolvedor B. 

Quando o desenvolvedor A considera o seu desenvolvimento concluído, 
ele segue os mesmos passos do desenvolvedor B: faz o check out para o ramo 

develop local e funde (merge) o ramo 
Confira, a seguir, essa ação e observe 
mover (push) o ramo develop para o 

$ git checkout develop 
Switched to branch 'develop' 
Your branch is up to date with 

$ git merge feature-15 
Updating 845cfd9..c87847£ 
Fast-forward 

arquivo2.c | 1 + 
1 file changed, 1 insertion(+) 

create mode 100644 arquivo2.c 

$ git push origin develop 

To /REPOSITORIO 


feature-15 ao ramo develop. 
atentamente o que ocorrerá ao tentar 
repositório central: 

'origin/develop'. 


Integração da equipe de Devs com o ct) ( 73 

! [rejected] develop -> develop (fetch first) 
error: failed to push some refs to '/REPOSITORIO! 
hint: Updates were rejected because the remote contains work 
that you do 
hint: not have locally. This is usually caused by another re-
pository pushing 
hint: to the same ref. You may want to first integrate the 
remote changes 
hint: (e.g., 'git pull ...') before pushing again. 

hint: See the 'Note about fast-forwards' in 'git push --help' 
for details. 

Quando o desenvolvedor A tentou mover (push) o seu ramo develop para 
o repositório central, a sua ação foi rejeitada. Essa situação ocorreu porque o 
ramo develop do repositório central contém objetos que não foram encon-
trados no ramo develop do repositório local do desenvolvedor A (observe 
os Aints apresentados abaixo do retorno de rejeição do push). 

Uma das ações que o desenvolvedor A pode tomar é baixar os objetos do 
ramo develop do repositório central que não estejam no ramo develop 
de seu repositório local. Uma vez que estejam em seu ramo develop local, 
os objetos novos devem ser analisados e, se não prejudicarem o arquivo pro-
duzido pelo desenvolvedor A, incorporados (merge) ao ramo develop local. 
Confira essas ações a seguir: 

$ git fetch origin 
remote: Enumerating objects: 4, done. 
remote: Counting objects: 100% (4/4), done. 
remote: Compressing objects: 100% (2/2), done. 
remote: Total 3 (delta 0), reused O (delta 0), pack-reused O 
Unpacking objects: 100% (3/3), 313 bytes | 2.00 KiB/s, done. 

From /REPOSITORIO 

845cfd9..aal2e87 develop -> origin/develop 

$ git merge origin/develop 
Merge made by the 'recursive' strategy. 

arquivo3.c | 1 + 

1 file changed, 1 insertion(+) 

create mode 100644 arquivo3.c 



74) ( Integração da equipe de Devs com o Git 
$ git push origin develop 
Enumerating objects: 7, done. 
Counting objects: 100% (7/7), done. 
Delta compression using up to 4 threads 
Compressing objects: 100% (4/4), done. 

Writing objects: 
Total 5 (delta 2), 
To /REPOSITORIO 

aal2e87..3c7icbb2 

100% (5/5), 522 bytes | 522.00 KiB/s, done. 
reused O (delta 0), pack-reused 0 
develop -—> develop 

Com essa ação, o desenvolvedor A pode considerar o seu trabalho concluído. 

Fowler (2020) propõe uma sériede padrões para grupos de desenvolvedores criarem 
ramos que possam ser melhor integrados e organizados. 

Ao aprender a trabalhar com o Gitflow, a produzir e versionar corretamente 

o seu código-fonte e a lidar com os conflitos na publicação de código-fonte, 
se estará preparado para atender ao requisito de integração contínua do DevOps. 
Referências 

CHACON, S.; STRAUB, B. Pro Git: everything you need to know about git. 2nd. ed. New 
York: Apress, 2014. 

COPELAND, L. Extreme programming. In: COMPUTERWORLD. [S. !:s. nJ, 2001. Disponível 
em: https://Awww.computerworld.com/article/2585634/extreme-programming.html. 
Acesso em: 11 ago. 2020. 

DRIESSEN, V. A successful Git branching model. In: NVIE. [S. /:s. n], 2010. Disponível em: 
http://nvie.com/posts/a-successful-git-branching-model. Acesso em: 12 ago. 2020. 

FOWLER, M. Continuous integration. In: MARTIN FOWLER. [S./:s. n], 2006. Disponível em: 
https://martinfowlercom/articles/continuousintegration.html. Acesso em: 12 ago. 2020. 


Integração da equipe de Devs com o st) ( 75 

FOWLER, M. Patterns for Managing Source Code Branches. In: MARTIN FOWLER. [5. |.: 
s.nJ, 2020. Disponível em: https://martinfowler.comyarticles/branching-patterns.html. 
Acesso em: 12 ago. 2020. 

MUNIZ, A. et al. Jornada DevOps: unindo cultura ágil, Lean e tecnologia para entregar 
software com qualidade. 2. ed. Rio de Janeiro: Brasport, 2020. 

WAZLAWICK, R. Engenharia de software: conceitos e práticas. Rio de Janeiro: Elsevier, 2013. 

J 

Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 

) 


Esta página foi deixada em branco intencionalmente. 



Configuração e 
manutenção de contêineres 
usando o Docker 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Apontar as principais características do Docker, as vantagens da utili-
zação de contêineres e a diferença entre contêiner e máquina virtual. 
m Descrever a instalação do Docker para a execução de imagens pre-
existentes (Docker Hub). 
E Criarum arquivo Dockerfile para a execução de imagens customizadas. 

Introdução 

Considere um sistema constituído por diferentes serviços e componentes, 
como servidor web, banco de dados, mensageria, entre outros. A incom-
patibilidade de componentes, serviços e tecnologias à medida que um 
determinado sistema é atualizado gera conflitos entre os serviços que 
necessitam de bibliotecas específicas com versões distintas. Além disso, 
muitas vezes se requer um tempo significativo para a (re)configuração do 
sistema. Sendo assim, cada serviço precisa resolver suas dependências, 
principalmente com relação às bibliotecas com o sistema operacional 
subjacente. Nesse sentido, há necessidade de uma solução que viabilize a 
modificação ou a substituição dos componentes conflitantes sem afetar 
os demais e, portanto, sem comprometer o sistema como um todo. Essa 
solução é conhecida como Docker. 

Neste capítulo, você conhecerá os principais conceitos do Docker, suas 
principais características e as vantagens de utilização de contêineres, bem 
como as diferenças e similaridades entre contêineres e máquinas virtuais. 
Além disso, conhecerá os principais passos da instalação do Docker em 
diferentes sistemas operacionais, visando à sua execução em ambien-
tes virtualizados. Na sequência, verá como criar um arquivo Dockerfile, 



78) ( contiguração e manutenção de contêineres usando o Docker 
focando na execução de imagens customizadas. Por fim, diante de todos 

os conceitos discorridos, você verá como utilizar o Docker em problemas 

em diferentes contextos que englobam o cenário dos sistemas atuais. 

1 Introdução ao Docker 

O Docker é uma solução que viabiliza a modificação ou a substituição de 
componentes conflitantes em um determinado sistema sem afetar os demais 
componentes e, principalmente, sem comprometer o sistema como um todo 
(SILVA, 2016). O Docker é um software e contêiner que fornece uma camada de 
abstração e automação, baseado, originalmente, em sistemas Unix. A Figura 1, 
a seguir, apresenta a estrutura de um contêiner. 

a D 

Processos Processos Processos 
Redes Redes Redes 
Montagem Montagem Montagem 

Figura 1. Estrutura básica de um contêiner. 
a J 

Com base na Figura 1, pode-se perceber que os contêineres são ambientes 
completamente isolados de toda infraestrutura subjacente. Eles viabilizam pro-
cessos próprios, isto é, possuem seus próprios serviços, sua própria interface de 
rede e sua própria montagem. Os contêineres são similares a máquinas virtuais, 
porém compartilham o mesmo kernel do sistema operacional (TANENBAUM; 
BOS, 2016) e possuem uma camada de software acima, chamada Docker. 



Configuração e manutenção de contêineres usando o Docker) ( 79 

Como visto, o Docker é uma solução baseada em contêineres originalmente 
baseada em sistemas Unix, cujo principal objetivo é empacotar aplicações 
em contêineres e enviá-los e executá-los em qualquer lugar, a qualquer mo-
mento, quantas vezes for necessário. Essa é uma das principais diferenças 
entre Docker, máquinas virtuais e contêineres, que serão abordados de forma 
mais detalhada no decorrer deste capítulo. Visando a apresentar as relações 
e diferenças entre esses conceitos, a Figura 2, a seguir, apresenta a estrutura 
básica de um Docker e de um contêiner. 

(a) (b) 
Figura 2. Relações e diferenças entre um (a) Docker e um (b) contêiner 

Conforme apresentado na Figura 2a, o Docker apresenta uma infraestrutura 
de hardware subjacente e, em seguida, o sistema operacional. A camada do 
Docker localizada acima do sistema operacional possibilita a execução de 
cada componente (ou aplicações) que constitui o sistema em um contêiner 
separado, com suas próprias bibliotecas e respectivas dependências, todas no 
mesmo sistema operacional, porém em contêineres separados. Já na Figura 2b, 
pode-se observar que os contêineres são ambientes completamente isolados, 
têm seus processos, suas próprias interfaces de rede e suas próprias montagens. 
No entanto, todos os contêineres compartilham o mesmo kernel do sistema 
operacional em que estão sendo executados, o qual é responsável por interagir 
com a infraestrutura de hardware subjacente. 



so ) ( Configuração e manutenção de contéineres usando o Docker 

Na camada acima, tem-se o Docker, por meio do qual é possível executar 
cada componente em um contêiner separado, com suas próprias bibliote-
cas e dependências, todas na mesma máquina virtual e no mesmo sistema 
operacional, porém em contêineres distintos. Por essa razão, é importante 
compreender as relações e diferenças entre contêineres e máquinas virtuais, 
apresentadas na Figura 3. 

6) 

[sauna rua 

a a 

Figura 3. Relações e diferenças entre (a) contêmeres e (b) máquinas virtuais 

Na Figura 3b, pode-se perceber uma camada mais baixa de infraestrutura 
de hardware, uma camada superior denominada monitor de máquina virtual, 
também conhecida como hypervisor, e, em seguida, cada máquina virtual 
supervisionada (TANENBAUM; BOS, 2016). Cada máquina virtual (VM, 
virtual machine) possui seu próprio sistema operacional (SO), bibliotecas 
(Libs) e dependências (Deps) dentro dela. Esse aspecto pode ocasionar uma 
sobrecarga no sistema/aplicação quando considerada uma maior utilização 

dos recursos subjacentes. Já na Figura 
de hardware subjacente e o Docker 
que o Docker gerencia os contêineres 
aplicações, bibliotecas e dependências 

Além da vantagem sobre a utilização 

3a, pode-se observar a infraestrutura 
instalado no sistema operacional, em 
que são executados com suas próprias 

independentes. 
do sistema, outra vantagem importante 

da utilização de um Docker é que o processo de configuração ocorre apenas 
uma vez, isentando que o sistema seja reconfigurado ou modificado a cada 
atualização ocorrida no sistema em questão. Portanto, cada uma das estratégias 



Configuração e manutenção de contêineres usando o Doc ter) ( 81 

de virtualização possui vantagens e desvantagens em diferentes aspectos, 
principalmente em termos de utilização, tamanho e inicialização (Figura 4). 
á 
Ulização Tamanho Inicialização Uização. Tamanho Inicialização 

Figura 4. Contéineres vs. máquinas Virtuais: inicialização, tamanho e utilização 

A comparação apresentada na Figura 4 considera que existem vários sis-
temas operacionais virtuais e kernels executados em VMs, as quais, por sua 
vez, acabam consumindo mais espaço em disco (tamanho). Desse modo, 
as VMs tornam-se mais pesadas nesse contexto, de modo que, por exemplo, 

o tamanho de uma VM pode requerer alguns gigabytes, ao passo que os 
Dockers são considerados mais leves por requererem apenas alguns megabytes 
de armazenamento. Esse aspecto também permite que os Dockers apresen-
tem uma inicialização mais leve, ou seja, eles podem ser inicializados em 
questão de segundos, ao passo que as VMs podem demorar alguns minutos. 
Em contrapartida, as VMs provêm um desacoplamento completo do sistema 
operacional e do hardware subjacente. 
Como existem vantagens e desvantagens em ambas as estratégias, atual-
mente é utilizada uma solução que acopla as duas estratégias em um mesmo 
ambiente, permitindo, assim, explorar as vantagens de cada uma delas em um 
mesmo objetivo (Figura 5). 



82 ) ( Configuração e manutenção de contéineres usandoo Docker 
Figura 5. Estratégia de utilização de contêineres e máquinas virtuais 

A Figura 5 representa a estratégia mais condizente com o cenário atual, com 
milhares de contêineres em execução dentro de VMs gerenciados por Dockers 
hosts. Dessa forma, é possível explorar os benefícios das duas estratégias em 
uma única solução, isto é, as vantagens da virtualização em oferecer a flexi-
bilização de montar e desmontar contêineres sob demanda (escalonamento 
rápido) aliadas aos benefícios do Docker, que propicia o gerenciamento de 
diferentes aspectos, como, por exemplo, aplicativos, bibliotecas, dependências 
e conflitos. 

Por fim, salienta-se que Dockers, contêineres e virtualização são estraté-
gias que se complementam, visando a suprir diferentes desafios dos sistemas 
distribuídos atuais, como, por exemplo, compatibilidade entre tecnologias, 
componentes e dependências de bibliotecas. A estratégia de utilização de 
contêineres e VMs tem sido amplamente utilizada nos sistemas distribuídos 
atuais, de modo a usufruir dos benefícios das três estratégias em um único 
ambiente. 

Arquitetura do Docker 

Primeiramente, foram apresentados os principais conceitos, bem como as 
características e vantagens do uso do Docker no desenvolvimento de DevOps 
(KIM et al., 2018). A fim de finalizar essa visão geral do Docker, a Figura 6, 
a seguir, apresenta a arquitetura dessa estratégia. 



Configuração e manutenção de contêineres usando o Docker ) (es 

a Docker daemon 

Figura 6. Arquitetura do Docker. 

Fonte: Adaptada de Docker (2020) 
u , 

Com base na Figura 6, é importante destacar que o Docker utiliza uma 
arquitetura cliente-servidor. O cliente Docker se comunica com o daemon 
Docker, que faz o trabalho pesado de construção (comando docker builid), 
execução e distribuição de seus contêineres Docker. O Docker daemon 
(também conhecido como dockered) escuta as solicitações da API Docker 
e gerencia objetos Docker, como imagens, contêineres, redes e volumes. 
O daemon também pode se comunicar com outros daemons para gerenciar os 
serviços Docker. Além disso, o cliente Docker e o daemon podem ser execu-
tados no mesmo sistema, assim como é possível conectar um cliente Docker 
a um daemon remoto do Docker. O cliente Docker e o daemon se comunicam 
usando uma API REST, por soquetes (neste exemplo, UNIX) ou uma interface 
de rede. Um registro Docker armazena imagens Docker. O Docker Hub é um 
registro público que qualquer pessoa pode usar, e o Docker está configurado 
para procurar imagens no Docker Hub por padrão, sendo possível executar 
seu próprio registro privado. A seguir, serão apresentados os principais pré-
-requisitos e etapas para a instalação do Docker com base no Docker Hub. 



84 ) ( Configuração e manutenção de contêineres usando o Docker 
2 Incorporando o Docker 

Antes de apresentar os principais passos para a instalação do Docker, é im-
portante apresentar uma visão geral do Docker de alto nível e ressaltar a 
necessidade desse software neste contexto (DOCKER, 2020). A Figura 6, 
a seguir, apresenta um exemplo de um sistema hipotético atual com aplicações, 
componentes e serviços com e sem contêineres e a camada de software Docker. 

ida 
Figura 7, A importância do Docker em um sistema hipotético atual. 

Na Figura 7, é apresentado um sistema hipotético, com e sem contêineres 
e Docker, constituído dos seguintes componentes: um servidor web com Node 
Express, banco de dados com MongoDB, sistemas de mensagens com Redis 
e serviço de orquestração com Ansible. Na Figura 7a, pode-se observar uma 
série de conflitos, principalmente em termos de compatibilidade de todos os 
serviços, aplicações e componentes com todas as camadas subjacentes, desde 
bibliotecas e dependências, passando pelo sistema operacional até a camada 
mais baixa de infraestrutura de hardware (bare metal). Dessa forma, é nítido 
que há necessidade de se garantir que todos os diferentes e distintos serviços, 
componentes e aplicações sejam compatíveis com a versão do sistema ope-
racional. Na Figura 7b, por sua vez, é possível constatar a real importância 
do Docker e dos contêineres quando da execução de cada componente em 
contêineres (containerize applications) com suas próprias dependências e 
bibliotecas, todos em contêineres separados, porém na mesma VM e no mesmo 
sistema operacional. 


Configuração e manutenção de contêineres usando o Docker ) (es 

Em termos práticos, o Docker é necessário para resolver alguns aspectos 
significativos que fazem parte dos sistemas atuais, os quais envolvem distintas 
tecnologias e diversas aplicações (de ponta a ponta do sistema) e refletem 
problemas de compatibilidade e dependências de bibliotecas, tempos longos 
de configuração dos ambientes e ambientes heterogêneos de desenvolvimento. 
Por meio dos contêineres, torna-se possível modificar ou alterar esses 
componentes sem afetar os outros, ou até mesmo o sistema como um todo. 
A configuração desses ambientes (também conhecidos como contêineres 
dockers) não é uma tarefa fácil, pois eles encontram-se em nível de abstração 
muito baixo, porém a camada Docker oferece uma ferramenta de alto nível, 
com várias funcionalidades, tornando o acesso, o manuseio e o funcionamento 
básico dessa estratégia mais fácil. Dessa forma, é importante ressaltar que 
a configuração de um Docker é extremamente importante nesse contexto, 
a fim de oferecer aos desenvolvedores alternativas simples de manipulação, 
desenvolvimento e gerência de um sistema, com comandos simples após a 
sua instalação. 

Instalação do Docker 

Atualmente, existem muitas versões de aplicações em contêineres prontas 
e disponíveis para utilização. Além disso, muitas empresas de desenvolvi-
mento disponibilizam seus produtos em contêineres em um Docker público 
(ou repositório), chamado de Docker Hub (do inglês Public Docker Registry) 
(DOCKER, 2020) ou Docker Store (Figura 8). 

| 

os nodes 
so express) 

Figura 8. Exemplo de Docker Hub. 


86 ) ( Configuração e manutenção de contéineres usando o Docker 
Na Figura 8, pode-se observar imagens de banco de dados, serviço de 
orquestração e mensageria, bem como serviços web. Os exemplos ilustra-
dos em cada contêiner seguem o mesmo sistema hipotético apresentado na 
Figura 7, com MongoDB, Ansible, Redis e Node Express, respectivamente. 
Em suma, no Docker Hub, é possível encontrar imagens dos sistemas opera-
cionais mais comuns, dos bancos de dados e de outros serviços e ferramentas. 

Quando o desenvolvedor ou o analista de sistemas identifica que necessita 
de determinada imagem, após instalar o Docker na máquina host, a aplicação 
requerida pode ser incorporada no sistema em questão a partir de imagens 
preexistentes buscadas através do Docker Hub. Essa operação é realizada 
de uma forma considerada simples, uma vez que, basicamente, é necessário 
apenas executar o comentado docker run + o nome da imagem que se 
deseja em um terminal. Se o programador não encontrar a imagem desejada, 
ele deve criar a sua própria imagem e incorporá-la no Docker Hub, tornando-a 
disponível ao público. 

Etapas para a instalação do Docker: Docker Hub 

O Docker está disponível em duas versões para instalação: Community e 
Enterprise. A versão Enterprise é paga, certificada e voltada para oferecer 
suporte à plataforma de contêineres empresariais, com alguns adicionais, 
como, por exemplo, sistema de gerenciamento de imagens (especialmente 
voltado à segurança das imagens) e plano de controle universal para gerenciar 
e orquestrar contêineres em tempo de execução. Em contrapartida, a versão 
Community é um conjunto de produtos Docker gratuitos e está disponível em 
Linux, Mac, Windows ou em plataformas de cloud, tais como AWS e Azure. 
Por tratar-se de uma versão gratuita, a versão Community será adotada como 
estudo de caso da instalação do Docker apresentada a seguir. 

Como mencionado, o Docker Community está disponível em diferentes 
sistemas operacionais, tais como Linux (CentOS, Debian, Fedora e Ubuntu), 
Windows e Mac. As etapas mais importantes são: (1) identificar se o sistema 
físico, VM ou computador em questão tem um sistema operacional compatível 
com os pré-requisitos do Docker; (II) acessar o site Docker e, em seguida, Get 
Docker, para ser redirecionado para a página do Docker Engine Community 
Editon; (III) escolher o sistema operacional em questão, seguindo os passos 
apresentados a seguir. 



Configuração e manutenção de contêineres usando o Docker ) ( 87 

E Docker no Ubuntu: 
= Seleção de distribuição: após selecionar o tipo de sistema opera-
cional, a distribuição desejada deve ser selecionada. Novamente, 
é preciso certificar-se sobre os pré-requisitos necessários para a 
instalação do Docker no sistema operacional em questão. No Ubuntu, 
por exemplo, os pré-requisitos são: o sistema deve ser 64 bits, com 
suporte às versões Focal (20.04 LTS), Bionic (18.04 LTS) e Xenial 

(16.04 LTS). Para tanto, deve-se digitar em um terminal o comando 
cat /etc/*release*. 
= Verificação de versão antiga e remoção desta: na sequência, ainda 
por meio de um terminal, deve-se digitar o comando sudo apt-get 
remove docker docker-engine docker.io contai-
nerd runc. Como superusuário, essa linha realiza a verificação 
de se há nenhuma versão anterior do Docker instalada no SO em 
questão, para, em caso afirmativo, removê-la. As versões mais antigas 
são: docker, docker. io ou docker-engine. 

= Métodos de instalação: (1) a maioria dos usuários configura os 
repositórios do Docker e instala a partir deles, para facilitar as tare-
fas de instalação e atualização. Essa é a abordagem recomendada. 
(ID) Alguns usuários baixam o pacote DEB, instalam-no manualmente 
e gerenciam as atualizações também de modo manual. Isso é útil 
em situações como a instalação do Docker em sistemas air-gap sem 
acesso à internet. (III) Em ambientes de teste e desenvolvimento, 
alguns usuários optam por usar scripts de conveniência automatizados 
para instalar o Docker. 

= Instalação do Docker: por meio de um terminal de comandos, 
primeiro, deve-se atualizar o índice de pacotes do sistema através 
do comando sudo apt-get update, de modo a garantir que 
será instalada a versão mais atual do software. Em seguida, deve-
-se utilizar o comando sudo apt-get install docker-
-ce docker-ce-cli containerd.io para instalar o Docker 
especificamente. 

m Docker em outras distribuições Linux: 

= Seleção de distribuição: após selecionar o tipo de sistema opera-
cional, a distribuição desejada deve ser selecionada. Novamente, 
é preciso certificar-se sobre os pré-requisitos necessários para a 
instalação do Docker no sistema operacional em questão. Os pré-
-requisitos dos sistemas operacionais CentOS, Debian e Fedora estão 
disponíveis no site oficial do Docker. Os passos para a instalação 



88 ) [ Configuração e manutenção de contêineres usando o Docker 
nessas variações de sistemas operacionais Unix são análogos aos 
comandos utilizados na instalação do Docker no Ubuntu, no en-
tanto, deve-se utilizar a sintaxe de comandos de cada uma dessas 
distribuições, respectivamente. 

E Docker no Windows: 
= Os pré-requisitos básicos para a instalação são: sistema operacional de 
64 bits, 4 GB de memória RAM e versões Windows Pro, Enterprise, 
Education ou Windows Server. 
= A instalação do Docker no Windows é necessária quando o desen-
volvedor possui apenas um computador sem acesso ao Linux. Nesse 
caso, existem duas opções: (1) criar uma VM Linux (VirtualBox ou 
VMWare Player), instalando algumas das distribuições compatíveis 
com o Docker, e seguir os passos de instalação descritos anterior-
mente; ou (II) instalar o Docker de forma nativa (esta alternativa 
será abordada a seguir). 
= No Windows, existem dois caminhos para instalação original do 
Docker: (1) via Docker Toolbox (especialmente para versão Windows 
Home); e (II) via Docker Desktop para Windows. 
= O Docker Toolbox é umas das opções originais que suportam a 
instalação do Docker, especialmente para versões mais antigas do 
Windows. Os pré-requisitos para a instalação desse pacote são: sis-
temas operacionais de 64 bits, Windows 7 (ou versões mais atuais) 
ea virtualização habilitada na BIOS. O Docker Toolbox possui uma 
série de ferramentas, tais como VirtualBox, Docker Engine, Docker 
Machine, Docker Compose e Kitematic Gui. Essas ferramentas auxi-
liam o usuário desde o download do Docker Toolbox até a instalação 
do Docker efetivamente. 
= Instalação: em suma, esse pacote reproduz uma interface gráfica 
através do Kitematic. A partir do ambiente gráfico, uma VM 
Linux é criada através do VirtualBox e, na sequência, realiza a 
instalação do Docker propriamente dito e de outros componentes 
essenciais de forma automatizada e através de interface gráfica. 
O Docker Toolbox trata-se de uma estratégia que facilita o trabalho 
para o desenvolvedor que nunca criou e configurou uma VM 
Linux desde o início. 
= A segunda alternativa de instalação no Windows (do inglês Docker 
Desktop for Windows) utiliza uma tecnologia nativa de virtualiza-
ção, denominada Hyper-V (MICROSOFT, 2018). Essa camada é 
responsável por realizar toda a interação entre as camadas adjacentes 



Configuração e manutenção de contêineres usando o Docker ) ( 89 

e subjacentes dentro do ambiente Windows. Além disso, é devido 
a essa tecnologia de virtualização que a instalação de Docker no 
Windows se restringe a versões mais robustas. 
= Instalação: a partir do Docker Hub, deve-se baixar a versão 

estável Get Stable. Uma ferramenta chamada Docker for Windows 
Installer será baixada. Para instalar o Docker, essa ferramenta 
deve executada. Quando a instalação termina, o Docker é iniciado 
automaticamente. O ícone do Docker na área de notificação indica 
que o Docker está em execução e acessível a partir de um terminal. 

E Docker no Mac: 

= Os pré-requisitos para a instalação são: macOS versão 10.13 ou mais 
recente, ou seja, High Sierra (10.13), Mojave (10.14) ou Catalina 
(10.15); o hardware do dispositivo deve ser um modelo 2010 ou 
mais recente. 

= Essa alternativa de instalação, denominada Docker Desktop para 
Mac, trata-se de um aplicativo de desktop de fácil instalação que 
auxilia a criar, depurar e testar aplicativos Docker em um Mac. 
O Docker Desktop para Mac é um ambiente de desenvolvimento 
completo, profundamente integrado com a estrutura do Mac OS 
Hypervisor, sua rede e o seu sistema de arquivos. Essa é a maneira 
mais rápida e confiável de se executar o Docker em um Mac. 

= Instalação: a versão estável pode ser instalada através do Docker 
Hub — Get Stable. Um arquivo chamado Docker.dmg será baixado. 
Na sequência, para iniciar o processo de instalação do Docker, esse 
arquivo deve ser executado. Quando a instalação for concluída e o 
Docker for iniciado, o ícone do Docker na barra de status superior 
mostra que o Docker está em execução e acessível a partir de um 
terminal. 

Dockerfile 

Primeiramente, foram apresentados os principais conceitos, características e 

vantagens do uso do Docker no desenvolvimento de DevOps. Posteriormente, 

foram apresentados os principais pré-requisitos e as etapas de instalação do 

Docker em diferentes sistemas operacionais com base em imagens preexis-

tentes a partir do Docker Hub. Nessa primeira alternativa de criação de um 

contêiner, é utilizado o comando docker run + nome da imagem 

desejada, o qual, através do Docker Hub, busca e baixa a imagem através 



90 ) ( Configuração e manutenção de contéineres usando o Docker 
da internet, executando o contêiner em questão. Nesta seção, será abordada 
uma segunda alternativa, a qual consiste em criar um contêiner quando da 
escrita de um Dockerfile. 

Sendo assim, pode-se concluir que o Dockerfile é uma forma que os de-
senvolvedores utilizam visando a criar imagens próprias. Considerando-se 
esse cenário, é importante destacar que, na primeira alternativa descrita, que 
considera o uso do Docker Hub para a criação de imagens, a imagem também é 
gerada com um Dockerfile, no entanto, este já está pronto. Em outras palavras, 
é realizada a utilização de um Dockerfile preexistente, escrito por outra pessoa. 

Nesse contexto, é importante destacar a diferença entre contêiner e imagem 
(TANENBAUM; BOS, 2016). A Figura 9, a seguir, apresenta a diferença e a 
relação existentes entre esses conceitos, incorporando o conceito de DockerFile. 

Dockerfile Imagem Contêiner 

— obvio À un 

construção execução 

Figura 9. Contêinervs. Imagem 

A diferença entre os dois conceitos é que uma imagem é um pacote ou um 
template, como, por exemplo, um template de uma VM, com o qual é possível 
trabalhar em um ambiente totalmente virtualizado. A partir disso, é possível 
criar um ou mais contêineres. 

Fique atento 

Lembre-se de que os contêineres executam instâncias de imagens isoladas, cada uma 
com seu ambiente e conjunto de processos. Na ilustração apresentada na Figura 9, 
a imagem refere-se à exibição de como um contêiner será efetivamente construído, 
por essa razão, as imagens não são, de fato, executadas ou inicializadas após a etapa 
de construção (do inglês, build), e sim dentro dos contêineres. 


Configuração e manutenção de contêineres usando o Docker ) (e1 

O principal aspecto que deve ser ressaltado nesse ponto é que o desenvol-
vedor escreverá o seu Dockerfile, de modo a construir uma imagem através 
do comando docker build, e, a seguir, o contêiner será de fato criado e 
executado com o comando docker run. Em suma, o Dockerfile representa 
a etapa inicial, a imagem, a etapa intermediária, e o contêiner, o ponto final 
do processo de criação de um contêiner a partir de um Dockerfile. 

Etapas para a instalação do Docker: 
criando um Dockerfile 

Para exemplificar as etapas de criação do Dockerfile, será utilizada como 
estudo de caso a criação de um contêiner com uma imagem do SO Ubuntu 
versão 18.04. As etapas são as seguintes: 

1. Para inicializar o processo de criação de uma imagem, abra um terminal 
e acesse a pasta na qual o Dockerfile foi criado. 
2. Abra o arquivo criado e adicione a linha especificando a imagem que 
deseja criar através do comando From. Neste caso, como se trata de 
um exemplo de criação da imagem do Ubuntu, a linha deve ser FROM 
SO: VERSÃO, isto é, FROM ubuntu:18.04. 
3. No terminal e na pasta na qual o Dockerfile foi criado, dispare o co-
mando docker build. (não esqueça do ponto -.-). Assim, o processo 
de construção da imagem através do arquivo será iniciado. 
4. Quando o processo de construção finalizar, execute o comando 
docker image 1s para verificar se a imagem foi criada com sucesso, 
avaliando os arquivos gerados. A invocação e o retorno desse comando 
estão ilustrados na Figura 10. 
REPOSITORY TAG IMAGE ID CREATED SIZE 
REPOSITÓRIO VERSÃO IMAGEM ID CRIAÇÃO TAMANHO 

ubuntu 18.04 7798292523 1dayago 72.3MB 

Figura 10. Retorno do comando de verificação da imagem criada através do 
Dockerfile no Ubuntu. 


92 ) ( Configuração e manutenção de contéineres usando o Docker 
5. As saídas representadas na Figura 10 indicam que a imagem criada a 
partir do Dockerfile está pronta para ser executada em um contêiner. 
6. Por fim, para a execução do contêiner criado, invoque o comando 
docker run. No Linux, esse comando pode ser invocado de duas 
maneiras (com ou sem o parâmetro -it): docker run + nome 
da imagem ou docker run -it nome da imagem, ou, 
ainda, docker run -it 7798f292e523 (id da imagem gerada). 
Em suma, o parâmetro -it é utilizado para explicitar que o contêiner 
é iterativo. Em outras palavras, a utilização do -i t como parâmetro do 
docker run viabiliza a interação do desenvolvedor com o contêiner, 
caso contrário, o contêiner permanece como se fosse um daemon, sem 
a possibilidade de interação com ele. 
Principais comandos e instruções do Dockerfile 

E FROM: essa instrução é obrigatória em um Dockerfile, sendo consi-
derada a instrução mais importante. Por meio dela, é possível definir 

o ponto inicial de uma imagem a ser criada com o Dockerfile escrito. 
Por exemplo, se o sistema em questão requer uma imagem do Java 
para incorporar o contêiner em criação, faz-se necessário especificar a 
imagem do Java (openjdk) no Dockerfile. De forma análoga à inserção 
da imagem do Ubuntu descrita anteriormente, a seguinte linha de 
comando deve ser incrementada no arquivo criado: FROM openjdk. 
E RUN: essa instrução é interessante porque ela pode ser invocada e exe-
cutada quantas vezes for necessário. Por meio dela, pode-se adicionar 
comandos na criação do Dockerfile para que sejam executados em 
camadas no decorrer das etapas que constituem a criação da imagem. 
Confira o exemplo mostrado na Figura 11. Quando da execução do 
segundo passo (comando docker build), o Dockerfile definido 
e demonstrado na Figura 11 será construído. Dessa forma, além do 
download da imagem do Ubuntu 18.04 para incorporar a imagem do 
sistema operacional na imagem em criação, as instruções RUN também 
serão executadas. A primeira delas (apt-get update) é um comando 
clássico dos sistemas Unix em geral, responsável por atualizar o repo-
sitório de pacotes do sistema. A segunda e última instrução RUN desse 
exemplo consiste na instalação do Java 9 através do comando apt-get 
install openjdk-9-y. A execução dessas instruções acarretará 
uma sequência de downloads, e, por fim, a imagem será gerada com um 



Configuração e manutenção de contêineres usando o Docker ) ( 93 

determinado ID (como mostrado na Figura 10). Considerando a criação 
de um contêiner a partir dessa imagem criada, tem-se: a imagem do 
sistema operacional Ubuntu 18.04, o repositório de pacotes atualizado 
ea versão do Java 9 instalada. Uma das vantagens da instrução RUN é 
que ela pode ser reutilizada na criação de outras imagens, incrementando 
mais instruções (além das existentes) ou não. 

Dockerfile EIN 

FROM ubuntu:18.04 

RUN apt-get update 

RUN apt-get install openjdk-8-jdk -y 

J 

Figura 11. Exemplo de utilização da instrução RUN no Dockerfile. 

CMD: a instrução CMD é semelhante à instrução RUN, pois a sintaxe é a 
mesma, e os parâmetros podem ser passados nessa instrução da mesma 
maneira que os parâmetros são passados na instrução RUN, basta trocar 

o nome da instrução de RUN para CMD. Quando da execução do Docker-
file, ilustrada na Figura 12, pode-se perceber que esse novo comando 
não será executado na etapa da criação. Isso ocorre porque, de fato, 
a instrução CMD executa o comando somente na criação do contêiner, 
e não na construção da imagem, ou seja, quando invocado o comando 
docker run + nome da imagem criada. 
Dockerfile 

FROM ubuntu:18.04 
RUN apt-get update 
RUN apt-get install openjdk-8-jdk -y 
CMD touch arquivo-novo-boas-vindas 

Figura 12. Exemplo de utilização da instrução CMD no Dockerfile. 


94 ) (contiguração e manutenção de contêineres usando o Docker 
E ADD: o papel principal dessa instrução é realizar a cópia de um ar-
quivo ou diretório, ou, ainda, efetuar o download de uma URL 
(da máquina host) e incorporá-la na imagem criada. Uma vez criada a 
imagem descrita no Dockerfile (Figura 13), deve-se executar o contêiner. 
O resultado esperado é: o arquivo passado como argumento estará 
dentro do contêiner. Além dessa demonstração, essa instrução apre-
senta outras funcionalidades interessantes; por exemplo, um diretório 
zipado é passado como argumento para ser incorporado no contêiner, 
ea própria instrução descomprime o diretório de forma automática. 
Ademais, destaca-se novamente que essa instrução permite o download 
de arquivos através de URLS. 

FROM ubuntu:18.04 
RUN apt-get update 
RUN apt-get install openjdk-8-jdk -y 
ADD arquivo-maquina-host arquivo-transferido / 

Figura 13. Exemplo de utilização da instrução ADD no Dockerfile. 

E WORKDIR: essa instrução tem como foco principal definir o ambiente 
de trabalho. É por meio dela que é definido onde as demais instru-
ções (RUN, CMD, ADD, etc.) serão executadas. Além disso, a instrução 
WORKDIR define o diretório padrão que será aberto quando o contêiner 
for executado. De acordo com o Dokerfile apresentado na Figura 14, 
quando o contêiner for acessado através dessa imagem, o resultado 
esperado será: o contêiner inicializado na pasta indicada (pasta-padrão) 
e o arquivo passado pela instrução ADD incorporado dentro contêiner. 



Configuração e manutenção de contêineres usando o Docker ) ( 95 

FROM ubuntu:18.04 

RUN apt-get update 

RUN apt-get install openjdk-8-jdk -y 

WORKDIR /pasta-padrao 

ADD arquivo-maquina-host arquivo-transferido 

Figura 14. Exemplo de utilização da instrução WORKDIR no Dockerfile. 

Portanto, fica claro que o processo de criação de uma imagem é uma etapa 

de extrema importância no aprendizado de Docker. A partir do Dockerfile, 
diversas comandos e instruções podem ser utilizados para atingir um deter-
minado propósito. 

Referências 

DOCKER. Get-Started. 2020. Disponível em: https://www.docker.com/get-started. Acesso 

em: 16 set. 2020 
KIM,G. etal. DevOps: como obter agilidade, confiabilidade e segurança em organizações 
tecnológicas. Tradução de João Tortello. Rio de Janeiro: Alta Books, 2018. 

MICROSOFT. Introdução ao Hyper-Vno Windows 10. 2018. Disponível em: https://docs.mi-

crosoft.com/pt-br/virtualization/hyper-v-on-windows/about/, Acesso em: 16 set. 2020. 
SILVA, W. E. Aprendendo Docker: do básico à orquestração de contêiners. São Paulo: 
Novatec Editora, 2016. 

TANENBAUM, A. S.; BOS, H. Sistemas operacionais modernos. 4. ed. São Paulo: Pearson 
Education, 2016. 


96 ) (configuração e manutenção de contêineres usando o Docker 
Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 


Uso do Docker para 
microsserviços 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Descrever as características fundamentais dos microsserviços. 
E Criar uma stack de microsserviços básica executando contêineres. 
E Identificar as características do ciclo de vida de contêineres. 

Introdução 

Este capítulo está organizado em três seções que buscam proporcionar a 
compreensão do significado dos microsserviços e como esse conceito se 
enquadra no ecossistema de tecnologias e práticas existentes na área da 
computação. A primeira seção busca proporcionar o entendimento dos 
microsserviços. A segunda seção apresenta tecnologias e técnicas para 
implantar microsserviços com o apoio da ferramenta Docker, entendendo 
os seus benefícios e como ela pode acelerar e facilitar a publicação e a 
gestão dos microsserviços. Por fim, a última seção apresenta os estágios 
do ciclo de vida de um contêiner e as instruções da tecnologia Docker 
que proporcionam a sua gestão. 

Neste capítulo, você estudará sobre a arquitetura de sistemas com 
base em microsserviços, algo fundamental para construir projetos de 
software contemporâneos, que habitam um ecossistema de serviços 
em cloud extremamente dinâmico, com muitas possibilidades para 
escalabilidade, alto desempenho, disponibilidade, entre outros requi-
sitos fundamentais para o sucesso de produtos. Além disso, conhecerá 
a ferramenta Docker, utilizada para a criação de imagens, instanciação, 
execução e gerenciamento de contêineres, competências básicas para 
um profissional atuante em DevOps. 



98 )l Uso do Docker para microsserviços 

1 Os microsserviços 

Quando se fala em desenvolvimento de software, pode-se construir uma série 
de funcionalidades que podem compor um produto ou parte de um produto. 
Essas funcionalidades podem, ainda, atender tanto a usuários finais como 
a outros sistemas. Desse modo, as funcionalidades de software podem ser 
classificadas como B2€ (do inglês bussiness to customer) ou B2B (do inglês 
business to business). 

O desenvolvimento de software deve visar ao atendimento de algum pro-
blema, ou seja, deve prover a solução para alguma necessidade que possa ser 
resolvida por meio do processamento de informações digitais. Para construir 
essa solução, pode ser utilizada uma grande variedade de tecnologias, incluindo 
linguagens de programação, servidores de bancos de dados, servidores web, 
sistemas operacionais, entre outras. Contudo, como encapsular essa solução 
em um produto para ser consumido pelos clientes, sejam estes usuários ou 
outros sistemas” Para essa finalidade, existem muitas alternativas. 

Com desenvolvimento de software tradicional, o software, ao ser compi-
lado, gera um arquivo executável, que, por sua vez, torna-se um processo de 
um sistema operacional ao ser executado. Esse processo pode se materializar 
tanto por meio de telas para interação com usuários como pode realizar a 
abertura de portas de rede ou esperar por requisições a serem feitas por outros 
software, a fim de prover o que chamados de serviço. 

Serviço seria um termo abstrato para uma função de software, entendida 
como qualquer rotina computacional que, ao receber ou não parâmetros de 
entrada, realiza um determinado processamento e devolve, ou não, algum 
tipo de dado de saída. Nesse sentido, o que diferenciaria uma função ou 
procedimento de um serviço? 

O termo serviço, na área de software, já tomou muitas formas. Um grande 
momento de popularidade foi o advento dos serviços web (web services), 
que se popularizou por meio do protocolo SOAP (do inglês Simple Object 
Access Protocol; ou Protocolo Simples de Acesso a Objetos, em português). 
Os web services foram uma das primeiras alternativas para possibilitar que 
dois sistemas, utilizando tecnologias heterogêneas, trocassem informações 
de forma padronizada, isto é, utilizando um protocolo de rede, normalmente 
baseado em HTTP (do inglês HyperText Transfer Protocol; ou Protocolo de 
Transferência de Hipertexto, em português). 



Uso do Docker para microsserviços ) [99 

Antes dos web services, havia outras tecnologias para consumo de funções 
de forma remota, tais como CORBA (do inglês Common Object Request 
Broker Architecture) ou RPC (do inglês Remote Procedure Call), muitas 
oriundas da programação distribuída, isto é, pedaços de software executando 
em diferentes computadores, que tinham como característica a necessidade 
de tanto o provedor do serviço (função) como o cliente adotarem tecnologias 
compatíveis. Esse tipo de desenvolvimento de software também ganhou força 
com a tecnologia JAVA corporativa, o JEE (JAVA Enterprise Edition), que 
possibilitou que, de forma simples, objetos JAVA, um cliente de serviço e um 
provedor de serviço se comunicassem por meio da rede de computadores, porém 
ainda limitados a ambos estarem baseados na mesma plataforma tecnológica. 

Os web services, por serem um conceito independente de tecnologia, pois 
a comunicação entre dois sistemas poderia ocorrer de forma padronizada 
por meio de um protocolo baseado no HTTP, foram muito aceitos, devido 
à sua versatilidade e escalabilidade. Desse modo, em virtude de demandar 
muitas regras para a publicação e o consumo de serviços, o protocolo SOAP 
acabou dividindo espaço com os web services baseados em REST (do inglês 
Representational State Transfer; ou Transferência de Representação de Estado, 
em português), sendo este outro padrão para a publicação e o consumo de 
funções de software baseados em HTTP. 

Uma das principais diferenças entre os web services REST e SOAP é que o 
primeiro consegue disponibilizar e consumir serviços de forma mais simples 
e leve, ao passo que o segundo demanda maiores controles para a publicação e 

o consumo de serviços, o que pode tornar o seu uso um pouco mais complexo. 
A Figura 1, a seguir, ilustra as principais diferenças entre as abordagens REST 
e SOAP. Apesar de atenderem ao mesmo propósito, a abordagem SOAP faz 
a comunicação por conteúdo XML (do inglês eXtensible Markup Language: 
ou Linguagem de Marcação Extensível, em português) seguindo o protocolo 
SOAP, ao passo que, na abordagem REST, as requisições e respostas ocorrem 
em formato livre, seguindo diferentes sintaxes, de acordo com a necessidade. 

100) ( Uso do Docker para microsserviços 
SOAP (WS-*) POX RSS JSON fm 
HTTP HTTP HTTP HTTP. 
GET POST PUT DEL 

SOAP REST 
Figura 1. Web services: REST vs. SOAP 
Fonte: Adaptada de [SOAP] (2019) 

Aliada ao desenvolvimento de web services, foi concebida uma nova 
forma de estruturar sistemas de software, originando-se, assim, o SOA (do 
inglês Service Oriented Architecture; ou Arquitetura Orientada a Serviços, 
em português). Apesar de o SOA estabelecer uma série de orientações sobre 
como segregar os serviços e disponibilizá-los para consumo, tais recomen-
dações ocorrem apenas no caráter lógico, sem entrar em detalhes quanto à 
infraestrutura e à separação física desses serviços. Nesse sentido, o DevOps 
entra em cena, trazendo não apenas preocupações quanto à estruturação 
lógica de projetos de sistemas, mas também assuntos relacionados ao projeto 
físico e a como publicar e disponibilizar esses serviços (MUNIZ et al., 2020). 

Mas, então, como surgiram os microsserviços? O que seria isso? Outra 
forma de publicação de funções de software? Outra alternativa de desenvol-
vimento de web services? Na verdade, microsserviço é um conceito, podendo 
ser implementado com uma grande variedade de tecnologias distintas, além 
de não concorrer com nenhum dos conceitos citados anteriormente. Portanto, 
é possível desenvolver um microsserviço publicando web services, seja em 
tecnologia SOAP, seja em REST, ou, ainda, ter um microsserviço para prover 
funções de software clássicas baseadas em qualquer uma das tecnologias 
elencadas nos parágrafos anteriores. 

Um microsserviço seria um processo de software (arquivo de programa de 
software executável processado por um sistema operacional). Contudo, esse 
processo de software não deve prover todas as funções de um software, e sim 
apenas um conjunto pequeno dessas funções, formado por funções que estejam 
intimamente ligadas e agrupadas de modo lógico em um mesmo processo. 
Essas funções podem, então, ser publicadas e fornecer o seu serviço de forma 
independente do restante do software. O conceito de microsserviço teve a sua 



Uso do Docker para microssenviços ) ( 101 

criação motivada pelo cenário contemporâneo de desenvolvimento de sistemas, 
em que os sistemas são demandados por alto grau de disponibilidade, além de 
que constantes evoluções e melhorias são solicitadas por clientes e usuários 
cotidianamente. 

Nesse contexto, pode-se entender que um software clássico, não baseado 
em microsserviços, produzia um único serviço executável, que continha todas 
as funções do sistema, o que pode ter suas vantagens para determinadas fina-
lidades, porém a incidência de erro em uma dessas funcionalidades poderia 
indisponibilizar o sistema como um todo. Outra situação é a necessidade de 
melhoria, em que qualquer nova versão do sistema demandava a necessidade 
de republicar o projeto como um todo. Sabe-se que grandes projetos de sofi-
ware podem ter a sua iniciação demorada, devido ao volume de código a ser 
levantado pelo servidor que provê a aplicação, levando, em alguns casos, de 
5 a 15 minutos ou períodos de tempo maiores. Muitas organizações possuem 
sistemas dessa dimensão e decidem publicar novas versões apenas fora do 
horário comercial, considerando que os minutos necessários para a publicação, 
que deixam a aplicação indisponível, não podem ocorrer para os clientes no 
horário de trabalho. 

Portanto, o microsserviço veio para proporcionar uma solução para es-
sas situações, sendo cada parte do software um pequeno serviço distinto, 
executando como um processo separado. Desse modo, se um serviço ficar 
indisponível, não necessariamente o software inteiro ficará indisponível. Além 
disso, quando for necessário publicar uma nova versão desse serviço, sendo 
este pequeno o suficiente, a sua republicação deve ocorrer em poucos instantes, 
sendo qualquer indisponibilidade praticamente imperceptível para o usuário. 

Mas como desenvolver software com microsserviços? Conceitualmente, 
pode-se desenvolver o software seguindo as diretrizes do microsserviço sem 
qualquer tecnologia adicional, simplesmente pensando a arquitetura do sistema 
em microsserviço. Contudo, faz-se necessário pensar em quais partes um 
software pode ser fracionado, para que estas funcionem de forma independente 
e, juntas, possam agregaro valor completo aos clientes e usuários do sistema. 

Nesse sentido, pode-se afirmar que uma das etapas mais importantes de um 
projeto de software baseado em microsserviços é saber segmentar as funções 
de software para que funcionem de forma independente. Todavia, isso pode 
não ser tão simples, pois a definição de microsserviço indica que este deve ser 
autônomo, ou seja, preferencialmente, os seus dados devem ser geridos pelo 
próprio microsserviço, evitando uma base de dados compartilhada por todos 
os microsserviços, o que ocasionaria um ponto único de falha para todos, 
sendo um grande ponto fraco na disponibilidade. 



102 ) ( Uso do Docker para microsserviços 
Portanto, o desenvolvimento de microsserviços trata-se da publicação de 
pedaços de software independentes, podendo estes serem publicados (prefe-
rencialmente) em computadores diferentes. Com isso em mente, hoje, qualquer 
desenvolvedor de software proficiente em uma linguagem de programação 
para publicação e consumo de serviços com conhecimento de banco de dados 
e algum domínio de sistemas operacionais e servidores de aplicação pode 
desenvolver e publicar software como microsserviços. 

Contudo, como publicar cada pedaço de software em um computador 
distinto? Isso não seria muito custoso? Por meio dessa reflexão, foram con-
cebidas uma série de tecnologias contemporâneas, como a virtualização. 
A virtualização de computadores ou servidores trata-se do processo de, em 
uma mesma máquina física, simular dois ou mais computadores, possibi-
litando que cada um contenha uma parte do software e funcione de forma 
totalmente independente um do outro. Em relação ao sistema operacional, 
a comunicação entre as máquinas virtuais possibilitará que a aplicação como 
um todo funcione em conjunto. 

A virtualização pode ser muito bem integrada com soluções em nuvem ou 
provedores de servidores corporativos. Na prática, cada máquina virtual pode 
rodar em computadores distribuídos, dando a cada microsserviço autonomia 
total em relação aos outros, inclusive aos recursos físicos. Ou seja, incidentes 
de natureza física, como interrupção no fornecimento de energia, falta de 
memória primária (HDs) ou secundária (RAM) e processador sobrecarregado, 
afetarão apenas um dos microsserviços, e não toda a aplicação. Sendo esta uma 
máquina virtual, na ocorrência de um desses incidentes, uma nova máquina 
pode ser instanciada com bastante facilidade. 

Nateoria, as coisas parecem se encaixar, porém muitos podem se perguntar: 
não seria complexo ter de desenvolver gerenciando tantas máquinas virtuais? 
Nesse sentido, foi concebido o conceito de contêineres (dockers), que seriam 
uma maneira mais leve e simples de se trabalhar com máquinas virtuais. 
No entanto, é preciso ressaltar que microsserviço é um conceito e pode ser 
implementado independentemente da tecnologia. O mais importante em um 
projeto de microsserviço é a arquitetura, segregada em microsserviços de 
forma lógica e coerente. Existem tecnologias (abordadas nas seções seguintes) 
para facilitar a implementação, porém, se o projeto de arquitetura não for 
bem feito, haverá uma grande aplicação, chamada de monolito, não baseada 
em microsserviço, sendo publicada de forma virtualizada e, talvez, com uso 
de contêineres, mas sem as práticas de microsserviços. A Figura 2, a seguir, 
ilustra como um componente de software desenvolvido como microsserviço 



Uso do Docker para microsserviços ) ( 103 

pode ser publicado com suporte de contêineres, que possibilitam a instanciação 
do serviço em diversos ambientes de forma homogênea. 
/ 

Pere= E nenpnes ) | Guião) cido 

Figura 2. Exemplo de contêiner com microsserviço. 

Fonte: Adaptada de [Microservices] ([201-?]). 

NU J 

2 Criando microsserviços com o apoio 
da ferramenta Docker 

Agora, serão apresentados os conhecimentos básicos necessários para dis-
ponibilizar um software como microsserviço. Atualmente, é muito comum 
que os microsserviços sejam disponibilizados como uma “imagem docker”, 
que pode ser instanciada em qualquer outro ambiente com bastante facili-
dade. Poderíamos, aqui, nos limitar a abordar uma “receita de bolo”, ou seja, 
um conjunto básico de procedimentos para a criação desta imagem, porém 
nosso objetivo de aprendizagem não se limita à execução desses procedimentos, 
e sim à compreensão dos conceitos e ao entendimento de cada etapa necessária 
durante o processo. 



104) (uso do Docker para microsserviços 
Primeiramente, faz-se necessário entender por que utilizar um contêiner. 
Antes da existência de ferramentas como o Docker, uma imagem de serviço 
seria disponibilizada por meio de uma máquina virtual, que pode ser criada 
utilizando ferramentas como Virtualbox ou Vmware workstation. Quando 
criamos uma máquina virtual e salvamos toda a sua configuração como 
uma imagem, estamos criando uma cópia completa do sistema operacional 
escolhido para essa máquina, assim como todo o software e as aplicações 
disponibilizadas. Entretanto, ter uma cópia completa do sistema operacional 
por máquina virtual pode tornar o processo de armazenamento e instanciação 
um pouco complexo e pesado. Em busca de uma alternativa mais leve para 
esse procedimento, foram estabelecidos os contêineres, uma alternativa às 
máquinas virtuais. Um contêiner não contém uma cópia completa do sistema 
operacional, mas sim apenas um conjunto de módulos e dependências neces-
sários para a execução da imagem (MUNIZ et al., 2020). A Figura 3, a seguir, 
ilustra como os contêineres são uma alternativa mais leve quando o objetivo 
é publicar microsserviços. 

— » 
VIRTUALIZAÇÃO TRADICIONAL CONTÉINER 
à 
Máquina virtual Máquina virtual Contêiner Contêiner 
tema operacional 
Hardware 
Figura 3. Contêiner vs. virtualização. 
Fonte: Adaptada de [Containers] (2014) 


Uso do Docker para microsserviços )l 105 

Nesse sentido, uma imagem é um pacote que contém todas as dependên-
cias para a execução de um software ou serviço. Além das dependências, 
a imagem traz as configurações realizadas, para possibilitar que o software, 
tendo as dependências necessárias, possa iniciar a sua execução. Em geral, 
uma imagem é organizada em camadas, contendo os módulos do sistema 
operacional, os servidores de web ou de aplicação, além do software de usuário, 

que disponibiliza o serviço em si. Uma imagem é gerada após o serviço ser 
configurado e testado, deixando este pronto para consumo, pois, ao ser criada, 
torna-se imutável. 

Saiba mais 

A vantagem de se ter o serviço disponibilizado como imagem é a sua facilidade de 
escalabilidade, pois possibilita que possa haver diversas máquinas executando o 
mesmo serviço. Além disso, quando uma máquina falhar, outra pode ser iniciada em 
um estágio pronto para execução, minimizando, assim, a indisponibilidade do serviço. 

O objetivo de uma imagem é ser instanciada uma ou mais vezes. Uma 
instância de imagem é chamada de contêiner, que representa a execução de 
um único aplicativo, processo ou serviço. A criação de contêineres com base 
em uma imagem é um trabalho de dimensionamento e depende de quando 

o serviço será demandando pelos usuários. Quanto maior for o volume de 
requisições, maior será a concorrência entre essas requisições, um indício 
de que mais contêineres são necessários, a fim de possibilitar a escalabili-
dade horizontal. Entender a diferença entre as escalabilidades horizontal e 
vertical é fundamental para se projetar sistemas baseados em contêineres. 
A escalabilidade vertical refere-se à quando é preciso aumentar a capacidade 
de um serviço, seja em processamento, memória ou qualquer outro recurso, 
e isso é feito por meio da ampliação dos recursos de hardware de um mesmo 
computador, tornando-o mais potente. Entretanto, a escalabilidade vertical 
costuma ter custos financeiros elevados e, até certo ponto, encontra limitações 
de expansão. Nesse sentido, a escalabilidade horizontal é uma alternativa, 
pois é capaz de ampliar as capacidades de um software por meio da criação 
de diferentes instâncias em diferentes computadores, possibilitando que, 
em conjunto, esses computadores possam atender às demandas de consumo 
de usuários e clientes. 

106 ) (uso do Docker para microsserviços 
Ao criar diversos contêineres para um serviço, constitui-se um cluster, 
isto é, uma coleção de contêineres expostos em um único host virtual. Por 

meio do cluster, um determinado software, chamado de orquestrador, pode, 
com base em dados, inferir o dimensionamento adequado para a criação dos 
contêineres. Essa orquestração pode ter um caráter dinâmico e ser difícil de 
gerir manualmente, por isso, existem software orquestradores que facilitam e 
automatizam essa tarefa, a exemplo do Kubernetes. O software orquestrador 
facilita a análise e a tomada de decisões quanto ao balanceamento de cargas 
de instanciação de contêineres e ao monitoramento em geral, possibilitando 
a gestão de ciclo de vida de contêineres que executam em um cluster. 

Quando se fala em clusters, é preciso entender que, muitas vezes, os dife-

rentes contêineres precisarão acessar um mesmo repositório de dados, ou seja, 

um sistema de arquivos comum. Nesse sentido, ao trabalhar com múltiplos 

contêineres, faz-se necessário estar ciente do uso de volumes, que são entendi-

dos como um sistema de arquivos disponíveis para leitura e escrita, que podem 

ser acessados pelos contêineres de determinada imagem. O uso de volumes 

possibilita que a aplicação e os usuários não tenham prejuízo ao destruir um 

contêiner e iniciar outro, pois nenhuma informação será gravada diretamente 

nele, apenas por meio do uso dos volumes. A Figura 4, a seguir, ilustra como o 

continer fica desvinculado do sistema de arquivos onde o volume é utilizado. 

Isso é fundamental para que um microsserviço publicado em contêiner tenha 

alta disponibilidade, podendo ser executado em múltiplas instâncias, porém 

sem manter informações de instâncias em qualquer contêiner em particular. 

4 D 

[6] Contêiner 
tmpfs 
mount 

Memória 

Área do Docker 

Figura 4, Acesso a volumes. 

Fonte: Adaptada de Docker (20203). 
A J 


Uso do Docker para microsserviços ) ( 107 

Utilizando a ferramenta Docker para 
a criação de contêineres 

A ferramenta que utilizaremos para criação de imagens e instanciação de 
contêineres neste capítulo será o Docker Community Edition (CE) (Docker 
Inc.), disponível para os sistemas operacionais Windows, Linux e Mac OS. 

Essa ferramenta possibilita a criação de imagens, a execução e o teste de 
contêineres em um computador local. A ferramenta Docker é muito útil, pois 
possibilita, por exemplo, que, ao ser executado em um ambiente Windows, 
sejam geradas imagens baseadas em dependências de um sistema operacional 
Linux, e que estas sejam executadas por meio de contêineres (ROGÉRIO, 
2018). A geração de imagens e a execução de contêineres Linux no sistema 
operacional Windows utilizam como dependência o próprio Hyper-V, de modo 
que esse recurso deve estar disponível na instalação do Windows utilizado. 
Usuários do sistema operacional Mac OS também conseguem trabalhar com 
contêineres Linux com base no Apple Hypervisor. 

Para usuários do Windows, a ferramenta Docker pode ser obtida no site 
oficial do sistema. Após realizar todo o procedimento de instalação, a ferra-
menta deve estar pronta para uso. Se a instalação ocorreu da forma esperada, 

o documento Docker deve estar disponível por meio do console do sistema 
operacional (prompt de comando -cmd) e, ao executar a instrução docker 
-version, deve ser exibida a versão do Docker instalado em seu compu-
tador. Qualquer outra mensagem (de erro ou de comando não reconhecido) 
pode indicar que a instalação não foi bem-sucedida. Caso isso ocorra, visite 
novamente o site da ferramenta Docker e siga as instruções de download e 
instalação. 
Nesta seção, veremos exemplos de uso da ferramenta Docker para a criação 
de stack de microsserviços básica. Os exemplos aqui demonstrados foram 
baseados no site oficial da ferramenta Docker, sendo estes incrementados 
e comentados com explicações que buscam alinhar a teoria com a prática. 
Para testar uma stack de microsserviços básica, faz-se necessário criar um 
contêiner com base em alguma imagem. O Docker contém um repositório 
de imagens públicas, já preconcebidas, conhecido como DockerHub. É im-
portante destacar que, no DockerHub, é possível registrar novas imagens e, 
inclusive, fazer integrações com outros populares repositórios de aplicações, 
como GitHub e Bitbucket. A ideia é que, com base em imagens básicas de 
sistemas operacionais, outras imagens possam ser criadas para prover serviços 
(QUAIATO, 2016). 



108) (uso do Docker para microsserviços 
O comando run do software Docker possibilita que, dada uma imagem, 
seja executado um contêiner. Uma imagem bastante simples baseada em 
sistema operacional Linux é disponibilizada no DockerHub com o nome de 
hello-world. Em seu computador, execute esse contêiner por meio do 
seguinte comando: docker run hello-worlad. Essa instrução verifica 
se a imagem existe no seu sistema operacional; em caso negativo, ela busca a 
imagem no DockerHub para download, realizando, assim, a sua instanciação. 
Se tudo ocorrer bem, o console do seu sistema operacional deve apresentar 
as seguintes mensagens: 

> docker run hello-world 

docker : Unable to find image 'hello-world:latest' locally 

latest: Pulling from library/hello-world 

1b9304010525: Pull complete 

Digest: sha256:c3b4ada4687bbaal70745b3e4dd8ac3£194ca95b2dos1 

8h417fb47e587949b5£ 

Status: Downloaded newer image for hello-world: latest 

Hello from Docker! 
This message shows that your installation appears to be working 

correctly. 

Feito o download da imagem e executada a sua instância por meio da criação 
do contêiner, é possível monitorar as imagens existentes em seu computador, 
assim como os contêineres em execução. O comando docker image 1s 
apresenta todas as imagens criadas ou baixadas para o seu computador. Já o 
comando docker container 1s -all1 apresenta todos os contêineres 
que estão em execução, iniciados pelo comando run. Para qualquer um dos 
comandos aqui apresentados, você pode encontrar mais opções de uso ao 
incluir o parâmetro --he1p, como, por exemplo: docker container--
-help. Comandos adicionais podem ser consultados na tabela de comandos 
apresentada a seguir. 



Uso do Docker para microsserviços ) ( 109 

Comando Explicação 

docker run Executa um comando em um novo contêiner. 

docker start Indicia um contêiner que esteja parado. 

docker stop Para um ou mais contêineres em execução. 

docker build Cria uma imagem a partir de um Docker file. 

docker pull Extrai uma imagem ou repositório de um registro. 

docker push Envia uma imagem ou repositório para um registro. 

docker export Exporta um sistema de arquivos de um contêiner para um 
arquivo .tar. 

docker exec Executa um comando em um contéinerem execução. 

docker search Busca imagens no DockerHub. 

docker attach Anexa um terminal de input ou output a um contêiner. 

docker commit Cria uma nova imagem considerando as alterações 
realizadas em um contêiner. 

Outro exemplo de imagem pode ser feito criando um Docker baseado no 
sistema operacional Linux, mais especificamente na distribuição Ubunto. 
Por meio do Docker, é possível criar um contêiner baseado na imagem do 
Ubunto e já inicializá-lo no modo interativo pelo console, possibilitando que 
sejam executados quaisquer comandos disponíveis para essa distribuição do 
sistema operacional. Para tanto, execute o seguinte comando com o seu Docker: 
docker run --interactive --tty ubuntu bash. Assim como 
no exemplo he11o-world, na primeira vez que esse comando for executado, 
a imagem do Ubunto será baixada do DockerHub. Uma vez inicializado o 
contêiner, você pode interagir com o sistema operacional Linux não só por esse 
console, mas por qualquer outra ferramenta de conexão, como clientes SSH, 
por exemplo. Desse modo, percebe-se como o Docker facilita a instalação de 
uma máquina virtual Linux para uso em um sistema operacional Windows. 

Um último exemplo para essa seção será a criação de um contêiner que dispo-
nibilizará, além do sistema operacional, um web server HTTP em execução. Para 
isso, utilizaremos a imagem do web server Nginx (2020). Para criar o web server 
com base nessa imagem, utilize o seguinte comando: docker run --detach 
--publish 80:80 --name webserver nginx. Ão executar esse 
comando, as seguintes mensagens devem ser exibidas em seu console: 


no ) (uso do Docker para microsserviços 

> docker run --detach --publish 80:80 --name webserver nginx 
Unable to find image 'nginx:latest' locally 
latest: Pulling from library/nginx 
fdd5d7827£33: Pull complete 
a3ed95caeb02: Pull complete 
716£7a5f3082: Pull complete 
7b10£03a0309: Pull complete 
Digest: ... 
Status: Downloaded newer image for nginx:latest 

Esse comando traz os parâmetros --publish 80:80 até então não 
utilizados. O objetivo do parâmetro publish é fazer um redirecionamento de 
portas para que, quando um requisição for feita para a porta 80 do computador 
local, a requisição seja redirecionada para a porta 80 desse contêiner. Nesse 
sentido, é possível acessar o web server inclusive quando ele é consumido por 
outras aplicações. Para tanto, por meio do seu computador local, em qualquer 
navegador web, basta acessar o endereço 127.0.0.1:80 ou localhost:80 para que 
a tela inicial do web server seja exibida (MACORATTI, 2018). 

Quando o sistema operacional Linux/Ubunto está em execução, a criação 
de outros elementos que compõem a stack básica de serviços pode variar de 
acordo com as tecnologias desejadas. Um exemplo de stack muito utilizada é 
um servidor web Apache, com suporte à linguagem PHP (do inglês Hypertext 
Preprocessor) e um banco de dados MySQL para persistência de dados. 

O primeiro passo para a criação dessa stack, após ter um contêiner Ubunto 
em execução, é a instalação do web server Apache. Para isso, utilizando o 
console do sistema operacional, execute os seguintes comandos, um de cada 
vez: sudo apt-get update; sudo apt-get install apache2; 
sudo service apache2 status. O primeiro comando atualizará 
a referência dos repositórios; o segundo fará o download e a instalação do 
servidor web Apache; e o terceiro exibirá no console o status da execução 
do serviço. Se tudo ocorrer como esperado, uma mensagem exibindo “ative 
(running) deve ser exibida. Essa etapa estará concluída quanto o web server 
estiver respondendo às requisições feitas via cliente de browser. 

Para garantir que o web server está respondendo nas portas 80 ou 443, 
pode-se configurar o firewall do Apache. Para isso, execute os seguintes 
comandos: sudo ufw app list; sudo ufw app info “Apache 
Full”. Após o segundo comando, deve ser exibido no browser que as portas 
80 e 443 estão ativas. A conexão pode ser testada por um navegador de internet 
pelo endereço http://127.0.0.1:80. 


Uso do Docker para microsserviços ) [im 

O segundo passo para a criação da stack é instalar o servidor de banco 
de dados. Para tanto, pode-se executar, no console do sistema operacional, 

o seguinte comando: sudo apt-get install mysql-server. Esse 
comando iniciará o processo de instalação do servidor de banco de dados 
MySQL. Durante o procedimento, você será solicitado para fornecer a senha 
do usuário root, que tem pleno acesso à administração desse servidor. Anote 
essa senha em local seguro, pois você precisará dela posteriormente para 
fazer conexões. 
Para finalizar a instalação da stack, é preciso habilitar o suporte à lingua-
gem PHP para o web service Apache, o que pode ser por meio da execução 
do seguinte comando: sudo apt-get install php libapache2-
-mod-php php-my sql. Esse comando não apenas fará o Apache suportar 
a interpretação dos arquivos com a extenção . php, mas também a conexão 
entre o PHP e o MySQL. Para essa instalação surtir os efeitos, faz-se necessário 
reiniciar o servidor Apache por meio do seguinte comando: sudo syste-
mctl restart apache2. 

Agora, a stack de serviços está criada, e você pode implementar o seu 
próprio microsserviço. Que tal criarmos um microsserviço para gerir o login 
de uma aplicação? Novos arquivos de programação . php podem ser criados 
no diretório /var /www/. Nesse diretório, crie um arquivo com o nome 
login.php. Esse arquivo terá os seguintes objetivos: a) criar uma tabela de 
logins, caso esta não exista; b) desenhar a tela com os campos de login e senha 
e um botão para submeter os dados; c) criar uma lógica PHP que analisará 
os campos enviados na requisição para verificar se conferem com os dados 
existentes para algum usuário existente no banco de dados. Um exemplo do 
código-fonte que deve constar no arquivo é apresentado a seguir: 

<html> 

<body> 

<?php 

//Criar a tabela no banco de dados 
$servername = "localhost"; 
Susername = "root"; 
Spassword = "password";//a senha que você definiou ao instalar 

o MySQL 
$dbname = "myDB"; 
//Conectando no banco de dados 
$conn = new mysqli ($servername, Susername, $password, Sdbname) 
; 

n2) (uso do Docker para microsserviços 

if ($conn->connect error) die("Connection failed: ". 
$conn->connect error); 
// sql para criar tabela 
$sql = "CREATE TABLE IF NOT EXISTS users ( 

id INT(6) UNSIGNED AUTO INCREMENT PRIMARY KEY, login VARCHAR (30) 

NOT NULL, password VARCHAR(30) NOT NULL)"; 

if ($conn->query ($sql) TRUE) ( 
echo "Tabela users criada com sucesso."; 
/!com a tabela criada, inserimos um usuário de testes 
$sql = "INSERT INTO users (login, password) VALUES ('Test', 

11231)"; 

Sconn->query ($sql) ; 
) else ( echo "Erro ocorreu ao criar a tabela: ". Sconn-berror;) 
if(isset($ POST["login"]))( 

$sql = "SELECT id FROM users where login="".$ POST["login" 

and senha: -S POST["senha"]."tm; 
$result = $conn->query ($sql); 
if ($result->num > 0) (

rows 
echo "Usuário autenticado com sucesso!"; 
telse( 
echo "Não foi achado usuário com as credenciais 
informadas! 

) 
; 
$conn->close() ; 
?> 
<!--Esta é a tela para o usuário tentar a autenticação --> 

<form method: OST"> 
<label for: "login">Login</label> <input type="text" 
name="login" id="login" /> 
<label for="senha">Senha</label> <input 
type="password" name="senha /> 
<input type="submit" /> 
</form> 
</body> 
</html> 



Uso do Docker para microsservios | (ms 

O exemplo apresentado foi o mais simples possível. O código-fonte poderia 
ter sido organizado de maneiras diferentes, otimizando diversos aspectos 
da programação, da arquitetura de sistemas, da usabilidade, da segurança, 
etc. Contudo, considerando que o foco desta seção é a criação da stack para 
microsserviço, optou-se por uma versão simplificada para essa solução. 

Para concluir, pode-se pensar como tornar essa stack de serviços criada 
no nosso contêiner em uma nova imagem para ser facilmente instanciada 
em outros computadores. Para isso, pode-se utilizar o comando docker 
commit ou docker build, que possibilita a criação de suas próprias 
imagens. Ou seja, após iniciar um contêiner com base em imagens preexis-
tentes, você pode desenvolver os seus serviços, que podem ser encapsulados 
em imagens próprias pelo comando docker build, que gerará uma nova 
imagem com base no contêiner do contexto e com diretivas para a geração de 
imagens disponibilizadas no arquivo Docker file. Esse arquivo é baseado no 
formato YAML, uma variação do popular formato XML. Os detalhes técnicos 
completos sobre o Docker file podem ser consultados no manual oficial do 
produto (DOCKER, [2020b)). 

Saiba mais 

Outro comando muito popular é o docker compose, análogo ao Docker run, 
estudado nesta seção. Esse comando possibilita que uma imagem seja iniciada em 
diversos contêineres. 

3 Gerenciando o ciclo de vida de contêineres 

Para compreender as características do ciclo de vida de um contêiner e como 
gerenciá-lo, faz-se necessário, primeiro, entender que, conceitualmente, um 
contêiner se comporta como um processo de um sistema operacional. Esse 
processo pode estar em uma série de estados ao longo de seu ciclo de vida, 
como novo (após criação pelo sistema operacional), pronto (quando se encontra 
apto para ser selecionado para execução), em execução (quando o escalonador 
de processos seleciona o processo para ser executado), em espera (quando está 



n14) (uso do Docker para microsserviços 

aguardando uma resposta de algum periférico ou evento) ou terminado (após 
concluir sua execução). 

No modelo de contêiner, cada contêiner é representado por um único pro-
cesso. Ao executar um contêiner do Docker, existe a opção de configurar um 
entrypoint, que pode sobrescrever o entrypoint padrão da imagem. 
Ele pode ser definido diretamente no Docker file ou ser passado como parâ-
metro ao se iniciar um contêiner pela opção run. O entrypoint indica um 
processo que será inicializado assim que o contêiner for instanciado e que será 
considerado para a gestão do ciclo de vida do contêiner. Quando o processo 
for concluído, o ciclo de vida do contêiner será encerrado. 

Ferramentas como orquestradores auxiliam na gestão do ciclo de vida de 
contêineres. Por exemplo, se o orquestrador foi configurado para manter três 
contêineres em execução e um destes falhar, ele criará outro contêiner para 
substituir aquele que apresentou falha. 

Um contêiner pode ter os seguintes estágios ao longo de seu ciclo de vida: 

E Criado (created): quando é criada uma nova instância de uma imagem, 
concebendo um contêiner, mas sem iniciar a sua execução de imediato. 
Esse estado pode ser alcançado por meio da execução do comando 
docker create. 
m Em execução (running): quando um contêiner inicia a sua execução, 
processando instruções e respondendo a requisições. Esse estado pode 
ser alcançado por meio da execução dos comandos docker start 
ou docker run. 
E Parado (stopped): esse estágio do ciclo de via é normalmente alcançado 
quando o contêiner é encerrado por algum problema no processo, como 
exceções, tais como falta de memória ou instruções que resultam em 
erros não tratados. Esse estado pode ser alcançado manualmente por 
meio da execução do comando docker stop e revertido por meio 
do comando docker start, sendo este reiniciado em um estado 
válido com base na configuração inicial da imagem. 
E Pausado (paused): quando a execução do contêiner é interrompida, 
mas pode ser retomada posteriormente. Esse estado pode ser alcançado 
por meio da execução do comando docker pause e revertido por 
meio do comando docker unpause. 



Uso do Docker para microsserviçs | ( 115 

E Deletado (deleted): quando um contêiner termina de executar todas as 
suas instruções ou o processo é encerrado por algum erro (após passar 
pelo evento stopped). Um contêiner pode ser movido manualmente para 
esse estágio por meio da execução do comando docker rm. 

A Figura 5, a seguir, apesenta um mapa da máquina de estados, que ilustra 
quais são as possibilidades de transição entre cada estágio de um contêiner, 
proporcionando, assim, um entendimento completo desse ciclo de vida. 

mM D 

E pe "O 

e=" 

Figura 5. Ciclo de vida de containers 

Fonte: Adaptada de Docker Saigon (2016) 
a , 

Além disso, é possível gerir os estágios em que estão cada um dos contêine-
res em seu Docker por meio dos comandos docker container ls -aou 
docker ps -a, que lista todos os contêineres geridos pelo sofiware Docker, 
exibindo, inclusive, o seu estado no ciclo de vida. Observe que o parâmetro 
“-a” é fundamental para poder visualizar os contêineres nos seus diferentes 
estágios, uma vez que a ausência dele pode ocasionar filtro de contêineres 
que se encontram em alguns estágios. O Quadro 1, a seguir, apresenta um 
exemplo de execução desse comando. 



6 ) ( Uso do Docker para microsserviços 
qem d>yos sinoyt dn obp smnoyt u"F0 uoueep, b- xutTbu,, bu | egogogpojaso 
aúpe> | d/6/€9 sunoy p dn obo sinoyp uS' QUTOdÃIJUS-I9%D0P, spa! PISLIBEIL/S 
bd | dayzeps Jnoy e dn ob smnoyt u"'S* QUTOdÃIJUO-I9%D0P, saubisod | ggepespapago 
ap obo sinoy € (0) pauixg obb ssnoys u“DNnoD Us" quTodÃIque/ asequanoo SUL6eecagra 
S| JQUIRJUOD J3D0Q OPUBUJOD Op OPÍNDaxa ap ojduax3 *| ospend 
Cilique aqui para vi isualizar est te conteúdo na horizontal. 

Uso do Docker para microsserviços ) (17 

Referências 

[CONTAINERS)]. In: PITANG. [S. |:s. n), 2014. Disponível em: https:/Awww pitang.com/ 
documents/portlet

file entry/20142/containers.png/41a52a82-7a7b-89bf-cdea-002ac-
cec9fd3. Acesso em: 7 set. 2020. 

DOCKER SAIGON. Docker internals. 2016. Disponível em: http://docker-saigon.github. 
io/post/Docker-Internals/ Acesso em: 7 set. 2020. 

DOCKER. Manage data in Docker. California: Docker, [2020]. Disponível em: https:// 
docs.docker.com/storage/. Acesso em: 7 set. 2020. 

DOCKER. Dockerfile reference. California: Docker, [20206]. Disponível em: https://docs. 
docker.com/engine/reference/builder/, Acesso em: 7 set. 2020. 

[SOAP). In: THIS TECHNOLOGY LIFE. [S. [:s. n), 2019. Disponível em: https://i2.wp.com/ 
www.thistechnologylife.com/wp-content/uploads/2019/04/SOAP-vs-REST-1.png. 
Acesso em: 7 set. 2020. 

MACORATTI, J. C. Docker: como compartilhar dados entre o Container e o host. 2018. 
http:/Awww.macorattinet/19/04/docker compart!.htm. Acesso em: 7 set. 2020. 

[MICROSERVICE]. In: IBM. [S. Ls. n)), 2019. Disponível em: https:/Avww.ibm.com/deve-
loperworks/br/cloud/library/cl-bluemix-microservices-in-action-part-2-trsAwhat-is-
-docker.png. Acesso em: 7 set. 2020. 

MUNIZ, A. et al. Jornada DevOps. 2. ed. Rio de Janeiro: BRASPORT, 2020. 

NGINX. California: NGINX, 2020. Disponível em: https:/Awww.nginx.com/. Acesso em: 
7 set. 2020. 

QUAIATO, V. Iniciando com Docker. in: LAMBDA. [S. :s. n, 2016. Disponível em: https:// 
www.lambda3.com.br/2016/01/iniciando-com-docker. Acesso em: 7 set. 2020. 

ROGÉRIO, F. Conhecendo, instalando e configurando Docker. 2018. Disponível em: https:// 
fabiorogeriosj.com.br/2018/02/15/Conhecendo-Instalando-e-Configurando-Docker. 
Acesso em: 7 set. 2020. 

Leitura recomendada 

KIM, G. et al, Manual de DevOps: como obter agilidade, confiabilidade e segurança em 
organizações tecnológicas. Rio de Janeiro: Alta Books, 2018. 


n8) (uso do Docker para microsserviços 

Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 


Prática de microsserviços 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Demonstrar aplicações de microsserviços. 
Em Diferenciar microsserviços de arquitetura monolítica. 
E Identificar os benefícios da utilização de microsserviços. 

Introdução 

Neste capítulo, você vai estudar cenários de aplicabilidade da arquitetura 
baseada em microsserviços. A ideia é que, a partir desse estudo, você 
consiga identificar situações em que é possível usufruir dos benefícios 
proporcionados por esse modo de organizar soluções de software. 

A primeira seção do capítulo apresenta um estudo de caso relativo ao 
desenvolvimento de uma solução baseada em microsserviços. A segunda 
seção faz uma análise comparativa entre o modelo arquitetural monolí-
tico e o modelo arquitetural baseado em microsserviços, ressaltando as 
vantagens e desvantagens de cada abordagem. Por fim, a terceira seção 
aborda os benefícios do uso da arquitetura baseada em microsserviços, 
destacando sua aderência a padrões modernos de desenvolvimento e 
entrega de software, como os métodos ágeis e a cloud computing. 

Ao longo do capítulo, você vai conhecer os cenários em que pode 
aplicar a arquitetura baseada em microsserviços e também os desafios 
e custos inerentes a essa arquitetura. Você ainda vai verificar que o de-
senvolvimento baseado em microsserviços pode demandar um esforço 
significativo para a entrega da primeira versão do software. Ademais, ele 
pode demandar uma gestão eficiente para manter o controle de versões 
de serviços e a difusão de conhecimento entre as equipes. Como você 
vai ver, o desenvolvimento baseado em microsserviços proporciona 
muita autonomia às equipes, de modo que um mesmo projeto pode ter 
seu desenvolvimento segregado entre diversos times, o que possibilita 
entregas mais rápidas. 



120 ) (prática de microsserviços 
1 Aplicações de microsserviços 

Os sistemas de software estão cada vez mais complexos, demandando diversas 
integrações com fontes de dados heterogêneas. Tais fontes de dados incluem 
bancos de dados não relacionais, dados não estruturados e sistemas legados. 
Ocorre ainda intercâmbio de dados entre tecnologias até então incompatíveis. 
Além disso, há a necessidade de atender a requisitos como segurança e alta 
disponibilidade em um contexto em que o volume de dados transmitidos e 
processados cresce de forma muito acelerada. 

Para atender às necessidades desse cenário desafiador, a arquitetura de 
microsserviços se mostra uma boa opção. Como analisa Newman (2020), 
a arquitetura baseada em microsserviços se torna mais atraente à medida que 
a complexidade do sistema aumenta. Para compreender isso melhor, observe a 
Figura 1: apesar de o esforço e a complexidade do projeto aumentarem quando 
a arquitetura de microsserviços é utilizada, seus benefícios compensam a 
escolha a partir do momento em que o sistema atinge determinado nível de 
complexidade. 

Produtividade 
Para sistemas menos

complexos, a carga extra

necessária para gerir os micros-

serviços tende a reduzir a 

produtividade 

Quando à complexidade 

aumenta, a produtividade cai

acentuadamente 

Com o baixo acoplamento dos 

microsserviços, a redução da

produtividade é menor 

Complexidade É importante lembrar que o conhecimento do time de desenvolvimento fará.

diferença em qualquer uma das opções escolhidas 
Figura 1. Ganho de produtividade versus aumento de complexidade do sistema. 

Fonte: Adaptada de Fowler (2015) 


Prática de microsserviços )l 121 

A arquitetura baseada em microsserviços pode ter um custo maior no 
início do projeto (quando comparado ao custo de um sistema monolítico). Isso 
ocorre pois é necessário um esforço extra para a gestão dos microsserviços, 
etal esforço deve ser considerado. Naturalmente, esse custo pode ser bastante 
reduzido com o uso de ferramentas e frames de apoio à construção de soluções 
baseadas em microsserviços. Essas ferramentas são fornecidas por importantes 
players do mercado. Como exemplo, você pode considerar o Azure Service 
Fabric, da Microsoft. Essa solução apoia a gestão de microsserviços. Ela pode 
ser utilizada tanto em ambiente local como em nuvem, facilitando muito a 
configuração do ambiente de execução e publicação do microsserviço. 

Fique atento 

Ao adotar uma solução baseada em microsserviço, você precisa conhecer os riscos 
inerentes a essa prática. Entre tais riscos, está a complexificação da coordenação e da 
gestão desses serviços, da gestão da comunicação entre os microsserviços e de toda 
a gerência de configuração de cada um dos microsserviços que compõem a solução. 

Escolha de tecnologias para o desenvolvimento 
de microsserviços 

O desenvolvimento baseado em microsserviços pode ser realizado com uma 
grande variedade de linguagens de programação e tecnologias. Algumas 
linguagens de programação podem contar com ferramentas ou frameworks 
que facilitam essa abordagem, enquanto outras podem demandar um trabalho 
mais árduo, exigindo mais experiência do desenvolvedor do software para a 
construção dos microsserviços. É claro que a escolha da tecnologia não é algo 
simples; tal escolha deriva de uma série de fatores, como padrão de tecnologiajá 
adotado por determinada organização, experiência dos profissionais envolvidos 
nos projetos e requisito não funcional definido pelo cliente. 

O exemplo de aplicação apresentado nesta seção é baseado na tecnologia 
Java, com apoio do framework Spring Boot. Esse framework foi selecionado 
justamente pois reduz o custo inicial de um projeto baseado em microsservi-
ços, possibilitando iniciar uma aplicação complexa com pouco esforço. Ele 
também possibilita dedicação maior à implementação dos microsserviços que 
buscam atender a requisitos funcionais de aplicação, em detrimento de esforço 



122 ) ( Prática de microsserviços 
em configuração de ambiente e infraestrutura para aplicação. Utilizar esse 
framework é muito simples. Em um projeto Java padrão, com suporte ao gestor 
de dependências Maven, basta, por exemplo, incluir a dependência spring-
-boot-starter-web. Com isso, um projeto web baseado nos serviços 
de aplicação Tomcat já estará à disposição para publicação de web services 
ou qualquer outro tipo de microsserviço a ser ofertado pelo protocolo HTTP. 

Aplicação de microsserviços: um estudo de caso 

O estudo de caso apresentado nesta seção é baseado na experiência relatada 
por Moreira e Beder (2015), documentada em periódico científico na área de 
Tecnologia da Informação (TI). O estudo de caso busca a construção de uma 
solução baseada em microsserviços para consulta, tratamento e transformação 
de dados oriundos de um sistema de gestão de dados de pesquisas. Além disso, 
busca produzir indicadores estatísticos que possam ser utilizados para compor 
relatórios gerenciais em formatos diversos. 

O sistema que faz a gestão dos dados de produção científica é chamado 
Rescarcher Content Management System (RCMS). Ele contém informações 
de trabalhos acadêmicos produzidos por integrantes de grupos de pesquisas 
de determinada universidade. Nesse sentido, os microsserviços a serem de-
senvolvidos por essa aplicação devem possibilitar o cálculo de indicadores 
estatísticos de produção acadêmica por pesquisador, por grupo de pesquisa, 
por área de atuação e por período de tempo a ser parametrizável. 

O sistema RCMS, por ser uma aplicação legada, não foi desenvolvido 
com base nos paradigmas da arquitetura de microsserviços, e sim como uma 
aplicação web monolítica, baseada nas tecnologias Java EE 7, Spring 4, JSF 2, 
PrimeFaces 5.2 e Bootstrap 3.2.0. Como é comum em aplicações monolíticas, 

o projeto como um todo é compilado como um único “executável”, um arquivo 
com a extensão . war que é publicado em um servidor de aplicação, no caso 
um servidor JEE GlassFish versão 4. Na conjuntura em que esse sistema foi 
desenvolvido — baseado em três camadas, separando seus componentes em 
visualização, controle e modelo de dados (persistência) —, ele foi suficiente 
para atender aos seus propósitos, considerando que seria instalado m um único 
servidor de aplicação e em um único servidor de banco de dados. 
Entretanto, com o crescimento do sistema, a manutenção se tornou cada 
vez mais difícil, sendo necessário implementar novas funcionalidades. Essa 

demanda foi oportuna para a refatoração do projeto, possibilitando o desen-
volvimento de novas funcionalidades como microsserviços e a adaptação do 
projeto para consumi-los. Neste estudo de caso, a nova funcionalidade foi 



Prática de microsserviços )( 123 

criada como um microsserviço, sendo chamado de “scorecard”. Na Figura 2, 
veja o sistema após a introdução do microsserviço. 

Scorecard commons RCMS commons 

POJOs Classes utilitárias 

Micro serviço -Scorecard RCMS 

webapp(html, css, etc) 

Web services. ManagedBeans 
Services Web services 
Entties Services 

DAOS Emite 
DAOS 

ES ES

Ff FA 

Figura 2. Arquitetura do sistema com o uso de microsserviços. 
Fonte: Adaptada de Moreira e Beder (2015) 
a x 

O projeto para fornecer os microsserviços contém unicamente os compo-
nentes dedicados ao seu escopo, como as classes de objetos de acesso a dados 
(Data Access Objects — DAOS), as classes de entidades de negócio e os web 
services para provimento de dados por requisições HTTP. Uma característica 
muito importante da arquitetura baseada em microsserviços, como indica a 
Figura 2, é que o microsserviço possui sua própria base de dados, tornando-
-se independente do sistema RCMS. Desse modo, caso o banco de dados do 
sistema RCMS esteja indisponível, o microsserviço continuará provendo os 
dados aos seus clientes. 



124 ) ( Prática de microsserviços 
Entretanto, vale destacar uma complexidade adicional intrínseca à arqui-
tetura de microsserviços: para que haja uma base de dados independente, 
com maior autonomia, é necessário desenvolver um integrador que copie 
os dados de uma base de dados para outra e mantenha-os sincronizados. 
Outra característica muito importante é que o microsserviço é desenvolvido 
e implantado de forma completamente independente da aplicação principal, 
podendo ser implantado separadamente. 

Análise dos resultados do estudo de caso 

Por meio do estudo de caso apresentado, você pode identificar os pontos 
fortes e fracos de sistemas orientados a microsserviços. É muito importante 
que você fique atento a esses pontos. Desse modo, ao projetar uma solução, 
você vai perceber quando os custos da arquitetura monolítica forem maiores 
do que seus benefícios. 

O estudo de caso apresentou um cenário em que foi possível implementar 
microsserviços em conjunto com sistemas monolíticos, indicando que os 
sistemas baseados nos dois modelos arquiteturais podem conviver de forma 
satisfatória. Como você observou, o desenvolvimento de microsserviços para 
agregar novas funcionalidades a um sistema proporciona velocidade de entrega. 
Essa abordagem também se mostra adequada para a realização de manutenção 
ou evolução de funcionalidades, já que não compromete o sistema como um 
todo nem afeta outras funcionalidades que estão operando corretamente no 
ambiente de produção. Esses benefícios são percebidos principalmente quando 
é feita uma analogia com a manutenção de sistemas grandes e complexos 
baseados na arquiteta monolítica. 

Segundo Newman (2020), em sistemas legados de grande porte baseados 
na arquitetura monolítica, é comum haver receio de realizar alterações em 
funcionalidades antigas, muitas vezes acopladas a outras funcionalidades. 
Isso ocorre pois uma mudança pode afetar diversos módulos, podendo gerar 
erros em vários pontos. Além disso, há o custo de testar novamente todas as 
partes afetadas. Esse tipo de problema é minimizado pelo uso da arquitetura 
baseada em microsserviço. Nessa arquitetura, cada funcionalidade tem grande 
nível de independência. Assim, é possível fazer manutenções pontuais, que 
afetam apenas o escopo do microsserviço em questão, e não todo o sistema. 



Prática de microsserviços ) [125 

Outra análise interessante que você pode fazer com base no estudo de 
caso vai ao encontro das recomendações propostas por Newman (2020), que 
considera benéfica a implantação de microsserviços para consumo em sistemas 
monolíticos existentes. Afinal, essa prática gradativamente agrega os benefícios 
dos microsserviços, em um contexto em que refatorar a aplicação como um 
todo implica incorrer em custos impeditivos para o projeto. 

Em suma, por meio do estudo de caso, você conheceu diferentes cenários 
em que pode aplicar o desenvolvimento de software baseado em microsser-
viços — tanto para um novo sistema quanto para um novo módulo de sistema 
legado. Como você viu, o desenvolvimento em microsserviços pode ocorrer 
de forma gradual em um produto, possibilitando que novas funcionalidades 
sejam desenvolvidas com tecnologias contemporâneas (não limitadas àque-
las adotadas na concepção da versão inicial do produto), que agregam seus 
benefícios a essas funcionalidades. Essa possibilidade é muito interessante, 
pois viabiliza que qualquer desenvolvedor de software, independentemente 
dos produtos de sistema com que trabalhe, pense em novos componentes já 
orientados pela arquitetura de microsserviços. 

2 Microsserviços e arquitetura monolítica: 
principais diferenças 

Agora que vocêjá viu exemplos de aplicações de microsserviços, vai conhecer 
melhor as principais diferenças entre sistemas baseados na arquitetura de 
microsserviços e sistemas monolíticos. A arquitetura de microsserviços é 
uma forma moderna de desenvolver sistemas buscando atender a uma série 
de necessidades demandadas pelos requisitos não funcionais de sistemas de 
software contemporâneos, que enfrentam desafios oriundos do advento de 
outras tecnologias, como a cloud computing e a Internet of Things (IoT). 
Por isso, conhecer a arquitetura monolítica e a arquitetura baseada em mi-
crosserviços, bem como suas respectivas aplicabilidades, é muito relevante. 



126 ) (ráca de microsserviços 
Como funciona a arquitetura monolítica 

O desenvolvimento baseado em arquiteturas monolíticas é uma forma natural 
de desenvolver software. Nesse tipo de desenvolvimento, independentemente 
do paradigma de programação utilizado e da organização lógica do código-
-fonte, todas as instruções do software são compiladas — ou, dependendo 
da linguagem, dispostas para interpretação em tempo de execução — em um 
único artefato executável. Isso caracteriza um software na arquitetura mono-
lítica, em que o sistema executa de forma integral em um mesmo computador. 
Um exemplo de arquitetura monolítica é apresentado na Figura 3. Repare que 
todas as funções para tela, regras de negócio e acesso a dados estão sendo 
disponibilizadas como um único processo de software. 

Browser Aplicação monolítica Banco de dados 

Figura 3. Arquitetura monolítica. 

Fonte: Adaptada de Machado (2017). 

Uma das problemáticas atreladas à arquitetura baseada em microsserviços 
é que, ao longo do tempo, o sistema torna-se mais complexo, seja pelo advento 
de novas funcionalidades, pelo aumento da quantidade de requisições ou pelo 
aumento do volume de dados processados. A seguir, veja exemplos de desafios 
encontrados na arquitetura monolítica. 

E Aumento do tamanho do software: com o passar do tempo, o software 
pode crescer. Mesmo que esse crescimento seja organizado, o que por 
vezes já é um desafio, o volume de código-fonte pode ficar muito grande 
(por questões como mudanças de tecnologia e rotatividade de equipe). 
Essa situação demanda muito esforço dos desenvolvedores na hora de 
encontrar o código com a regra de negócio específica. 



Prática de microsserviços ) (127 

E Alto grau de acoplamento: o software pode, ao longo do tempo, ter 
algo grau de acoplamento. Como suas funções são interdependentes, 
qualquer manutenção pode gerar erros ou inconsistências no sistema 
como um todo. 
E Escalabilidade limitada do sistema: quando é necessário escalar o 
sistema, para atender a mais requisições, por exemplo, pode-se utilizar 
técnicas de replicação ou load balance. Porém, a aplicação dessas 
técnicas exige que o sistema como um todo seja replicado, mesmo que 
apenas funcionalidades específicas sejam demandadas. 
E Dificuldade de migrar para novas tecnologias: quando um novo 
módulo é desenvolvido, ele precisa seguir as tecnologias do projeto 
como um todo, mesmo que tal desenvolvimento ocorra muitos anos 
depois da criação do projeto. Por exemplo, um projeto desenvolvido 
originalmente em Java não poderia, mantendo a arquitetura monolí-
tica, ter um módulo novo desenvolvido na linguagem Node,js. Essa 
abordagem limita os desenvolvedores e o produto a tecnologias que 
eventualmente se tornam legadas. 
EB Dificuldade de implantar novas funcionalidades em ambiente de 
produção: qualquer alteração do código-fonte demanda o redeploy 
do sistema como um todo, causando indisponibilidade de uso para 
os usuários. Consequentemente, as alterações não podem ser feitas 
a qualquer tempo, de modo que muitas vezes a correção de um erro, 
mesmo pronta e validada em ambiente de homologação, precisa esperar 
para ser implantada em momento menos inoportuno para o negócio. 

Na arquitetura monolítica, como o sistema está concebido como um único 
bloco de código-fonte ou executável, o desenvolvimento é mais ágil. Assim, 
é possível entregar uma versão inicial do produto, como finalidades de prova 
de conceito (Proof of Concept — PoC) ou de mínimo produto viável (Mini-
mum Viable Product — MVP), com grande agilidade. Porém, suas maiores 
limitações aparecem após esse momento, quando o sistema precisa expandir 
para atender a novos requisitos de negócio ou evoluir tecnologicamente. 



128) ( Prática de micros rvIÇOS 
Como funciona a arquitetura baseada 
em microsserviços 

A arquitetura baseada em microsserviços tem como diretrizes o desenvolvi-
mento do software de modo que suas funcionalidades sejam disponibilizadas 
como pequenos serviços, sendo cada serviço entregue como um único processo. 
A granularidade de um serviço depende do conjunto de regras de negócio 
que ele implementa. Mas o objetivo é que cada serviço seja desenvolvido, 
gerenciado e implantado de forma independente. Desse modo, é possível 
superar algumas limitações intrínsecas à arquitetura monolítica. A seguir, 
veja as principais características da arquitetura bascada em microsserviços. 

E Evolução simplificada do sistema: o código-fonte terá seu escopo 

limitado a uma funcionalidade específica, de modo que ele não cresce 
indefinidamente, mas apenas quando aquela funcionalidade depende 
de ajustes, seja para fins tecnológicos ou de regras de negócio. 

E Baixo acoplamento: o baixo acoplamento é buscado e encarado como 

uma boa prática desde o início da difusão do paradigma de programação 
orientada a objetos. Porém, os sistemas limitados à arquitetura monolí-
tica fazem essa separação de forma lógica, o que dá margens para que 
eventuais problemas de alto acoplamento não sejam identificados com 
tanta facilidade. Com a arquitetura monolítica, a separação das funções 
ocorre de forma física, com processos separados, o que contribui muito 
para o baixo acoplamento. 

= Escalabilidade do sistema: como o sistema é composto pela implan-
tação de serviços individuais, seu processo de escalabilidade é muito 
mais simples. Afinal, é possível ampliar o fornecimento de uma função 
específica, em vez de o sistema como um todo. 

m Precisão nos custos de disponibilização do sistema: sistemas que 

têm custos associados ao seu uso, o que é bastante comum em soluções 
entregues por meio de provedores de cloud computing, ficam muito mais 
precisos, ocasionando a redução de custos. Isso ocorre porque, quando 
é necessário ampliar o consumo de determinada função, apenas ela é 
ampliada, com a criação de novos contêineres, por exemplo. Logo, não é 
necessário replicar o software inteiro, o que geraria custos mais elevados. 



Prática de microsserviços ) (129 

= Flexibilidade de tecnologia: não é necessário vincular o produto como 
um todo a uma tecnologia específica, pois há baixo acoplamento entre 
os serviços. Dessa forma, cada serviço pode ser desenvolvido com a 
tecnologia que for mais adequada à sua finalidade, o que também pode 
ser um grande fator de redução de custos no desenvolvimento da solução. 

m Facilidade na implantação de mudanças: quando um serviço apresenta 
um problema de lógica de programação e necessita de uma correção, 
ou quando há uma solicitação de mudança, a alteração pode ser feita 
no serviço específico, e este pode ser implantado separadamente. 
Tal recurso é muito útil: enquanto um serviço é implantado, todos os 
demais continuam disponíveis para os usuários. Isso reduz muito os 
impactos de publicações de versões, assim como possibilita que estas 
sejam implementadas de forma mais constante. Essa, inclusive, é uma 
das diretrizes da integração contínua. 

mB Estrutura do projeto de código-fonte: quando se desenvolve um 
projeto de sofiware, este passa a ter uma estrutura particular. Claro que 
isso varia de acordo com as linguagens e tecnologias utilizadas. Mas, 
de forma genérica, sabe-se que cada microsserviço precisa ser compilado 
e entregue de forma independente, o que pode exigir, por exemplo, que 
cada serviço esteja em um projeto de código-fonte separado. Essa é 
uma grande diferença na organização da estrutura interna do sistema 
em comparação com a arquitetura monolítica. 

Uma palavra que define bem a arquitetura baseada em microsserviços é 
especialização. Afinal, cada serviço tem total autonomia em relação ao seu 
projeto de software, podendo ser desenvolvido com linguagem de programação 
particular, com a base de dados mais adequada ao seu propósito e com grande 
isolamento em relação aos demais serviços. Qualquer impacto relacionado 
ao desempenho ou à disponibilidade de um microsserviço não se estende aos 
outros. Isso ocorre porque, dadas as tecnologias de virtualização, entende-se 
que cada microsserviço fica limitado aos recursos de hardware e infraestrutura 
dedicados a ele. 



130) ( Prática de microsserviços 
Fique atento 

O maior desafio na arquitetura baseada em microsserviços é a integração dos serviços. 
Tal integração, assim como o processo de implantação de cada serviço individual-
mente, certamente torna essa arquitetura mais complexa de ser gerida. Por outro lado, 
é possível que cada serviço seja desenvolvido por equipes diferentes, inclusive de 
diferentes empresas, o que pode aumentar a produtividade e a velocidade de entrega 
dos projetos. Para gerir essas complexidades, o perfil DevOps vem se especializando 
e dotando profissionais de ferramentas de tecnologias de apoio. O objetivo disso é 
tornar a gestão da arquitetura de microsserviços simples e prática. 

Diferenças entre a arquitetura de microsserviços 
ea monolítica 

Agora que vocêjá conhece as características, as vantagens e as desvantagens 
das arquiteturas monolítica e bascada em microsserviços, vai compará-las a 
partir do Quadro 1, a seguir. 

a pp 
Quadro 1. Arquitetura monolítica e arquitetura baseada em microsserviçositetura baseada 

Caracte-Arquitetura monolítica! Arquitetura baseada em 
rística microsserviços 

Gover-A coordenação de projetos No desenvolvimento em 

nança baseados na arquitetura microsserviços, a disponibilidade 
monolítica é muito mais sim-| da solução é muito privilegiada, 
ples. Entretanto, a principal poisa falha em um serviço 
desvantagem é que, como pontual não impacta os demais 

o software está inteiramente pontos de entrada do software. 
ligado a um único processo, 

qualquer falha pode indispo-
nibilizá-lo como um todo. 

Prazo do Na entrega da primeira Dada a complexidade das inte-

projeto versão do software, essa grações dos microsserviços, o seu 
arquitetura consegue ser desenvolvimento tem um custo. 
muito mais veloz, pois não adicional, que é a construção des-
necessita de integrações sas integrações. Esse custo pode 
internas. ser visto como um custo adicional 

impeditivo no início do projeto. 

(Continua) 



Prática de microsservços ) (131 

(Continuação) 

Quadro 1. Arquitetura monolítica e arquitetura baseada em microsserviçositetura baseada 

Atualização | A atualização do software Dado o baixo acompanhamento, 
é mais penosa no cada serviço pode ser publicado 
desenvolvimento monolítico, | individualmente. Isso é positivo 
pois a publicação de uma pois não impacta o sistema 
nova versão demanda a como um todo. 
indisponibilização do sistema | E, como o serviço tende a ser 
como um todo, deixando os | algo pequeno, seu tempo de 
usuários sem aceso a ele por | implantação também tende a 
diversos minutos. Tal situação | ser menor. Quanto à tecnologia, 
pode ser impeditiva, o que a atualização torna-se muito 
depende do tipo do produto. | mais simples, 
Outra desvantagem quanto já que cada microsserviço 
à atualização é relacionada pode ser desenvolvido com 
à tecnologia, que de modo linguagens e ferramentas 
geral fica limitada àquela distintas. 
utilizada inicialmente, no 
momento da concepção do 
projeto. 
Complexi-| Na arquitetura monolítica, A arquitetura de 
dade a complexidade do sistema microsserviços tende a manter 
tende a aumentar à medida a mesma complexidade 
que o sistema evolui. Isso independentemente do 
torna as manutenções mais tamanho do sistema, pois 
custosas ao longo do tempo. | as modificações e as novas 
Além disso, normalmente são | funcionalidades são feitas em 
necessários desenvolvedores | novos microsserviços. 
experientes para realizar a 
manutenção do produto. 
Escalabili-| A escalabilidade é A escalabilidade com 
dade prejudicada, pois, para criar microsserviços pode ser feita 
um novo nó em uma solução em serviços particulares. Então, 
de alto desempenho ou alta a alta disponibilidade é muito 
disponibilidade, é necessário mais barata, pois é focada nos 
replicar o software como um serviços que de fato são mais 
todo, não apenas os serviços demandados e precisam ter 
específicos. uma disponibilidade ou um 
desempenho mais crítico. 
(Continua) 


132) (Prática de microsserviços 
(Continuação) 
Quadro 1. Arquitetura monolítica e arquitetura baseada em microsserviçositetura baseada 

No primeiro momento 
do desenvolvimento do 
software, a arquitetura 
monolítica tende a ser mais 
barata, porém, à medida 
que o software precisa ser 
evoluído, seus custos tendem 
a ser maiores. 

Integração A integração, de modo geral, 
ocorre apenas quando é 
necessário comunicar-se 
com sistemas externos. 
Comunicações desenvolvidas 
como parte do mesmo 
produto de maneira 
geral não necessitam de 
integrações. 

Fonte: Adaptado de Machado (2017). 

No primeiro momento, os 
custos de desenvolvimento 
são maiores, pois é necessário 
implementar as integrações 
e realizar a implantação 
de diversos microsserviços 
de forma individualizada. 
Entretanto, com o passar do 
tempo, quando o software 
precisa ser evoluído, a inclusão 
de novo serviço tem o custo 
normal, independentemente 
do tamanho do software já 
desenvolvido anteriormente. 

Sistemas baseados em 
microsserviços têm utilizado 
a integração de serviços de 
forma bastante natural, seja para 
serviços desenvolvidos dentro 
ou fora da organização. Essa 
característica possibilita muitas 
facilidades quando é necessário 
fazer integrações com outros 
produtos ou fontes de dados. 

Escolher entre a arquitetura de microsserviços e a arquitetura monolítica 
não é uma tarefa simples. Como você viu, cada arquitetura oferece suas van-
tagens e desvantagens. Em suma, não há um modelo que seja melhor em todos 
os aspectos. Alterar um produto legado de forma integral também pode ser 
custoso. Nesse sentido, uma mudança gradual, realizada à medida que novos 
serviços precisem ser introduzidos ou que serviços existentes demandem 
melhorias, pode ser uma boa alternativa. 



Práticade microsserviços )[ 133 

3 Benefícios da utilização de microsserviços 

Na seção anterior, você conheceu diversos benefícios da arquitetura baseada 
em microsserviços. Nesta seção, você vai conhecer outros benefícios da uti-
lização dos microsserviços — no que tange não apenas às diretrizes gerais 
desse modelo arquitetural, mas também às suas formas de implementação 
baseadas nas tecnologias disponíveis atualmente. 

Uma maneira cada vez mais comum de publicar serviços é a cloud com-
puting, na qual um software pode ser publicado para os usuários sem que 
a empresa fornecedora tenha de adquirir qualquer recurso de hardware ou 
telecomunicação. Utilizando essa abordagem, ao prover um serviço em nuvem, 
a empresa realiza pagamento sob demanda, de acordo com a quantidade de 
recursos que o serviço está consumindo, o que é diretamente proporcional a 
quanto os usuários estão demandando de requisições e processamento. Isso 
é particularmente útil e tem uma boa aderência ao modelo arquitetural de 
microsserviços, já que cada serviço pode ser publicado separadamente e ter 
seus custos mensurados de forma individual (ZRP, 2017). Outro benefício 
de utilizar a cloud computing é que todo o hardware é disponibilizado pelo 
provedor do serviço. Assim, um microsserviço pode ter mais ou menos ins-
tâncias, com mais ou menos recursos de hardware, de acordo com a demanda. 
Tal demanda ainda pode ser “elástica”, isto é, pode aumentar ou diminuir de 
acordo com o período, reduzindo investimentos altos em um serviço que nem 
sempre é utilizado. 

Nesse sentido, ao adotar esse modelo arquitetural, as empresas buscam 
a possibilidade de escalar os negócios e a produtividade a fim de manter 
produtos de software. Essa arquitetura proporciona a construção de uma 
solução organizada e o trabalho com equipes subdivididas, o que pode gerar 
grande produtividade. 

Outro grande benefício da arquitetura baseada em microsserviços é sua 
aderência aos métodos ágeis de desenvolvimento de software. Nesse contexto, 
pequenos times trabalham em sprints para entregar alguma funcionalidade 
que agrega valor para os usuários. Essa funcionalidade, definida pelos mé-
todos ágeis, no contexto de microsserviços, pode representar um serviço em 
particular, de forma que sua implantação é facilitada. Isso reduz muito o nível 
de interdependência entre as diferentes equipes que trabalham para evoluir 
funcionalidades de um mesmo produto de software. 



134) Prática de microsserviços 
Desafios 

Mesmo proporcionando diversos benefícios, a arquitetura baseada em micros-
serviços também implica grandes desafios para os fornecedores de soluções 
baseadas nessa abordagem. Um primeiro desafio é a necessidade de migração 
da arquitetura monolítica para a arquitetura em microsserviço. Como você 
viu, essa migração pode ser realizada de forma gradual, contudo o desenvol-
vimento do primeiro microsserviço pode ser bastante impactante, pois implica 
uma mudança cultural significativa em relação ao modo de desenvolver e 
distribuir software. 

O controle de versões no desenvolvimento baseado em microsserviços 
também é mais complexo, pois é necessário controlar o versionamento de cada 
microsserviço em particular, e não apenas da solução monolítica. Por isso, 
a adoção de ferramentas de apoio ao controle de versões se torna fundamental. 

Por último, um sistema baseado inteiramente na arquitetura de micros-
serviços tem uma quantidade de serviços considerável, e muitas vezes esses 
serviços são desenvolvidos por equipes distintas. Portanto, a equipe respon-
sável pela gestão do produto deve organizar de modo muito claro o sistema 
e o conjunto de serviços existentes. Assim, garante-se que o produto tenha 
uma continuidade sustentável ao longo do tempo; ademais, evita-se que times 
diferentes desenvolvam recursos duplicados. 

Referências 

FOWLER, M. MicroservicePremium. Chicago: ThoughtWorks, 2015. Disponível em: https:// 
martinfowler.com/bliki/MicroservicePremium.html. Acesso em: 2 out. 2020. 

MACHADO, M. Micro Serviços: qual a diferença para a arquitetura monolítica? São 

Paulo: Opus Software; 2017. Disponível em: https:/Awww.opus-software.com.br/micro-

-servicos-arquietura-monolitica/ Acessado em: 2 out. 2020. 

MOREIRA, P. F. M.; BEDER, D. M. Desenvolvimento de aplicações e micro serviços: um 
estudo de caso. Tecnologias, Infraestrutura e Software, São Carlos, v. 4, n.3, p. 209-215, 
set/dez. 2015. 

NEWMAN, S. Migrando sistemas monolíticos para microsserviços: padrões evolutivos para 
transformar seu sistema monolítico. São Paulo: Novatec, 2020. 

ZRP. À arquitetura de microsserviços em nuvem. São Paulo: ZRP, 2017. Disponível em: 
http://labs.zrp.com.br/2017/12/05/microsservicos-em-nuvem/. Acesso em: 2 out. 2020 


Prática de miciossenaços) ( 135 

Leitura recomendada 
KIM, G. etal. Manualde DevOps: 
organizações tecnológicas. Rio 
como obter agilidade, confiabilidade 
de Janeiro: Alta Books, 2018. 
e segurança em 
d 
Fique atento 
Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 
d 


Esta página foi deixada em branco intencionalmente. 



Ferramenta de 
orquestração de 
contêineres 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Explicar a orquestração de contêineres. 
E Reconhecer as principais características do Kubernetes. 
E Demonstrar o gerenciamento de objetos do Kubernetes. 

Introdução 

No desenvolvimento moderno, as aplicações não são mais monolíticas, 
mas sim consistem em diversos componentes organizados em contêi-
neres, que precisam trabalhar de forma conjunta para que as aplicações 
possam funcionar adequadamente. A orquestração de contêineres refere-
-se à automação da implantação, do gerenciamento, da escala e da rede 
de contêineres. No entanto, quando se utiliza essa abordagem como base 
de uma aplicação com vários serviços, o gerenciamento do ciclo de vida 
desses contêineres torna-se um problema. Para tanto, é recomendado o 
uso de orquestradores de contêineres, a fim de coordenar a criação e a 
remoção dessas unidades, entre outras funcionalidades. 

Neste capítulo, você conhecerá as principais características da or-
questração de contêineres, bem como as suas vantagens. Além disso, 
conhecerá a ferramenta mais utilizada pelas empresas, o Kubernetes. 
Por fim, conhecerá as principais características dessa ferramenta, como 
vantagens de uso, painel de controle, tipos de objetos e gerenciamento. 



138) ( Ferramenta de orquestração de contémeres 
1 Orquestração de contêineres 

Para implantar um software, precisa-se não só dele, mas também de suas 
dependências, o que envolve interpretadores, pacotes, extensões, compiladores, 
entre outros. Além disso, faz-se necessário fazer a sua configuração, obter 
detalhes específicos, licenças e senhas de banco de dados, a fim de tornar o 
software um serviço utilizável (ARUNDEL; DOMINGUS, 2019). 

Antes, a resolução desse problema podia ser feita com o uso de Puppet, 
em que se fazia a automação do processo de entrega de software, permitindo 

o provisionamento de máquinas físicas ou virtuais, a orquestração, a emissão 
de relatórios, bem como a distribuição de códigos em fase inicial de desenvol-
vimento, testes, lançamentos ou atualizações. Além disso, algumas linguagens 
também passaram a fornecer os seus próprios mecanismos de empacotamento, 
como arquivos JAR, do Java, egg, do Python, e gem, do Ruby, porém eles são 
muito específicos (ARUNDEL; DOMINGUS, 2019). 
É necessário muito tempo e esforço para provisionar servidores, fazer a 
sua conexão à rede, sua implantação, configuração, atualização com patches 
de segurança, realizar o seu monitoramento e gerenciamento, entre outras 
atividades (ARUNDEL; DOMINGUS, 2019). Pensando nisso, foi introduzido 

o conceito de contêineres. Assim como é mais fácil carregar um contêiner 
cheio de mercadorias para um caminhão ou navio, em vez de mover cada 
mercadoria por vez, o conceito de contêiner em DevOps segue a mesma linha. 
Um contêiner é um ambiente de software onde é possível instalar uma 
aplicação ou microsserviço e contém todas as dependências da biblioteca, 
os binários e uma configuração básica, necessária para a sua execução 
(CASALICCHIO, 2016), permitindo uma capacidade de carga muito maior, 
a diminuição de custos e a facilidade de manuseio (ARUNDEL; DOMINGUS, 
2019). Ao utilizar contêineres, é possível prover o isolamento da aplicação, 
dependências e recursos de maneira similar a uma máquina virtual (VM, 
virtual machine), mas de maneira mais leve, em virtude de os contêineres 
compartilharem o kernel com o sistema hospedeiro (RUBENS, 2017). Dessa 
forma, o uso de contêineres padroniza o ambiente de execução da aplicação e 
facilita a implantação (deploy) de forma independente (KANG; LE; TÃO, 2016). 

A indústria de nuvem passou a adotar a tecnologia de contêineres tanto para 
uso interno (BURNS et al., 2016) quanto para oferecer serviços baseados em 
contêineres e plataformas de desenvolvimento de contêineres (DUA; RAJA; 
KAKADIA, 2014). 



Ferramenta de orquestração de contêineres | (139 

A Open Container Initiative (OCT) é um projeto da Linux Foundation para 
promover padrões e especificações para contêineres. Alguns de seus princípios 
de orientação são listados a seguir (KHAN, 2017): 

E um contêiner não está vinculado a construções de nível superior, como 
um cliente específico ou uma pilha de orquestração; 
E um contêiner não está fortemente associado a nenhum fornecedor ou 
projeto comercial específico; 
E um contêiner é portátil em uma ampla variedade de sistemas opera-
cionais, hardware, arquiteturas de CPU, nuvens públicas, entre outros. 

O uso de contêineres como base de uma aplicação com vários serviços 
representa um desafio para o gerenciamento do ciclo de vida desses contêineres. 
A orquestração de contêineres refere-se ao processo de organizar o trabalho 
de componentes individuais em camadas de aplicações. Para isso, recomenda-
-se o uso de uma ferramenta chamada de orquestradores de contêineres, 
que tem como objetivo coordenar a criação e a remoção dessas unidades de 
processamento de contêineres, entre outras funcionalidades (LINTHICUM, 
2015). As ferramentas de orquestração ajudam a automatizar a manutenção das 
aplicações, substituem automaticamente os contêineres com falha e gerenciam 
a liberação de atualizações e reconfigurações desses contêineres durante o 
seu ciclo de vida. Além disso, elas facilitam o gerenciamento de fluxos de 
trabalho e tarefas complexas da tecnologia da informação (TI). 

Em suma, a orquestração auxilia a combinar várias tarefas automatizadas 
e suas configurações em grupos de sistemas ou máquinas. As equipes de TI 
precisam gerenciar vários servidores e aplicações, o que não é recomendável 
fazer de forma manual, uma vez que, quanto maior a complexidade do sistema, 
mais difícil será a gestão de todos os componentes variáveis. 

Alguns dos principais recursos de uma ferramenta de orquestração de 
contêineres são listados a seguir (KHAN, 2017): 

gerenciamento e agendamento do estado do cluster; 
alta disponibilidade e tolerância a falhas; 
garantia de segurança; 
monitoramento e governança; 
simplificação da rede; 
ativação de descoberta de serviços; 
possibilidade de implantação contínua. 



140 ) ( Ferramenta de orquestração de contêineres 
Atualmente, é possível encontrar diversas ferramentas de orquestração, 
sendo as principais listadas a seguir. 

m Kubernetes: desenvolvida pela Google, essa ferramenta passou a ser 
gerenciada pelo Cloud Native Computing Foundation. Esse serviço 
é, de longe, o mais utilizado pelas empresas, em virtude de ter sido 
um dos pioneiros em orquestração e por ser open source. É ideal para 
empresas de grande porte, já que seus recursos suportam descobrir, 
escalar e balancear aplicações (FREITAS, 2019). 
E Amazon Elastic Container Service (Amazon ECS): é um serviço para 
gerenciar contêineres de forma rápida e com escalabilidade, facilitando 
a execução, a interrupção e a gestão de contêineres. Esse serviço per-
mite fazer tudo isso através de chamadas simples de API (application 
programming interface; ou interface de programação de aplicações, 
em português) (DAVID, 2018). 
E Docker Swarm: é uma ferramenta open source da Docker e uma das 
mais utilizadas. Essa ferramenta permite que a execução de contêine-
res ocorra de forma distribuída em um cluster, realizando o controle 
da quantidade, o registro, a implantação e a atualização de serviços 
(GIROLINETO, 2017). 
m Rancher OS: é um serviço de orquestração open source da Rancher 
Labs que pode ser encontrado no Git Hub. A ferramenta oferece muitos 
recursos para a criação e a gestão de uma infraestrutura baseada em 
contêineres de forma fácil e prática (BARRADAS, 2018). 
m OpensShift: criada pela RedHat, é uma ferramenta que toma como 
base o empacotamento do Docker, com a gestão de cluster com base 
no Kubernetes. Essa ferramenta se destaca pelo fato de ser possível a 
sua integração com outras ferramentas do RedHat (FREITAS, 2019). 
E Nomad: serviço da HashiCorp.com que permite fazer a implantação e o 
gerenciamento de aplicações baseadas em contêineres. Essa ferramenta 
possui um grande conjunto de APIs, que ajudam na automatização da 
implantação, no dimensionamento de aplicações e atualizações, auxi-
liam os desenvolvedores a gerenciá-las diretamente, além de fazerem 
a gestão automática de falhas (LICCARDI, 2020). 



Ferramenta de orquestração de contêineres ) (im 

Depois de implementadas, as ferramentas de orquestração podem tra-
zer diversos benefícios quanto a produtividade, portabilidade e segurança. 
A seguir, são listadas algumas vantagens dessas ferramentas (MANGAT, 2020): 

E aumentam a produtividade, uma vez que simplificam a instalação, 
reduzindo o número de erros de dependência; 
E como esse tipo de ferramenta é mais fácil de ser utilizado, é possível 
fazer implantações mais rápidas e de forma mais simples; 
E há uma sobrecarga menor,já que os contêineres ocupam menos recursos, 
se comparados com as VMs; 
E há melhora na segurança, uma vez que os usuários dessas ferramentas 
podem compartilhar recursos específicos de forma segura; 
E permitem que os usuários escalem aplicações com um único comando, 
aumentando a portabilidade; 
E ao aderirem aos princípios da imutabilidade da infraestrutura, incen-
tivam o desenvolvimento de sistemas distribuídos. 

2 Kubernetes 

O Kubernetes é um sistema de código aberto que faz o gerenciamento de 
aplicações em contêineres em um ambiente em cluster. Desde que foi criado, 
em 2014, o Kubernetes cresceu e se tornou um dos maiores e mais populares 
projetos de código aberto do mundo (HIGHTOWER; BURNS; BEDA, 2017). 
Além disso, ele se tornou a API padrão para a criação de aplicações nativas na 
nuvem, sendo utilizado em praticamente todas as nuvens públicas. Conhecido 
por sua portabilidade, o ponto de partida inicial do Kubernetes é o próprio 
cluster, que possibilita mover cargas de trabalho sem ter a preocupação de 
redesenhar a sua aplicação ou redefinir a sua infraestrutura. 

A seguir, são listados os principais benefícios do Kubernetes: 

E orquestra contêineres em múltiplos hosts, em nuvens públicas, privadas 
ou híbridas; 
mB faza otimização do uso do hardware, maximizando a disponibilidade 
de recursos para a execução das aplicações; 
E fornece maior agilidade para escalar aplicações em contêineres e re-
cursos relacionados; 



142) (Ferramenta de orquestração de contêineres 
apresenta maior vantagem na distribuição de aplicações em contêineres; 
faz o gerenciamento e a automatização da maior parte das implantações 
e as atualizações das aplicações; 
garante a integridade e a autorrecuperação das aplicações em contêineres; 
define os fluxos de trabalho de integração e entrega com base nas 
preferências da empresa; 

m fornece integrações e mecanismos para fazer a coleta de dados e 
métricas; 
E não possui a exigência de idioma ou linguagem específica para con-
figurar os sistemas. 

Os principais componentes de arquitetura do Kubernetes (Figura 1) são 
descritos a seguir (ELDRIDGE, 2018). 

Figura 1. Diagrama de um cluster do Kubernetes. 
Fonte: Isabekyan (2016, documento on-line). 


Ferramenta de orquestração de contêineres) ( 143 

API Server: é um componente front-end do plano de controle do 
Kubernetes que expõe a API dele. 
Cluster: conjunto de nós associados a, pelo menos, um nó mestre (master 
node) e vários nós de trabalho (work node), que podem ser máquinas 
virtuais ou físicas. 
Etcd: local onde são armazenados os valores-chave consistentes e de 
alta disponibilidade utilizados como repositório de backup para todos 
os dados do cluster. 
Master: gerencia o agendamento e a implantação de instâncias de 
aplicações entre os nós. O conjunto completo de serviços que o nó 
mestre executa é conhecido como plano de controle. A comunicação 
do mestre com os nós ocorre através do servidor da API Kubernetes. 
Scheduller: atribui nós aos pods, dependendo das restrições de recursos 
e políticas que foram definidas para tal. 
Controller Manager: componente do plano de controle que executa os 
processos do controlador. Cada controlador é um processo separado, 
e eles são compilados em um único binário e processo com o intuito de 
reduzir a complexidade. Tem-se os seguintes controladores: 
= Controlador de nó: recebe e responde quando os nós são desativados. 
= Controlador de replicação: mantém o número correto de pods para 

cada objeto do controlador de replicação no sistema. 
= Controlador de pontos finais: preenche o objeto endpoint. 
= Controlador de conta de serviço e token: faz a criação de contas 

padrão e tokens de acesso à API para novos namespaces. 
Pods: um grupo de contêineres que compartilham recursos e o mesmo 
endereço de IP (Internet Protocol; ou Protocolo de Internet, em por-
tuguês). O estado desejado dos contêineres é descrito em um pod por 
meio de um objeto YAML (YAML Ain't Markup Language; ou YAML 
Não é Linguagem de Marcação, em português) ou JSON (JavaScript 
Object Notation; ou Notação de Objetos JavaScript, em português). 
Kubelet: processo do agente executado por cada nó do Kubernetes 
(Kubernetes node). É responsável pelo gerenciamento do estado do 
nó: inicia, para e mantém os contêineres de aplicações com base nas 
instruções do plano de controle. Esse processo recebe todas as suas 
informações através do servidor da API Kubernetes. 
PodSpec: esses objetos são passados para o kubelet através do servidor 
API. 



144) ( Ferramenta de orquestração de contêineres 
Dashboard do Kubernetes 

O painel da ferramenta Kubernetes é uma interface de usuário baseada na web 
e pode ser utilizado para aplicações em contêineres, solucionar problemas e 
fazer o gerenciamento de recursos. Por exemplo, por meio do painel de controle 
(Figura 2), é possível fazer uma atualização sem interrupções ou reiniciar um 
pod. Além disso, é possível obter informações sobre qualquer erro que possa 
ter ocorrido no processo. 

O tubernetes a temo tour 1 6 

4 a o RR no na 

cupom 

e er 

Figura 2. Painel de controle do Kubernetes 
U Fonte: Kubernetes (2020b, documento on-line) 

Fique atento 

Ainterface do usuário do painel não é implantada por padrão. Para implantá-la, faz-se 
necessário executar o seguinte comando no kubect, que é a ferramenta de linha de 
comando do Kubernetes: 

kubectl apply -f https://raw.githubusercontent.com/kuber-

netes/dashboard/v2.0.0/aio/deploy/recommended.yaml 
A A 


Ferramenta de orquestração de contêineres ) ( 145 

O painel de controle do Kubernetes possui as seguintes seções: 

E Navigation: o painel de navegação apresenta os objetos do Kubernetes 
definidos no cluster organizados por categorias. 
m Admin Overview: o painel de visão geral do administrador mostra 
nós, namespaces, volumes e detalhes específicos de cada um deles. 
m Workloads: na seção de carga de trabalho, tem-se todas as aplicações 
em execução apresentadas por categoria de tipo de carga. Cada carga de 
trabalho pode ser vista em detalhes, como a visualização de informações 
de status, especificações e relacionamentos entre objetos. 
E Services: na seção de serviços, tem-se os recursos que permitem expor 
serviços ao mundo exterior e fazer a descoberta deles em um cluster. 
E Storage: na seção de armazenamento, são apresentados os recursos 
utilizados pelas aplicações para armazenar dados. 
m Config Maps and Secrets: nessa seção de configuração, é possível 
acessar os recursos para configurar aplicações que estão sendo execu-
tadas, bem como editar e gerenciar objetos. 
Em Logs viewer: refere-se à seção de visualização de logs relacionados 
com a lista de pods. Nela, é possível pesquisar /ogs de contêineres que 
pertencem a um único pod. 

3 Gerenciamento de objetos Kubernetes 

Os objetos Kubernetes são entidades persistentes utilizadas para representar 

o estado do cluster. Em suma, eles podem descrever (KUBERNETES.IO): 
m as aplicações em contêineres que estão sendo executadas e seus nós; 

Bm os recursos disponíveis para cada uma delas; 

Os objetos Kubernetes são representados como arquivos JSON ou YAML 
na API Kubernetes e são utilizados para criar, atualizar e excluir objetos no 
servidor Kubernetes. Esses arquivos são retornados pelo servidor em resposta 
a uma consulta ou solicitação de API. Além disso, os objetos Kubernetes são 
utilizados para representar o estado do cluster. Alguns dos objetos Kubernetes 
são apresentados a seguir (ESSIEN, 2020; REIS, 2017): 



146 ) ( Ferramenta de orquestração de contêineres 
E Pod: como visto, o pod consiste em um grupo de um ou mais contêineres 
que compartilham os mesmos recursos. Eles podem ter os seguintes 
estados: pendente, em execução, sucedido, falha e desconhecido. 
m ReplicationController: é utilizado para criar várias cópias 
de um mesmo pod, de forma a garantir que o número de pods defi-
nido esteja no estado de execução a qualquer momento. Além disso, 
o ReplicationController cria outro pod para substituir, caso um pod 
pare ou seja destruído. 
E Deployment: também é utilizado para gerenciar pods, bem como 
para dimensionar a aplicação ao aumentar a quantidade de pods em 
execução ou atualizando-a. Possui três estados no seu ciclo de vida: 
progresso, concluído e falha. 
EB StatefulSet: controla a ordem de implantação de um conjunto 
de pods e atribui identificadores a eles. Com isso, quando um pod é 
relançado em outro nó, ele receberá o mesmo identificador. 
mB DaemonSet: é utilizado para gerenciar pods. Por meio dele, é possível 
garantir que um pod específico seja executado em todos os nós que 
pertencem a um cluster quando desejado. 
E Service: define um conjunto de pods semelhantes (réplicas) que 
fornecem um serviço. Os serviços são utilizados para distribuir pedidos 
de comunicação entre os pods. 
Bm ConfigMaps: é utilizado para separar os dados de configuração dos 
contêineres na aplicação. Além disso, eles são capazes de modificar 
os dados de forma dinâmica em tempo de execução. Esse objeto deve 
ser utilizado para guardar informações confidenciais. 
Em Volume: diretório do host montado no pod que possui o mesmo ciclo 
de vida dele. Se o pod for eliminado, o volume também será. 

Saiba mais 

Para saber mais sobre cada tipo de objeto mencionado, confira a seção de Leituras 
recomendadas. 


Ferramenta de orquestração de contêineres | ( 147 

Quase todos os objetos do Kubernetes possuem dois campos relacionados 
à configuração deles: spec e status. O primeiro é referente à especificação, 
eo outro, ao estado do objeto. Para os objetos com uma especificação, faz-se 
necessário informar o seu estado desejado. Já o campo status é fornecido 
e atualizado pelo próprio Kubernetes, com base no estado atual do objeto. 
Além desses campos, ao criar um objeto, faz-se necessário fornecer outras 
informações básicas, como o name (que deve ser único por tipo de objeto). 

Quando a API Kubernetes é utilizada para criar o objeto, esse pedido deve 
conter as informações como JSON no corpo da solicitação. Em geral, essas 
informações são fornecidas na ferramenta de linha de comando através de um 
arquivo . yaml. A partir daí, a ferramenta faz a conversão em JSON quando 
faz o pedido à API. Vale ressaltar que alguns campos são obrigatórios no 
arquivo . yaml na hora de criar o objeto, como: 

apiVersion: para informar a versão da API que está sendo utilizada. 
kina: tipo de objeto que será criado. 
metadados: dados que ajudam a identificar o objeto de forma exclusiva. 
spec: para informar o estado desejado para o objeto. 

Além disso, cada tipo de objeto tem uma especificação diferente a ser 
utilizada no momento da sua criação, e cada especificação tem campos especí-
ficos, que podem ser consultados na documentação oficial da API Kubernetes 
(kubernetes.io). 

Confira, a seguir, um exemplo de arquivo . yaml que mostra os campos obrigatórios 
ea especificação de um objeto: 
apiVersion:apps/vl fpara versões anteriores à 1.9.0, use 
apps/vibeta? 
kind: Deployment 
metadata: 

name: sagah-deployment 
spec: 
selector: 
matchLabels: 
app: sagah 


148 ) (Ferramenta de orquestração de contêineres 
replicas:2 finforma à implantação para executar 2 pods 
correspondentes ao template 
template: 
metadata: 
labels: 
app: sagah 

spec: 
containers: 
-name: sagah 

image: sagah:1.14.2 
ports: 
-containerPort: 80 

Fonte: Adaptado de GitHub (c2020). 
N A 

Por meio do kubectl, é possível criar e gerenciar objetos de diferentes for-
mas. Contudo, lembre-se de que é preciso escolher uma forma e utilizar somente 
ela, visto que combinar diferentes técnicas para isso pode não ser uma boa 
opção. No Kubernetes, há duas formas de aplicar mudanças na configuração 
de uma aplicação: imperativa e declarativa. O Quadro 1, a seguir, apresenta 
diferentes técnicas de gerenciamento de objetos Kubernetes. 

á N 
Quadro 1. Descrição de técnicas de gerenciamento de objetos Kubernetes 

Técnica de Ambiente | Número de Curva de 
gerencia-Operaem recomen-escritores apren-
mento dado suportados dizado 

Comandos Objetos Desenvol-Mais de 1 Muito baixa 
imperativos | ativos vimento 

Configuração | Arquivos Produção Apenas 1 Moderada 
imperativa individuais 
do objeto 

Configuração | Diretórios Produção Mais de 1 Muito alta 
declarativa de arquivos 
do objeto 

Fonte: Adaptado de Kubernetes (20203), 
Ni J 


Ferramenta de orquestração de contêineres ) ( 149 

A configuração imperativa refere-se à abordagem tradicional do 
Kubernetes, em que o kubectl especifica a operação que se deseja aplicar, como 
create, replace, scale, entre outras, flags opcionais e, no mínimo, um nome 
de arquivo. Observe, a seguir, alguns exemplos de comandos imperativos: 

kubectl create -f sagah.yaml fcria os objetos definidos em um 
arquivo de config 
kubectl delete -£ sagah.yaml -f redis.yaml fexclui os objetos 
de dois arquivos 
kubectl replace -f sagah.yaml fatualiza os objetos substituindo 
a configuração ativa 

Já a configuração declarativa do objeto não diz como uma mudança 
será realizada, e sim apenas qual mudança deve ser realizada. Assim, o sis-
tema do Kubernetes decidirá qual a melhor forma de aplicá-la e igualará o 
estado atual ao estado desejado da aplicação. Portanto, o usuário não define 
as operações que serão executadas. Observe, a seguir, alguns exemplos de 
comandos declarativos: 

kubectl apply -f sagah-deployment.yaml fazer a implantação 
do exemplo no cluster 
kubect1 diff -£ configs/ fprocessa todas os arquivos de config 
no diretório e diferencia quais mudanças serão realizadas 
kubectl aplly -f configs/ faplica as mudanças 

A Google lançou um curso sobre microsserviços escalonáveis com o Kubernetes, 
disponível no site Udacity, para ensinar sobre o gerenciamento de contêineres de 
aplicações. O curso é gratuito e possibilita aprender mais sobre como criar arquivos 
de configurações no Docker, configurar e iniciar um cluster, gerir uma implantação e 
dimensionar e atualizar aplicações. Além disso, ele apresenta as melhores práticas para 
utilizar contêineres no desenvolvimento e na arquitetura de novos microsserviços. 


150 ) ( Ferramenta de orquestração de contêmeres 
Referências 

ARUNDEL, .; DOMINGUS, J. Cloud native DevOps with kubernetes: building, deploying, 
and scaling modern applications in the cloud. Boston: O'Reilly Media, 2019. 

BARRADAS, T. O que é rancher? É para criar gado? [S. |]: Medium, 2018. Disponível em: 

https://medium.com/thiagobarradas/o-que-%C3%A9-o-rancher-%WC3%A9-pra-criar-

-gado-cd2009a5f271. Acesso em: 27 jun. 2020. 

BURNS, B. etal. Borg, omega, and kubernetes. Queue, v. 14, n. 1, p. 70-93, 2016. 

CASALICCHIO, E. Autonomic orchestration of containers: problem definition and rese-
arch challenges. in: EAI INTERNATIONAL CONFERENCE ON PERFORMANCE EVALUATION 
METHODOLOGIES AND TOOLS, 10, 2016, Taormina. Proceedings [.]. Taormina: EAI, 2016. 

DAVID, B. AWS: Amazon web services tutorial for beginners. [S. 1:s. nJ, 2018. 

DUA, R; RAJA, A. R;; KAKADIA, D. Virtualization vs containerization to support paas. 
In: INTERNATIONAL CONFERENCE ON CLOUD ENGINEERING, 2014. Proceedings [...]. 
[S. 1): IEEE, 2014. p. 610-614, 

ELDRIDGE, |. What is container orchestration? [S. |]: New Relic, 2018. Disponível em: 
https://blog.newrelic:com/engineering/container-orchestration-explained/. Acesso 
em: 27 jun. 2020. 

ESSIEN, D. Understangind kubernetes objects. [S. |]: Megalix Blog, 2020. Disponível em: 
https:/Avww.magalix.com/blog/understanding-kubernetes-objects. Acesso em: 
28 jun. 2020. 

FREITAS, R. Ferramentas de orquestração de containers: como escolher? Rio de Janeiro: 

Vertigo Tecnologia, 2019. Disponível em: https://vertigo.com.br/ferramentas-de-or-

questracao-de-containers/ Acesso em: 27 jun. 2020. 

GIROLINETO, R. Docker swarm: exemplo de utilização. [S. 1]: Medium, 2017. Dis-
ponível em: https://medium.com/opensanca/docker-swarm-exemplo-de-
-utilizaWC3%A7WC3%A30-bSbce4C247b0. Acesso em: 27 jun. 2020. 

GITHUB. [Kubernetes]. San Francisco, c2020. Disponível em: https://raw.githubusercon-

tentcom/kubernetes/website/master/content/en/examples/application/deployment. 

yami. Acesso em: 23 jul. 2020. 

HIGHTOWER, K.; BURNS, B.; BEDA, J. Kubernetes: up and running: dive into the future of 
infrastructure. Boston: O'Reilly Media, 2017. 

ISABEKYAN, N. Introduction to kubernetes architecture. [S. |): X-team, 2016. Disponível 
em: https://x-team.com/blog/introduction-kubernetes-architecture/. Acesso em: 
23 jul. 2020. 

KANG, H; LE, M, TAO, S. Container and microservice driven design for cloud infras-

tructure DevOps. /n: INTERNATIONAL CONFERENCE ON CLOUD ENGINEERING, 2016. 

Proceedings [.J. [S. |]: IEEE, 2016. p. 202-211. 


Ferramenta de orquestração de contêineres | ( 151 

KHAN, A. Key characteristics of a container orchestration platform to enable a modern 
application. [EEE Cloud Computing, v. 4, n. 5, p. 42-48, 2017. 

KUBERNETES. Kubernetes object management. [S. 1], 2020a. Disponível em: https:// 

kubernetes.io/docs/concepts/overview/working-with-objects/object-management/. 

Acesso em: 23 jul. 2020. 

KUBERNETES. Web UI (dashboard). [S. 1), 2020b. Disponível em: https://kubernetes.io/ 
docs/tasks/access-application-cluster/web-ui-dashboard/. Acesso em: 23 jul. 2020. 

LICCARDI, A. T. Nomad vs kubernetes without the complexity. [S. 1]: Codemotion, 2020. 
Disponível em: https://Awww.codemotion.com/magazine/dev-hub/backend-dev/ 
nomad-kubernetes-but-without-the-complexity/. Acesso em: 27 jun. 2020. 

LINTHICUM, D. Container orchestration tools, strategies for success. Newton: TechTarget, 

2015. Disponível em: http://searchitoperations.techtarget.com/tip/Container-orches-

tration-tools-strategies-for-success. Acesso em: 27 jun. 2020. 

MANGAT, M. Whatis container orchestration? Benefits and howit works. [S. |): phoenixNAP, 
2020. Disponível em: https://phoenixnap.com/blog/what-is-container-orchestration. 
Acesso em: 27 jun. 2020. 

REIS, D.C. e gestão de conteinerizadas. Dissertação (Mestrado)

S. Execução aplicações 2017. 
— Universidade do Porto, Porto, 2017. Disponível em: https://repositorio-aberto.up.pt/ 
bitstream/10216/109650/2/236222.pdf. Acesso em: 27 jun. 2020. 
RUBENS, P. What are containers and why do you need them? CIO, 27 jun. 2017. Disponível 
em: http://www.cio.com/article/2924995/enterprise-software/what-are-containers-
-and-why-do-you-need-them.html, Acesso em: 27 jun. 2020. 

Leituras recomendadas 

CAMISSO, J.; JETHA, H.; JUELL, K. Kubernetes for full-stack developers. New York: Digita-
Ocean, 2020. Disponível em: https://assets.digitalocean.com/books/kubernetes-for-
-full-stack-developers.pdf. Acesso em: 23 jul. 2020. 

ELLINGWOOD, J. Uma introdução ao Kubernetes. New York: DigitalOcean, 2018. Dispo-
nível em: https://www.digitalocean.com/community/tutorials/uma-introducao-ao-
-kubernetes-pt. Acesso em: 23 jul, 2020. 

SANTOS, L. Kubernetes: tudo sobre orquestração de contêineres. [S. []: Casa do Código, 
2019. 

SARVEPALLI, K. C. Kubernetes objects. [S. |]: Medium, 2018. Disponível em: https://me-
dium.com/echkrishna/kubernetes-objects-e0a8b93b5cdc. Acesso em: 23 jul. 2020. 

VAUGHAN-NICHOLS, S. What is puppet? And why should you consider it for your cloud and 
servers? San Jose: HPE, 2017. Disponível em: https:/Awww.hpe.com/us/en/insights/arti-
clesAvhat-is-puppet-and-why-should-you-consider-it-for-your-cloud-and-servers-1711. 
html. Acesso em: 26 jun. 2020. 

e d 


152 ) (Fermento de orquestração de contêineres 
Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 


Cluster Kubernetes 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Descrever a criação de um cluster Kubernetes. 
E Demonstrar o escalonamento de um serviço no Kubernetes. 
m Explicar o funcionamento de aplicações stateless e stateful. 

Introdução 

O cluster Kubernetes é um sistema de orquestração de contêineres cujo 
principal objetivo é automatizar a implantação e o dimensionamento de 
aplicações O Kubernetes permite que os desenvolvedores implantem 
suas próprias aplicações sempre que quiserem. No entanto, o Kubernetes 
não beneficia apenas os desenvolvedores: ele também auxilia a equipe 
de operações, viabilizando o monitoramento e o (reJagendamento au-
tomático de aplicações em caso de falhas de hardware. Em resumo, para 
os administradores de sistemas, o foco passa da supervisão de aplicativos 
individuais para, principalmente, a supervisão e o gerenciamento do Ku-
bernetes e do restante da infraestrutura, enquanto o próprio Kubernetes 
cuida dos aplicativos. 

Neste capítulo, você vai ver como criar um cluster Kubernetes. Além 
disso, você vai conhecer os principais pré-requisitos e conceitos rela-
cionados a essa tarefa, bem como os principais comandos necessários 
para realizar os primeiros manuseios no cluster Kubernetes e para realizar 
a escala de aplicações. Por fim, você vai ler sobre o funcionamento de 
aplicações stateless e stateful. 



154 ubernetes 

1 Introdução ao Kubernetes 

O Kubernetes, também conhecido pelo acrônimo “K8s”, é um produto open 
source desenvolvido na linguagem Go e utilizado para automatizar a implan-
tação, o dimensionamento e o gerenciamento de aplicativos em contêiner 
(KUBERNETES, 2020; CAMISSO; JETHA; JUELL, 2020). No âmbito da 
orquestração de contêineres prontos para produção, o Kubernetes agrupa 
os contêineres que constituem uma aplicação em unidades lógicas, visando 
a facilitar o gerenciamento e a descoberta de serviços. Um dos elementos 
responsáveis pela eficácia e pela eficiência do Kubernetes no mercado são os 
anos de experiência na execução de contêineres em produção no Google. Esse 
fator se alia a outros aspectos importantes, como as melhores ideias e práticas 
da comunidade baseada na cultura DevOps (KIM et al., 2018). 

O Kubernetes foi concebido com base nos mesmos princípios que viabi-
lizam que o Google execute milhares de contêineres semanalmente. O K8s 
pode então ser redimensionado sem a necessidade de se aumentar a equipe 
de operações (KUBERNETES, 2020). Uma característica importante do 
Kubernetes é a flexibilidade, pois ele consegue prover aplicativos de forma 
consistente e facilitada independentemente da complexidade da tarefa em 
questão (em testes que ocorrem local ou globalmente). 

Outra característica importante é que o Kubernetes é open source, e esse 
fato garante liberdade. Assim, o Kubernetes pode ser utilizado de diferentes 
maneiras, como em seu cluster próprio (de forma local), em um sistema de 
arquitetura híbrida ou até mesmo em qualquer provedor de computação na 
nuvem pública. Essa liberdade permite que as aplicações sejam facilmente 
movidas para onde for necessário. A partir disso, entende-se que a instalação 
de um cluster Kubernetes pode ser realizada de diferentes formas. 

Criação de um cluster Kubernetes 

Na sequência, você vai conhecer os passos para a instalação de um cluster 
Kubernetes em três nodos no Linux. Essa criação requer um ambiente com 
três máquinas virtuais (Virtual Machines — VMs) com o sistema operacional 
Ubuntu instalado. Quase todos os passos descritos devem ser realizados nas 
três VMs manipuladas que compõem o ambiente em questão (exceto os que 
explicitam que devem ser executados apenas no nodo master, espécie de nodo 
principal). A seguir, veja o passo a passo. 



Cluster Kubernetes || 155 

1. Atualizar o repositório e instalar o Advanced Package Tool (APT): 
apt-get update && apt-get install -y apt-trans-
port-https. Essa etapa consiste em preparar as VMs para os passos 
posteriores. Além de atualizar os repositórios do sistema operacional 
em questão, esse comando também ativa o suporte https do APT. 
2. Verificar se a memória virtual swap (TANENBAUM; BOS, 2016) está 
desabilitada por meio do comando swapoff -a. A ideia é garantir 
que a swap fique desabilitada permanentemente, isto é, que essa fun-
cionalidade não seja habilitada automaticamente quando a máquina 
for reiniciada. Dessa forma, é necessário acessar o arquivo fstab, 
cujo caminho é etc/fstab, e comentar a linha de entrada referente 
à swap, isto é, a linha em que a swap está sendo declarada. O comando 
para isso é: vim /etc/fstab. É preciso adicionar o símbolo de 
comentário (H) no início da linha. 

Fique atento 

A swap desabilitada é um pré-requisito fundamental do sistema operacional para 

viabilizar a criação de um cluster Kubernetes. 

3. Realizar a instalação do Docker. De modo a garantir que a última versão 
do Docker seja instalada, o seguinte comando deve ser utilizado: curl 
-fsSL https://get.docker.com | bash. Esse comando vai 
executar o script que está na página get . docker, o qual adiciona o 
repositório do Docker nas máquinas e, na sequência, realiza a insta-
lação do Docker de forma automatizada. Finalizando a execução do 
script nas três máquinas, pode-se dizer que o Docker já está em todo 
o ambiente. Você deve se certificar de que a instalação do Docker 
foi bem-sucedida em todo o ambiente. Para tal avaliação, o comando 
docker -version é interessante, uma vez que a saída esperada 
desse comando no terminal é a listagem de informações referentes ao 
Docker, principalmente a sua versão. 

156 | | Cluster Kubernetes 

4. Adicionar o repositório do Kubernetes no ambiente. 
= A primeira etapa desse passo consiste em importar a chave do 
repositório (nesse caso, do Ubuntu) por meio do comando curl 
-s https://packages.cloud.google.com/apt/doc/ 
apt-key.gpg | apt-key add -. A saída desse comando no 
terminal deve ser simplesmente a palavra “ok”. 

= Após importar a chave, a segunda etapa consiste em adicionar o 
repositório do Kubernetes no Ubuntu. Para tanto, um echo com 

o endereço do repositório é utilizado: echo deb http://apt. 
kubernetes.io/ kubernetes-xenial main" > /etc/ 
apt/sources.list.d/kubernetes.list. É importante 
destacar que a saída do comando echo é redirecionada (“>” é o 
símbolo de redirecionamento) para o arquivo kubernetes. list, 
cujo caminho está especificado (/etc/apt /sources.list.d— 
diretório em que se encontram todos os arquivos de repositórios no 
Linux). Toda vez que esse diretório é modificado, você deve utilizar o 
comando apt-get updade para que o sistema faça o download de 
toda a listagem de pacotes disponíveis nele. O repositório Kubernetes 
deve ter sido adicionado e atualizado no ambiente. 
5. Instalar componentes importantes do Kubernetes, o kubelet, o ku-
bectl eco kubeadm: apt-get install kubelet Kkubectl 
kubeadm -y. 
= O kubeadm é o componente responsável por montar o cluster 
Kubernetes. Em outras palavras, o kubeadm será utilizado para 
realizar a montagem do cluster propriamente dito. 
= O kubect1, também conhecido como “Kube Control”, é o 
componente responsável por operar o cluster. Por meio dele, são 
realizadas todas as interações com o cluster. É por meio do kubect 1 
que se realiza o deployment de uma aplicação, por exemplo, entre 
tantas outras ações possíveis. 

= O kubelet é um agente do Kubernetes que roda em todos os nodos 
do ambiente. Esse componente é fundamental pois é responsável 
pela comunicação de todos os nodos do ambiente com a Application 
Programming Interface (API) do Kubernetes do nodo master. 



Cluster Kubernetes 157 

6. Iniciar, após a seleção de uma das VMs como nodo master, o kubeadm 
que já foi instalado (ele será iniciado apenas nessa VM): kubeadm 
init --apiserver-advertise-address $ (hostname 
-1).O comando kubeadm init vai iniciar o cluster no nodo master 
e pode levar um ou dois minutos para ser executado completamente. 
Em outras palavras, esse comando vai originar todo o ambiente ne-
cessário para o funcionamento do cluster que está sendo criado. Em 
seguida, os parâmetros após o init consistem em apontar o IP prin-
cipal, ou seja, o IP do nodo master. Além de iniciar o nodo master, 
a saída desse comando vai indicar quais são os próximos passos e qual 
é o comando necessário para que outros hosts sejam adicionados ao 
cluster em questão. Veja a Figura 1. 

To start using your cluster, you need to run the following as 
a regular user: 

1. mkdir -p SHOME/. kube 
2. sudo cp -i /etc/kubernetes/admin.conf 
SHOME/.kube/config 
3. sudo chown S(id -u):$(id -g) SHOME/.kube/config 
Figura 1. Saída do comando kubeamd init. 

7. Depois de o nodo master Kubernetes ser inicializado com sucesso, 
criar estruturas de diretórios indicadas como saída do passo anterior. 
Todos os comandos utilizados nessa etapa deverão ser invocados como 
superusuário, ou seja, como root e no nodo master. O primeiro passo 
(mkdir -p SHOME/.kube) consiste na criação de um diretório 
que conterá um arquivo de configuração e as chaves, que são muito 
importantes para a comunicação do kubect 1 com o clusterem ques-
tão. O segundo (sudo cp -i ...) consiste em realizar a cópia do 
arquivo admin. conf para o diretório criado no primeiro passo com 
o nome config. Por fim, o terceiro comando mostrado na Figura 1 
tem como foco ajustar as permissões para o usuário corrente, o qual 
modifica o “dono do diretório” a partir do parâmetro id. Em suma, 
o diretório foi criado, o arquivo admin. config foi copiado para o 
diretório (. kube) como config e, por fim, as permissões desses 
arquivos foram alteradas. 

158 || Cluste Kubernetes 

Outra saída importante do comando kubeadm init é o comando 
kubeadm join. Veja a Figura 2. 

Figura 2. Saída do comando kubeadm join 

A Figura 2 apresenta um exemplo do comando kubeadm join, que é 

composto pelo hash, pelo IP, pela porta, entre outros artefatos. Executando 

esse comando como root, é possível unir os nodos do ambiente ao cluster 

Kubernetes. Para tanto, esse comando deve ser copiado e colado nos outros 

dois nodos do cluster. A saída desse comando indica que as outras máquinas 

(nodos) a partir desse momento também fazem parte do cluster. Observe a 

Figura 3. 

foi inform 

Figura 3. Mensagem de resposta após execução do comando kubeadm join. 

A mensagem explicitada na Figura 3 informa que os demais nodos do 
cluster foram unidos ao nodo master. Para verificar se as outras máquinas 
realmente fazem parte do cluster, isto é, se todos os nodos estão integrados, 
execute o comando kubect 1 get nodes no nodo master. A saída desse 
comando lista as três máquinas que fazem parte do cluster. Veja na Figura 4. 



Cluster Kubernetes || 159 

Name Status Roles Age Version 
kube-01 NotReady master im v1.9.3 
kube-02 NotReady <none> 19s v1.9.3 
kube-03 NotReady <none> 17s v1.9.37 

Figura 4. Saída do comando kubect1 get nodes. 

A Figura 4 mostra um exemplo da saída do comando kubect 1 get nodes 

disparado da máquina master. Perceba que todos os nodos que compõem o 

cluster são listados com: 

o nome de cada um deles (name); 
o estado atual (status), e nesse caso nenhum dos nodos está pronto ainda; 
o papel de cada um dos nodos no cluster (roles), e nesse caso apenas 
o nodo 1 é master; 
o tempo de existência de cada um deles (age); 
as suas respectivas versões (version). 
Perceba que o status de todos os nodos é Not Ready, ou seja, os nodos já 
estão no cluster, mas ainda não estão prontos. Portanto, é necessário instalar 

o pod network. O pod network é um complemento (add-on) do Kubernetes 
que viabiliza a configuração de políticas de rede (network policy) e outros 
recursos no ambiente. 
Saiba mais 

Complementos do Kubernetes são pods e serviços que implementam recursos de 
cluster. Os pods são as menores unidades de execução do Kubernetes e estendem a 
funcionalidade desse sistema, viabilizando a instalação de complementos para uma 
variedadede recursos de cluster, incluindo rede e visualização, por exemplo. Isto é, eles. 
realizam toda a comunicação com os nodos; assim, são componentes fundamentais 
para o funcionamento da rede e de vários outros serviços do cluster. 


160 | | Cluster Kubernetes 

O Weave Net será utilizado nessa criação do cluster Kubernetes. O Weave 
Net cria uma rede virtual que conecta contêineres Docker em vários hosts e 
permite sua descoberta automática. Com o Weave Net, os aplicativos baseados 
em microsserviços portáteis, que consistem em vários contêineres, podem ser 
executados em qualquer lugar: em um host, em vários hosts ou até mesmo em 
provedores de nuvem e datacenters. 

Para obter o Weave Net e realizar o deploy no nodo master (kube-01), 
execute o comando: kubect1 apply -f£ "https://cloud.weave. 
works/k8s/net?k8s-version=$ (kubectl version | base64 

[| tr -d 'An'!). Veja resultado na Figura 5. 

ave-net" created 
6 dae reated 

Figura 5. Resultado após executar o comando referente ao 
Weave Net no nodo master 

Observando a Figura 5, perceba que o Weave Net foi obtido com sucesso, 

os pods foram criados. A partir desse momento, os nodos deverão estar com 

ostatus ready (“pronto”). 

Para verificar se todos os nodos estão realmente prontos, no nodo master, 
execute o comando kubect1 get nodes novamente. Veja o resultado da 
execução desse comando na Figura 6. Ela demonstra que o cluster Kubernetes 
está configurado e que seus nodos estão em pleno funcionamento (todos os 
nodos do cluster estão com o status Ready). 

Name Status Roles Age Version 
kube-01 Ready master 4m v1.9.3 
kube-02 Ready <none> 3m v1.9.3 
kube-03 Ready <none> 2m v1.9.32 

Figura 6. Saída do comando kubect 1 get nodes após deploy do Weave Net 


Cluster Kubernetes 

2 Manipulação, gerenciamento e 
escalonamento 

Após a instalação do cluster Kubernetes, você deve realizar os primeiros passos 
para o uso efetivo do ambiente criado. Afinal, quando o cluster é criado, por 
padrão o nodo master não recebe novos contêineres, ou seja, se você criar 
alguma aplicação, o master não vai recebê-la. Confira a seguir alguns passos 
iniciais necessários para a utilização do cluster Kubernetes. 

1. Listar os nodos: antes de iniciar o uso efetivo do cluster, é interessante 
verificar quais são os nodos (ou nós) em execução. Para realizar essa 
etapa, basta utilizar o comando kubectl get nodes como root. 
Esse passo é importante para saber quais são os nós que compõem o 
cluster e, a partir disso, começar a utilização dele. Um exemplo com a 
saída desse comando está ilustrado na Figura 6. Observando a Figura 6, 
perceba que o cluster é composto por três nodos (kube-01, kube-02 

e kube-03); um deles é o master e os demais são os workers. 

2. Conferir informações de um nodo específico: após visualizar os nodos 
que compõem o cluster e certificar-se de que estão execução, você 
pode verificar a descrição de um nodo específico por meio do comando 
kubectl describe nodes + nome do nodo desejado, 
por exemplo, kubectl describe nodes kube-01, como root. 
A saída desse comando é extensa e exibe muitas informações sobre 

os nodos, como a quantidade de pods que estão em execução neles. 

3. Verificar quais são os pods do próprio Kubernetes que estão em 
execução no cluster: para efetivar essa verificação, é preciso invocar 
o comando kubectl get pods -n + namespace no nodo 
master. Nesse caso, o comando é kubect1 get pods -n kube-
-system. Esse comando lista algumas informações sobre todos os 
pods que constituem o Kubernetes, isto é, sobre os pods internos do 
próprio Kubernetes (normalmente 12 pods), tais como name, read, 
status, restarts e age. Veja algumas informações sobre os pods listados: 
(i) kube-apiserver, que é um componente que roda no master e é 
responsável pela API do Kubernetes; (ii) etcd, que também é executado 
no master e é um componente que armazena tudo o que está ocorrendo 
com o Kubernetes no momento da invocação do comando, ou seja, todas 
as informações sobre o Kubernetes localizam-se no et cd, também 
conhecido como um “banco de informações”, e o componente que se 

162)| Cluster Kubernetes 

comunica como etcdéo kube-apiserver. Dessa forma, perceba 
que todos os pods listados estão dentro do namespace do Kubernetes 
(pods de uso exclusivo do Kubernetes), que éo kube-system, e todos 
esses pods são essenciais para que o Kubernetes funcione. Além dos 
descritos, também há outros exemplos, como kube-controllers 
ekube-proxy's. 

Também é possível utilizar um comando para analisar informações sobre 
um pod. Para tanto, no nodo master, como root, utilize: kubect1 describe 
pod + nome do pod -n + namespace. Por exemplo: kubect1 
describe pod kube-apiserver-kube-01 -n kube-system. 
A saída desse comando é uma vasta lista de informações sobre o pod pesqui-
sado, como data de criação, comandos e volumes. 

É importante salientar que estão sendo verificados pods do próprio Ku-
bernetes, pois ainda não foi realizado nenhum deploy no nodo criado. Outro 
comando interessante relacionado com o get pods é o seguinte: kubect 1 
get pods -o wide -n kube-system, também como root do nodo 
master. Veja a saída desse comando no Quadro 1. 

/ 
Quadro 1. Saída do comando kubect1 get pods -o wide -kube-system ) 

Name READY | Status | Restarts | Age IP Node 

etcd-1/1 Running 0 19s 10.32.0.2 kube-01 

-kube-01 

kube-api-1/1 Running 0 19s 10.32.0.3 kube-01 

server-

-kube-01 

weave-2/2 Running 0 19s 10.40.0.1 kube-03 

-net-6sqp2 

weave-2/2 Running 0 19s 10.40.0.2 kube-02 

-net-f8641 


Cluster Kubernetes || 163 

O Quadro 1 apresenta uma amostragem do resultado da execução do 
comando mencionado. A saída desse comando lista os pods que estão execu-
tando no cluster Kubernetes criado. Perceba que o comando mostra diversas 
informações, como o nome dos pods que estão rodando, o número de réplicas do 
deployment que estão disponíveis para os usuários (seguindo um padrão pronto/ 
desejado), o status (nesse caso, os pods estão em execução), a quantidade de 
vezes que os pods foram reiniciados, o tempo de execução do deployment, o IP 
de cada um dos pods e o nome dos nodos que estão executando cada um deles. 

1. Realizar o primeiro deploy: o objetivo dessa etapa é realizar o pri-
meiro deployment no cluster Kubernetes que acabou de ser criado 
e, como você viu nos passos anteriores, está pronto para manuseio e 
gerência. Para realizar essa demonstração, será realizado o deploy do 
Nginx. O Nginx Docker é uma aplicação web leve e popular utilizada 
para desenvolvimento no servidor. É um servidor web open source 
desenvolvido para executar em vários sistemas operacionais. Dessa 
forma, a partir dessas características interessantes, o Docker garantiu 
suporte a essa aplicação. 
= Execução do comando no nodo master: kubectl run meu-
-nginx --image nginx. 

= O nome do deployment é meu-nginx. Após o parâmetro image, 
é necessário informar o nome da imagem que será utilizada para o 
deployment. Nessa linha de comando, o comando run é responsável 
por realizar o deployment, ou seja, ele vai buscar e preparar a ima-
gem do Nginx e, na sequência, iniciar o pod de modo a executar o 
Nginx no cluster. A saída desse comando no terminal será a seguinte: 
deployment .apps/meu-nginx created. Isto é, o deployment 
da imagem meu-nginx foi criado com sucesso. 

2. Realizar verificações: depois da realização do primeiro deployment, 
é preciso realizar as mesmas verificações descritas anteriormente por 
meio do comando kubect1 get pods. No entanto, nesse momento 
serão verificados os pods existentes no cluster, e não os pods internos 
do cluster (parâmetro kube-system), ou seja, do próprio Kubernetes. 
Veja a saída desse comando executado como root no nodo master no 
Quadro 2. 

Quadro 2. Saída do comando kubect 1 get pods após deploy do Nginx no cluster 
Kubernetes criado 

Name READY Status Restarts | Age 

meu-nginx-0/1 ContainerCreating | 0 19s 
64f497€8fd-
296v9 

No Quadro 2, perceba que as informações reportadas são as mesmas 
descritas no Quadro 1, mas há menos dados, pois trata-se de um comando 
mais simples (sem os parâmetros -o wide). Nesse exemplo, há algumas 
diferenças quanto ao status do pod. O comando reportou que existe um pod, 

o Nginx, com o respectivo nome criado; em especial, apontou que o contêiner 
está sendo criado no status reportado. 
3. Fazer verificação completa: seguindo a mesma metodologia utilizada 
no segundo passo, uma verificação completa será aplicada ao pod recém-
-criado. O comando para essa verificação é: kubectl describe 
pods meu-nginx-64f497f8fd-296v9. Todas as informações 
referentes ao nodo reportadas no segundo passo serão reportadas para 
o pod invocado no comando. 
4. Verificar as informações do deployment que foi realizado: como 
mencionado anteriormente, quando o comando run é utilizado, 
um deployment é criado. Assim, nesse passo, a ideia é verificar as in-
formações do deployment que foi realizado. Para tanto, o comando a ser 
utilizado (como root no nodo master) é: kubect1 get deployments. 
Confira a saída desse comando no Quadro 3. 
Quadro 3. Saída do comando kubect 1 get deployments após deploy do Nginx 
no cluster Kubernetes criado 

Name Desired | Current | Up-to-date | Available | Age 

meu-nginx | 1 1 1 1 

495 


Cluster Kubernetes 165 

O Quadro 3 ilustra a saída do comando utilizado. O campo name apresenta 

o nome do deployment que foi passado no parâmetro namespace (nesse 
exemplo, meu-nginx) do comando run quando o deployment foi criado. 
O campo desired exibe o número de réplicas do deployment no cluster (que 
podem ser definidas no momento da criação do deployment). Por sua vez, 
current indica quantas réplicas do pod estão em execução no momento, 
enquanto up-to-date exibe o número de réplicas do que
deployment foram 
atualizadas para atingir o estado desejado. Já available reporta a quantidade 
de réplicas do deployment que estão disponíveis para os usuários do cluster 
e, por fim, há o campo age, que já foi descrito. 

1. Escalar imagens do Kubernetes: para esse fim, o comando a ser 
utilizado éo kubect1l scale deployment meu-nginx --re-
plicas=10 (no nodo master como root). Depois desse comando, 
10 réplicas do Nginx serão incorporadas ao cluster Kubernetes em 
questão. A saída desse comando será: deployment .extensions/ 
meu-nginx scaled, ou seja, a mensagem confirma que o pod rela-
cionado ao Nginx foi escalado com sucesso. Para ver mais detalhes sobre 
esse escalonamento, ou seja, sobre a escala realizada, basta utilizar o 
comando kubect1l get deployments (descrito no sétimo passo). 
Veja a saída desse comando no Quadro 4. 
Quadro 4. Saída do comando kubect1 get deployments após deploy do Nginx 
no cluster Kubernetes criado 

Name Desired | Current | UP-TO-DATE | Available | Age 

meu-nginx | 10 10 10 5 2m 


166 || Cluste 

As informações exibidas no Quadro 4 são análogas às informações mos-
tradas no Quadro 3. A diferença é que no Quadro 4 existem 10 pods do Nginx 
em execução no cluster, e não apenas uma. 

1. Obter mais detalhes sobre a escala de pods realizada: para isso, 
é possível utilizar o comando kubect 1 get pods novamente, com a 
diferença de que agora os 10 pods estão em execução no cluster, isto é, 
a tabela vai reportar todos os parâmetros que constituem a saída desse 
comando para cada um dos pods que foram implantados e estão em 
execução no cluster. Ademais, é importante ressaltar que o comando 
kubectl get pods.deployments concatena os resultados dos 
dois comandos anteriores em uma única saída. 
2. Saber onde cada um dos pods está executando no cluster: depois 
de verificar as primeiras informações sobre o pod e os deployments, 
também é interessante saber onde cada um dos pods está executando no 
cluster. O comando kubect1l get pods -o wide executado no 
nodo master como root fornece informações adicionais, especialmente 
o local de execução de cada uma das réplicas escaladas. Veja a saída 
desse comando no Quadro 5. 
O Quadro 5 ilustra uma amostragem de quatro das 10 réplicas do Nginx 

escaladas no oitavo passo. A saída desse comando lista todas as informações 
relativas aos pods que estão executando no cluster Kubernetes criado (veja 
o Quadro 4). Além disso, indica o IP e o nodo onde cada uma das réplicas 
incorporadas no cluster está executando. 

3. Remover o deployment realizado no quarto passo: nesse momento, 
o cluster possui 10 pods do Nginx em execução. Para remover, é preciso 
utilizar o comando kubect1 delete meu-nginx. A saída desse 
comando consiste na seguinte mensagem: deployment .exten-
sions “meu-nginx” deleted, confirmando que o deployment 
foi removido com sucesso. Para verificar se o deployment realmente foi 
removido, utilize o comando kubect1l get deployments. A saída 
será No resources found, ou seja, nenhum pod foi encontrado 
no cluster. 

Cluster Kubernetes 167 

“aisnjp OU xuIBN Op Aojdap op ejeosa e sode sp tm 

zo-san%

T'O"Ob'OT

s6T

0Suquuny T/T 

| 

6MPIU-PIg3L6Ib9-xUTBU 

now€o-sanx

E"O'ZE"OT

s6T

0

buzuung

T/T

|

pbbua-psg3L6b3/9-xutbu-newz0-sanx

E"O"OP"OT

s6T

0

butuuny

T/T

|

sPç19-p383L6I49-xutBu-now€o-san%

Z'0'zE"OT

s6T

0butuuny T/T 

| 

6496Z-P383L6b3b9-xuTbu-now 

o-

spod

396

T3994nx 

opueuo> 

op 

epies 

's 

ospend 

Ci 
ique aqui para visual lizar este conteúdo na horizontal. 


168 Cluster Kubernetes 

4. Verificar todos os pods em execução: para verificar tanto os pods do 
próprio Kubernetes quanto aqueles adicionados ao cluster, você pode 
usar o comando kubectl get pods --all nameespaces. 
A saída desse comando listará todos os pods de todos os namespa-
ces em execução (nesse exemplo, kube-system para os pods do 
Kubernetes e meu-nginx para os pods adicionados). 
3 Aplicações stateless e stateful 

Como você já viu, o Kubernetes é um mecanismo de orquestração de contê-
ineres open source para automatizar, gerenciar e escalonar o deployment de 
aplicações (KUBERNETES, 2020). No entanto, você deve considerar alguns 
critérios importantes antes de executar uma nova aplicação em um cluster em 
produção. Em especial, você deve considerar o estado da aplicação a partir de 
uma avaliação da arquitetura subjacente a ela. 

Existem diferentes classificações relativas ao estado de cada aplicação. 
O estado de uma aplicação é a condição ou a qualidade dela em determinado 
momento, isto é, o seu estado de existência (REDHAT, 2020). A fim de deter-
minar se uma aplicação é stateless ou stateful, você deve levar em consideração 

o tempo em que o estado de interação da aplicação é registrado e a maneira 
como as suas informações necessitam ser armazenadas. A seguir, você vai 
conhecer os principais conceitos das aplicações stateless e stateful e verificar 
como o Kubernetes lida com cada uma delas. 
Aplicações stateless 

As aplicações ou processos stateless tratam-se de aplicações sem estado. 
Elas fornecem uma função ou um serviço e utilizam a rede de distribuição de 
conteúdo (Content Delivery Network — CDN) (REDHAT, 2020). Exemplos 
desse tipo de aplicações são a web e os servidores de impressão que processam 
solicitações a curto prazo. 

Uma aplicação stateless não depende de armazenamento persistente. É o 
caso de um site formado por conteúdos estáticos (imagens, códigos Javascript, 
CSS, descrição do HTML e do CSS, etc). A quantidade de vezes que as URLs 
são requisitadas não interfere no resultado, pois o site será sempre estático. 
Nesse cenário, o seu cluster fica responsável basicamente pelo código e por 



Cluster Kubernetes 

outros conteúdos estáticos que são hospedados nessa aplicação. Dessa forma, 
por tratar-se de uma aplicação sem persistência (nesse exemplo), gera o mesmo 
resultado, sem alterações em bancos de dados e sem escritas. Quando o pod 
é excluído, não resta nenhum arquivo no cluster, isto é, não fica armazenada 
nenhuma referência ou informação sobre antigas transações. Nesse sentido, 
pode-se dizer que as aplicações stateless são e lidam com recursos isolados. 

Como exemplo de transação stateless, considere uma pesquisa on-line. 
Se, depois de digitar o conteúdo a ser consultado no mecanismo de busca, 

o usuário não obtém um resultado bem-sucedido (devido a, por exemplo, uma 
interrupção na pesquisa ou até mesmo o encerramento dela), ele deve iniciar 
o processo novamente. Nesse contexto, as transações stateless são como 
máquinas de vendas automáticas: uma solicitação é realizada e uma resposta 
para essa solicitação é recebida. Assim, fica evidente que é necessário realizar 
uma solicitação por vez (REDHAT, 2020). 
Aplicações stateful 

Por outro lado, as aplicações ou processos stateful tratam-se de aplicações com 
estado. As aplicações e processos stateful podem ser utilizados mais de uma 
vez, como é o caso de serviços de e-mails e serviços bancários na modalidade 
on-line (REDHAT, 2020). 

Uma aplicação stateful pode prover diferentes resultados, até mesmo quando 
as requisições são exatamente iguais. Um exemplo simples desse tipo de 
aplicação é um banco de dados. Ao contrário das aplicações stateless, as 
aplicações stateful possuem persistência, então os dados armazenados podem 
ser modificados a partir de requisições externas, ou seja, de requisições de 
um usuário ou até mesmo do próprio sistema, de acordo com a sua natureza. 
Nesse tipo de aplicação, os dados persistidos e utilizados na montagem das 
respostas variam no tempo; assim, pode haver resultados diferentes para as 
mesmas solicitações/requisições. 

Diferentemente das transações stateless, as transações stateful consideram 
resultados de transações anteriores, ou seja, os resultados das suas solicitações 
estão diretamente relacionados com as transações ocorridas previamente, 
de modo que as transações anteriores podem afetar o resultado das solicitações 
atuais (REDHAT, 2020). Considerando o comportamento descrito, é importante 
mencionar que aplicações stateful utilizam sempre os mesmos servidores para 
processar uma solicitação do usuário. 



170]| Cluster Kubernetes 

Ao contrário das aplicações stateless, que não armazenam nenhuma infor-
mação quando uma transação é efetivada, as transações stateful, se porventura 
forem interrompidas ou encerradas durante o processamento, podem retomar 

o estado da transação de onde pararam. Isto é, como as informações anteriores 
e históricos são armazenados, em caso de erro durante o processamento de 
uma transação stateful, não há necessidade de iniciar o processo da transação 
novamente do zero. Uma das principais características das aplicações stateful 
é o acompanhamento de informações, configurações e atividades recentes. 
Um exemplo prático e simples de transação stateful é uma conversa contínua 
e periódica com a mesma pessoa. 
Fique atento 

Apesar das diferenças, as aplicações statefule stateless são muito parecidas. Por exemplo, 
considere uma aplicação stateless que não requer armazenamento a longo prazo, 
mas, por outro lado, permite que o servidor acompanhe todas as solicitações do 
mesmo cliente por meio dos cookies (REDHAT, 2020). Dessa forma, pode-se dizer que 
as características dos dois tipos de aplicações se assemelham cada vez mais. 

Funcionamento das aplicações no Kubernetes 

Atualmente, a maioria das aplicações utilizadas no cotidiano é do tipo stareful. 
Além disso, a partir dos avanços tecnológicos crescentes, os microsserviços 
e contêineres auxiliam no processo de criação e deployment de aplicações na 
nuvem. Inicialmente, os contêineres foram projetados visando às aplicações 
stateless; no entanto, com o passar do tempo, o projeto dos contêineres foi 
sendo alterado e, na sequência, as aplicações stateful também começaram 
a ser empacotadas para suas respectivas execuções em contêineres. Assim, 
a conteinerização das aplicações tornou-se uma prática comum da comunidade 
baseada na cultura de DevOps. Ela visa principalmente a facilitar a transfe-
rência e a execução das aplicações em todos os ambientes, de qualquer lugar e 
a qualquer momento, independentemente da estrutura subjacente ao sistema. 



Cluster Kubernetes 

Coma intensificação da utilização da conteinerização, surgiram diferentes 
gerenciamentos de contêineres stateful e stateless. Esses gerenciamentos podem 
ser feitos de três maneiras: via armazenamento de dados, por meio do Kuberne-
tesea partirdo Statefulset. O StatefulsSet é um objeto workloadAPI 
(API voltada para “carga de trabalho”) que compõe o mecanismo Kubernetes 
utilizado para gerenciar aplicações stateful. O StatefulSet gerencia o 
deployment e o dimensionamento de um conjunto de pods e fornece garantias 
relativas à ordenação e à singularidade desses pods (KUBERNETES, 2020). 

Assim como um deployment, o Statefulset gerencia pods que são 
baseados em uma especificação de contêineres idênticos. Mas, ao contrário de 
um deployment (que gera um ID diferente para cada um), 0 StatefulSet 
mantém uma identidade fixa para cada um de seus pods (KUBERNETES, 
2020). Esses pods são criados a partir da mesma especificação, no entanto não 
são intercambiáveis, isto é, cada um tem um identificador persistente, que é 
mantido em caso de qualquer reprogramação ou atualização. 

Se for necessário usar volumes de armazenamento para fornecer persistência 
para o workload, o StatefulSet também pode ser utilizado como parte da 
solução. Embora os pods individuais em um State fulSet sejam suscetíveis 
a falhas, os identificadores de pod persistentes facilitam a correspondência 
entre os volumes existentes e os novos pods que substituem os que falharam. 

Os StatefulsSets são valiosos para aplicações requerem um ou mais 
dos seguintes aspectos: 

identificadores de rede únicos e estáveis; 
armazenamento estável e persistente; 
deployment e dimensionamento ordenados; 
atualizações contínuas ordenadas e automatizadas. 

Estabilidade é sinônimo de persistência no (reJagendamento (scheduling) 
de pods. Se uma aplicação não exigir nenhum identificador estável ou de-
ployment ordenado, exclusão ou escalonamento, a solução é realizar o deploy 
dessa aplicação utilizando um objeto workload que fornece um conjunto de 
réplicas stateless. Deployment ou ReplicaSet são soluções mais adequadas 
nesse cenário de necessidades. Um deployment fornece atualizações para pods 
ReplicaSet,eo objetivo deum ReplicaSet é manter um conjunto estável 
de pods de réplica em execução a qualquer momento (KUBERNETES, 2020). 



172 || Cluster Kubernetes 
Referências 
CAMISSO, J,; JETHA, H,; JUELL, K. Kubernetes for full-stack developers. New York: Digita-
IOcean, 2020. Disponível em: https://assets.digitalocean.com/books/kubernetes-for-
-full-stack-developers.pdf. Acesso em: 25 set. 2020. 
KIM, G. etal. Manual de DevOps: como obter agilidade, confiabilidade e segurança em 
organizações tecnológicas. Rio de Janeiro: Alta Books, 2018. 
KUBERNETES. [5. . NJ, 2020. Disponível em: https://kubernetes.io/. Acesso em: 
25 set. 2020. 
REDHAT. Aplicações nativas em nuvem: stateful x stateless. [S. |:s. nJ, 2020. Disponível 
em: https://www.redhat.com/pt-br/topics/cloud-native-apps/stateful-vs-stateless. 
Acesso em: 25 set. 2020. 
TANENBAUM, A. S.; BOS, H. Sistemas operacionais modernos. 4. ed. São Paulo: Pearson, 
2016. 
Leitura recomendada 
DOCKER. Get-started. California: Docker, 2020. Disponível em: https:/Awvww.docker.com/ 
get-started. Acesso em: 25 set. 2020. 
a » 
Fique atento 
Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 
d 


Fundamentos do Jenkins 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Apontar as principais características do Jenkins, sua configuração 
básica e o fluxo de execução por meio de uma aplicação-base. 
E Descrever a criação de um pipeline no Jenkins para entrega contínua. 
E Reconhecer os plugins do Jenkins e suas funcionalidades. 

Introdução 

Aintegração contínua (CI, continuous integration) é uma das técnicas mais 
poderosas que pode ser adotada ao longo do processo de desenvolvi-
mento de software e apoia uma boa parte da cadeia de ferramentas do 
DevOps. A Cl é uma prática de desenvolvimento de software em que os 
membros de uma equipe fazem a integração do código em um repositório 
compartilhado várias vezes ao dia (FOWLER, 2006). Cada integração é 
verificada por um build automatizado, que pode detectar erros e localizá-
-los facilmente. Acredita-se que essa técnica ajuda a reduzir os problemas 
de integração e permite que a equipe desenvolva um software coeso 
de forma mais rápida. 

Jenkins é uma ferramenta open source de integração contínua escrita 
em Java. Essa ferramenta possui uma comunidade bem ativa e, atual-
mente, é utilizada por equipes de diferentes tamanhos em projetos que 
adotam, além de Java, outras diversas linguagens e tecnologias, como 
.NET, Ruby, Groovy, Grails e PHP (SMART, 2011). 

Neste capítulo, você conhecerá a ferramenta Jenkins, bem como 
as suas principais funcionalidades. Além disso, conhecerá o processo 
de instalação e configuração básica da ferramenta. Em seguida, verá 
como criar um pipeline para a entrega contínua (CD, continuous delivery) 
na ferramenta. Por fim, conhecerá os principais plugins que existem para 
auxiliar a execução de atividades no Jenkins. 



14) ( Fundamentos do Jenkins 
1 Jenkins 

O conceito de integração contínua (CI) chegou para revolucionar o processo de 
desenvolvimento do software, mudando completamente a forma como a equipe 
de desenvolvimento pensa e age. Uma boa estrutura de CI pode otimizar as 
etapas do processo até a implantação, ajudando a identificar e corrigir bugs de 
forma mais rápida e auxiliando as equipes a fornecer mais valor comercial ao 
usuário final (SMART, 2011). Entre os diversos benefícios da CI, destacam-se 
(DUVALL; MATYAS; GLOVER, 2007): 

E redução de riscos; 
E redução de processos manuais repetitivos; 
E geração de um software implantável em qualquer lugar, a qualquer 

momento; 
E melhora da visibilidade do projeto; 
E ganho de confiança no produto de software. 

O Jenkins é uma ferramenta open source de CI escrita em Java que possui 
interface do usuário simplificada e intuitiva. Além disso, essa ferramenta é 
considerada fácil de ser utilizada, possuindo uma curva de aprendizado muito 
baixa. Um ponto marcante sobre o Jenkins é que ele possui uma comunidade 
enorme e ativa, a qual é dinâmica e possui listas de discussão com membros 
empolgados e acolhedores. Em virtude disso, a cada semana, tem-se mais re-
cursos sendo lançados, bem como atualizações de plugins e correções de bugs. 

A Figura 1 apresenta um resumo básico do processo de desenvolvimento de 
software. Observe que, na Figura la, o desenvolvedor é responsável por todas as 
etapas do ciclo de desenvolvimento. Já na Figura 1b, ele faz apenas a criação do 
código e o commita no repositório, de modo que o Jenkins faz todas as outras 
atividades. Nesse contexto, é possível compreender o princípio de CI com a 
ferramenta, já que o desenvolvedor fica com a parte criativa (programação) 
e o Jenkins fica responsável por executar todas as tarefas repetitivas, como 
testes e deploy (BOAGLIO, 2016). Com essa automatização, é possível manter 

o sistema atualizado frequentemente, o que seria difícil sem o apoio de uma 
ferramenta. Além disso, o Jenkins possui integração através de plugins com 
várias outras ferramentas, como Rundeck, Slack e Gitlab, motivo pelo qual 
ele é tão aceito no mercado para realizar automações. 

Fundamentos do Jenkins ) (15 

Mr à 

(a) (b) 
f f Jenkins 
avisa o usuário cria o código + avisa o usuário cria o código +

commita repositório commita repositório

no 

no 

Jenkins Jenkins

publica o sistema testao sistema publica o sistema testa o sistema 

Figura 1. (a) Ciclo de desenvolvimento sem o Jenkins; (b) ciclo de desenvolvimento com 

o Jenkins. 
Fonte: Adaptada de Boaglio (2016), 
A popularidade do Jenkins tem crescido em virtude das características 

que ele oferece para desenvolvedores, sendo as principais listadas a seguir 

(SONI, 2015). 

E Open source e gratuito: é um recurso que recebe grande apoio da 
comunidade. 

Bm Facilidade na instalação em diferentes sistemas operacionais: é um 
programa autossuficiente baseado em Java, pronto para ser executado 
com pacotes nos sistemas operacionais Windows, Mac OS e Linux. 

m Facilidade na configuração: facilmente instalado e configurado por 
meio da sua interface da web, apresentando verificações de erro e uma 
função de ajuda integrada. Além disso, é possível customizar a sua 
interface com base nos gostos do usuário. 
Facilidade de atualizações: tem ciclos de lançamentos frequentes. 
Variedade de plugins disponíveis: há centenas de plugins no Update 
Center, a fim de integrar o Jenkins com todas as ferramentas do grupo 
de ferramentas de CI/CD. 

E Facilmente extensível: pode ser estendido com o uso de plugins, ofe-
recendo possibilidades quase infinitas de utilização. 



176 ) ( Fundamentos do Jenkins 
m Facilidade na distribuição: pode distribuir facilmente o trabalho em 
várias máquinas para compilações, testes e implantações mais rápidos 
em várias plataformas. A arquitetura do master suporta a distribuição 
de builds para reduzir as cargas no servidor de CI. 

Conceitos fundamentais 

Antes de se aprofundar um pouco mais no Jenkins, é necessário compreender 
alguns conceitos importantes para ter um melhor entendimento (SONI, 2015): 

Bm Job: tarefa que o Jenkins deve executar, a qual normalmente possui 
alguns parâmetros de execução. 

mB Build: é a execução de um job e pode possuir os seguintes status: 
falha, instável, feito com sucesso, pendente (esperando), cancelado ou 
desligado. 

m Artifact (artefato): pacote resultante de um build executado com su-
cesso; pode ser um arquivo pom.xml, JAR, WAR, entre outros. 

E Dashboard (painel principal): local onde se tem uma visão geral da 
execução de todos os jobs da ferramenta. Esse painel apresenta ícones 
relacionados ao clima para mostrar o status de cada job. Por exemplo, 
o símbolo de ensolarado mostra que está tudo correto, ao passo que o 
símbolo de tempestade indica a existência de problemas. 
Executor: threads que rodam os builds dos Jobs. 
Master node (nó mestre): roda o Jenkins e controla os nós escravos. 
Slave node (nó escravo): máquinas auxiliares conectadas. 
Workspace (área de trabalho): existe em cada job. É o local onde são 
baixados todos os arquivos necessários e onde são rodados os processos 
exigidos. 

Instalação e configuração 

O Jenkins é baseado no Java e pode ser instalado por meio dos pacotes do 
Ubuntu ou por meio do download do WAR. A seguir, será apresentado um 
breve tutorial da instalação do Jenkis, adicionando o repositório do Debian e 
utilizando-o para instalar o pacote com o APT (Advanced Packaging Tool). 



Fundamentos do Jenkins ) (17 

Para seguir as instruções, são necessários os seguintes requisitos (ANDER-
SON; JUELL, 2020): 

= um servidor Ubuntu 18.04 configurado com um usuário sudo não raiz 
e firewall seguindo o guia de configuração (ELLINGWOOD, 2018). 
Além disso, é recomendável começar com 1 GB de memória RAM, 
no mínimo; 
E Java 8 instalado seguindo os direcionamentos definidos para o OpenJDK 
no Ubuntu 18.04 (VLASWINKEL, 2020). 

Para aproveitar as últimas correções e os recursos da ferramenta, utilize 
os pacotes mantidos pelo projeto para instalar a ferramenta. Em seguida, siga 
os seguintes passos (ANDERSON; JUELL, 2020): 

1. Adicione a chave do repositório ao sistema: 
$ wget -q -O -https://pkg.jenkins.io/debian/jenkins.io.key | 

sudo apt-key add -

2. O sistema retornará OK quando ela for adicionada. Depois disso, adi-
cione o endereço do repositório de pacotes Debian ao sources. 
list do servidor: 
$ sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable 
binary/ > /etc/apt/sources.list.d/jenkins.list' 

3. Quando os pacotes estiverem funcionando, execute update para que 
o apt utilize o novo repositório: 
$ sudo apt update 

4. Por fim, instale o Jenkins e suas dependências: 
$ sudo apt install jenkins 



v)[ Fundamentos do Jenkins 

Agora que a ferramenta e as dependências estão funcionando, inicie o 
servidor do Jenkins. Para isso, siga os seguintes passos (ANDERSON; JUELL, 
2020): 

1. O Jenkins deve ser iniciado utilizando o systemct1: 
$ sudo systemctl start jenkins 

2. Como ele não mostra os resultados, o comando status pode ser 
utilizado para checar se o Jenkins foi iniciado com sucesso: 
S$ sudo systemctl status jenkins 

3. Se está tudo bem, o início do resultado deve mostrar que o serviço está 
ativo e configurado para iniciar quando for inicializado: 
Output 

* jenkins.service -LSB: Start Jenkins at boot time 
Loaded: loaded (/etc/init.d/jenkins; generated) 

Active: active (exited) since Thu 2020-07-09 10:13:37 UTC; 
10min ago 
Docs: man:systemd-sysv-generator(8 
Tasks: O (limit: 1153) 
CGroup: /system.slice/jenkins.service 

Agora que a ferramenta está funcionando, é hora de ajustar as regras do 
firewall, para que seja possível ter acesso a um navegador web para terminar 
a configuração inicial. Para isso, siga os seguintes passos (ANDERSON; 
JUELL, 2020): 

1. O Jenkins funciona na porta 8080 por padrão. Então, é preciso abri-la: 
S sudo ufw allow 8080 



Fundamentos do Jenkins | (179 

2. Verifique o status do ufw para confirmar as novas regras: 
$ sudo ufw status 

3. Aparecerá a informação dizendo que o tráfego é permitido nessa porta 
de qualquer lugar: 
Output 

Status: active 

To Action From 

OpenssH ALLOW Anywhere 
8080 ALLOW Anywhere 
OpenssH (v6) ALLOW Anywhere (v6) 
8080 (v6) ALLOW Anywhere (v6) 

Fique atento 

Se o firewall não estiver ativo, os comandos a seguir permitirão o OpenSSH e habilitarão 

o firewall: 
S$ sudo ufw allow OpensSSH 
$ sudo ufw enable 

Após a instalação da ferramenta e a configuração do firewall, siga os 
seguintes passos para finalizar a configuração (ANDERSON; JUELL, 2020): 

1. Visite o IPdoSeuServidor ou domínio 8080 utilizando o nome do do-
mínio ou o IP do seu servidor. Aparecerá uma tela de Unlock Jenkins, 
que apresentará o local da senha inicial, conforme a Figura 2. 

180) ( Fundamentos do Jenkins 
Pa 

Getting Started 

Unlock Jenkins 

e Jenki et up x, à password has been y 
tothe log (no ) and this fileon the s 
[war/Nib/ Jenkins secrets /initislhdainPassmord 
Please opy the password from either location and pe 
Amro porem 

Figura 2. Tela Unlock Jenkins. 

Fonte: Anderson e Juell (2020, documento on-line). 

2. No terminal, utilize o comando cat para mostrar a senha: 
$ sudo cat /var/lib/jenkins/secrets/initialAdminPassword 

3. Copie a senha alfanumérica de 32 caracteres do terminal, utilize-a no 
campo Administrator password e clique para continuar. 
4. Em seguida, aparecerá uma tela com opções. Clique na opção Install 
suggested plugins e será iniciada a instalação de forma automática. 
Concluída a instalação, aparecerá uma requisição para fazer a configu-
ração do primeiro usuário administrativo. É possível ignorar essa etapa 
somente utilizando admin e a senha utilizada anteriormente. Para criar 
um novo usuário, coloque o nome e a senha desejados para o seu usuário. 
Em seguida, aparecerá a tela Instance configuration, na qual será con-
firmado o URL preferido para a instância do Jenkins. Informe o nome 
do domínio para o seu servidor ou o IP dele e, depois, clique em Save 
and finish. Em seguida, aparecerá uma página contendo Jenkins is 
ready confirmando a configuração. 
7. Para visitar o painel e começar a usar a ferramenta, é só clicar em Start 
using Jenkins. 

Fundamentos do Jenkins ) ( 181 

Pipeline para entrega contínua 

O termo pipeline, no contexto do Jenkins, refere-se a um grupo de eventos 
ou jobs que são interligados em uma sequência, conforme a Figura 3. Basi-
camente, uma combinação de plugins no Jenkins dá suporte à integração e 
à implementação de pipelines de CD. Toda mudança do software passa por 
um processo complexo até a sua liberação (jenkins.io), o que envolve a sua 
construção com confiança e com repetições, bem como a evolução do build 
através de vários estágios de testes e de deploy. 

Mr 5 

de Build 

Agile Development

EO 

Continuous Integration 

Continuous Delivery 

Continuous Deployment 

Devops 

Figura 3. Mapa de fluxo de valor. 

Fonte: Ballot (2019, documento on-line). 
A J 

Imagine a seguinte situação: você está desenvolvendo uma pequena apli-
cação no Jenkins e deseja a compilar, testar e implantar. Para fazer isso, você 
precisará alocar três jobs no Jenkins para executar cada etapa. Dessa forma, 

o jobl criará o build, o job2 executará os testes e o job3 fará o deploy 
da aplicação. Você pode usar um plugin para executar essa tarefa, e, após a 
criação dos três jobs, eles serão colocados em sequência e serão executados 
como um pipeline. 

182 ) (Fundamentos do Jenkins 
Essa estratégia é eficaz para pequenas aplicações, pois o pipeline pode se 
tornar muito complexo se for preciso executar vários outros processos, além 
desses três desejados. Dessa forma, o custo de manutenção seria muito alto, 
e seria trabalhoso criar tantos jobs, bem como fazer os seus gerenciamentos. 
Nesse contexto, foi introduzido um recurso chamado de Jenkins Pipeline Project 
(LATEEF, 2020), com o intuito de definir todo o fluxo de implantação através 
de codificação. Todos os jobs que são definidos como padrão no Jenkins são 
gravados de forma manual como um script e armazenados em um sistema de 
controle de versionamento local (GURU99, c2020). Desse modo, a definição 
de um pipeline é gravada em um arquivo de texto chamado jenkinsfile, o qual 
pode ser acessado, editado e ter o seu código verificado a qualquer momento 
pelos desenvolvedores. 

É possível utilizar um editor de texto, um editor de groovy ou a página 
de configuração no Jenkins para criar o jenkinsfile. Além disso, utiliza-se a 
linguagem específica de domínio do pipeline, que é o Groovy DSL. A cria-
ção desse arquivo pode trazer diversos benefícios imediatos para o projeto. 
O jenkinsfile é escrito com base em duas sintaxes: declarativa e com script. 
Esses dois tipos são construídos de formas diferentes. 

O pipeline declarativo é uma funcionalidade relativamente nova, desen-
volvida para facilitar a leitura e a escrita do código de pipeline, que é escrito 
em um jenkinsfile, que pode ser checado em um SCM. Além disso, ele busca 
oferecer uma sintaxe mais simplificada e diversificada do Groovy. Esse tipo 
de pipeline é definido dentro de um bloco pipeline. 

Em contrapartida, o pipeline com script é o modo tradicional, em que o 
jenkinsfile é gravado na interface da instância do Jenkins. Além disso, ele 
utiliza a sintaxe de Groovy mais rigorosa, pois foi o primeiro a ser construído. 
Esse tipo de pipeline é definido dentro de um bloco node. 

A adoção de pipeline no Jenkins pode trazer diversas vantagens para o 
projeto, como as listadas a seguir (GURU99, c2020; LATEEF, 2020): 

E ele é implementado como um código que permite que vários usuários 
editem e executem o processo do pipeline; 
E se o servidor reiniciar bruscamente, o pipeline será reiniciado 
automaticamente; 
E ele suporta grandes projetos e pode executar vários jobs e até usar 
pipelines em um /o0p com condições, operações com /fork ou join, 
permitindo que tarefas sejam executadas em paralelo; 



Fundamentos do Jenkins ) (183 

E o processo de pipeline pode ser pausado e aguardar a retomada até que 
haja uma entrada do usuário; 
m melhora a interface do usuário, incorporando a entrada do usuário no 
pipeline; 
pode reiniciar a partir de pontos de verificação salvos; 
m pode ser integrado com vários outros plugins. 

Conceitos fundamentais 

Para ter uma melhor compreensão do funcionamento do pipeline no Jenkins, 

faz-se necessário, primeiro, compreender alguns conceitos importantes, que 

estão relacionados à sintaxe do pipeline (jenkins.io): 

E Pipeline: é um modelo definido pelo usuário de um pipeline de CD. 
O código de um pipeline define todo o processo de criação, que geral-
mente inclui stages (etapas) para a criação de uma aplicação e, depois, 
realizar os testes e a entrega. Além disso, é uma parte essencial da 
sintaxe do pipeline declarativo: 

pipeline ( 
) 

E Node (nó): é uma máquina que faz parte do ambiente Jenkins, a qual 
é capaz de executar um pipeline. Além disso, é uma parte essencial da 
sintaxe do pipeline de script: 

node ( 

E Stage (estágio): esse bloco define uma série de steps (etapas) em um 
pipeline. Por exemplo, Build, Test e Deploy se reúnem em um stage. 
Em geral, ele é utilizado por muitos plugins para visualizar ou apresentar 

o status do pipeline. 
E Step (etapa): refere-se a uma única tarefa que executa um processo 
específico em um horário que definido. 



184) [ Fundamentos do Jenkins 
Configurar pipeline no Jenkins 

A seguir, será demonstrado como implementar um pipeline no Jenkins. 
O primeiro passo a ser executado é instalar os plugins Build Pipeline e Build 
Pipeline View. Você pode verificar se já tem esses plugins instalados em 
Manage Jenkins > Manage Plugins. Caso já tenha, eles aparecerão na aba 
Installed. Em caso negativo, eles aparecerão na aba Available para serem 
baixados e instalados. 

Após a instalação do plugin, siga os seguintes passos (GURU99, c2020): 

. Primeiro, para criar um pipeline, clique no botão + no canto esquerdo 
do painel do Jenkins. 
Em seguida, informe um nome para ele no campo View name. 
Nas opções que aparecem abaixo desse campo, selecione Build pipeline 
view e clique no botão OK. 
Na próxima tela, informe mais detalhes para configurar o pipeline. Por 
enquanto, você só precisa aceitar a configuração default e escolher o 
primeiro job no campo Select initial job. Após isso, clique no botão 
Apply e, depois, em OK. Feito isso, o Jenkins apresentará uma tela 
mostrando o pipeline do seu item. 
Para executar o pipeline, primeiro coloque os jobs em uma sequência. 
Para isso, selecione o job criado no passo 4 e clique em Configure. 
Em seguida, na seção Build triggers, marque o checkbox da opção Build 
after other projects are built e selecione o projeto desejado no campo 
Projects to watch. Dessa forma, sua sequência de jobs será criada. 
No painel do Jenkins, crie uma view clicando no botão +, selecione a 
opção Build pipeline view e clique em OK. Na seção Pipeline view 
configuration, em Pipeline flow, selecione o primeiro job a ser execu-
tado. Agora, selecione o job que está ligado com outros jobs, conforme 
mostrado na etapa 5. Dessa forma, você selecionará os jobs que serão 
executados no pipeline um por um. Quando o pipeline está sendo exe-
cutado, é possível checar o status dele, em que: 
= vermelho: significa que o pipeline falou; 
= verde: significa que o pipeline foi bem-sucedido. 

7. Para executar o pipeline, basta clicar no botão Run. 

Fundamentos do Jenkins ) ( 185 

Saiba mais 

Para saber mais sobre a implementação de pipelines declarativo e com script, acesse 
a documentação oficial, disponível no site Jenkins. 

3 Plugins 

Os plugins são a principal forma de aprimorar as funcionalidades de um 
ambiente Jenkins para atender a necessidades específicas. Atualmente, exis-
tem mais de 1.500 plugins que podem ser instalados, como ferramentas para 
auxiliar a análise de dados, provedores de nuvem, notificações, ferramentas 
de controle de versão, métricas de qualidade, personalização de interface, 
jogos, entre outros. 

Pode-se fazer o download dos plugins do Jenkins de forma automática com 
suas dependências no Update Center, um serviço que fornece uma lista de 
plugins open source que foram desenvolvidos e mantidos pela comunidade. 
Existe um grande volume de informações disponíveis sobre plugins, mas 
alguns pontos-chave são listados a seguir (BERG, 2012): 

Bm Existem muitos plugins e muito outros serão desenvolvidos. Para 
acompanhar essas alterações, faz-se necessário monitorar a seção de 
gerenciador de plugins no Jenkins. 
m Se você commitar suas melhorias, elas se tornarão visíveis para muita 
gente. Com isso, muito provavelmente, seu código será aprimorado, 
pois a comunidade do Jenkins é bem cuidadosa. 
Bm No geral, é mais fácil adaptar um plugin existente para uma finalidade 
do que escrever um do zero. 
m A maioria dos fluxos de trabalho de plugins é de fácil entendimento, 
porém, à medida que cresce o número de plugins que estão sendo utili-
zados, aumenta a possibilidade de ter erros inesperados de configuração. 
m Ao manter as convenções do Jenkins, a quantidade de código-fonte que 
você escreve é minimizada, e a legibilidade, aprimorada. 

Coma grande diversidade de plugins disponíveis para o Jenkins, é possível 
implantar quantos se desejar, com o intuito de aumentar a produtividade e 
tornar a ferramenta ainda mais útil. 



186 ) ( Fundamentos do Jenkins 
Plugins do Jenkins 

A seguir, são apresentados alguns dos principais plugins do Jenkins disponíveis 
e suas funcionalidades (TORRE, 2018). 

E Dashbord View (painel de visualização): esse plugin fornece um 
novo painel na ferramenta para monitorar o status de todos os jobs em 
execução. Além disso, ele auxilia o rastreamento do tempo de duração de 
cada trabalho e a execução do tempo, ambos fundamentais para apoiar 

o gerenciamento. Por meio desse plugin, é possível gerenciar builds e 
mostrar os seus resultados e testes, além do fato de que os usuários 
podem organizar os itens de exibição de forma eficaz. 
E View Job Filters (visualizador de filtros de jobs): esse plugin auxilia 
na criação de diferentes visualizações para os jobs. Ele fornece formas de 
incluir/excluir jobs automaticamente, utilizando elementos como status 
do build, tipo de job, permissões de usuários, entre outros. Com esse 
plugin, é possível combinar diversos filtros com o intuito de estreitar 
os resultados. 

mB Folders (pastas): esse plugin ajuda no agrupamento dos jobs no Jenkins 
ao permitir que os usuários criem pastas. Os usuários podem definir 
taxonomias personalizadas, como por tipo de projeto ou de organização. 
As pastas são aninhadas e é possível definir visualizações dentro delas. 
É um plugin que tem sido muito baixado. 

Plugins de monitoramento 

A seguir, são apresentados os principais plugins de monitoramento e suas 
funcionalidades. 

E Monitoring (monitoramento): esse plugin monitora o desempenho 
do Jenkins com o JavaMelody. Com ele, é possível visualizar gráficos 
sobre a memória, CPU, carga do sistema e estatísticas de solicitações 
http. Além disso, é possível ter acesso a erros e /ogs, relatórios e lista 
de processos do sistema operacional. 



Fundamentos do Jenkins ) (187 

E Metrics (métricas): esse plugin fornece “verificações de saúde”, 
expondo a API do Dropwizard, com o intuito de obter métricas para 
mantê-lo informado sobre o que está acontecendo em tempo real, retor-
nando um status de PASS/FAIL. Com ele, é possível verificar o status 
do espaço em disco, os plugins (p. ex., se eles falharam ao iniciar), 

o espaço temporário no Jenkins, o deadlock e diferentes métricas, como 
histograma e timer. 
Plugins aperfeiçoadores de desempenho 

A seguir, são apresentados os principais plugins aperfeiçoadores de desem-
penho e suas funcionalidades. 

E Performance (desempenho): com esse plugin, é possível gerar relatórios 
de ferramentas de testes populares, como JMeter, Gatling, Selenium, 
Junit, Taurus, entre outros. Esse plugin gera gráficos a partir de relatórios 
de tendências de desempenho e robustez. Com base nisso, ele define o 
status da build como correta, instável ou falha. 

m Performance Publisher (editor de desempenho): com esse plugin, 
é possível gerar relatórios globais e de tendência para analisar os resul-
tados dos testes com todas as ferramentas de teste. Ele faz a análise dos 
arquivos gerados e gera estatísticas, relatórios e análise sobre a saúde 
do projeto a partir desses arquivos, destacando regressões e modifica-
ções. Entre outros recursos que podem ser encontrados nesse plugin, 
destacam-se: definição de limite instável quando atingir um número 
de teste com falhas, ponto de análise múltipla, tempo de compilação 
e execução. 

Plugins para lidar com a escalabilidade do Jenkins 

A seguir, são apresentados os principais plugins para lidar com a escalabilidade 
do Jenkins e suas funcionalidades. 

m Kubernetes: a partir desse plugin, é possível usar o Kubernetes para a 
infraestrutura, embora o processo não seja tão simples assim. O plugin 
cria um pod do Kubernetes para cada agente iniciado, definido pela 
imagem do Docker para ser executado, e, depois, o interrompe após 
cada build. 



188) ( Fundamentos do Jenkins 
m Swarm: esse plugin oferece apenas um método para ativar e desativar 
os slaves do Jenkins. Ele permite que os nós se juntem a um master pró-
ximo, formando um cluster ad-hoc. Além disso, esse plugin torna mais 
fácil para escalar um cluster Jenkins, criando e derrubando novos nós. 

E Amazon Elastic Container Service (ECS)/Fargate (serviço de con-
têiner da Amazon): esse plugin hospeda a execução de jobs dentro de 
contêineres do Docker. O Jenkins delega ao Amazon ECS a execução das 
compilações nos agentes baseados no Docker. Cada build do Jenkins é 
executado em um contêiner Docker dedicado, que é destruído no final. 

Plugins de análise de testes 

A seguir, são apresentados os principais plugins de análise de testes e suas 
funcionalidades. 

m Test Results Analyzer (analisador de resultados de teste): esse plugin 
oferece maior visibilidade, mostrando o histórico dos testes de um 
build. Por meio dele, é possível filtrar esses resultados com base nos 
status pass, fail e ignored nos testes. Esse plugin é bem simples de 
ser utilizado e fornece vários tipos de representações gráficas, como 
gráficos de linha, de pizza e barra. 

EB bootstrapped-multi-test-result-report (relatório de testes): esse 
plugin possibilita a geração de um relatório HTML dos resultados 
de um teste automatizado em diversas ferramentas, como Cucumber, 
TestNG, Junit e RSpec. Com ele, é possível gerar relatórios interativos 
para se ter uma ideia geral de todos os resultados e detalhes dos status 
de cada etapa. 

Plugins de pipeline/fluxo 

A seguir, são apresentados os principais plugins de pipeline/fluxo e suas 
funcionalidades. 

= Build Pipeline (pipeline da build): esse plugin fornece uma visua-
lização dos jobs que compõem a build, a upstream e a downstream. 
Com ele, é possível definir trigger manuais para jobs que precisam de 



Fundamentos do Jenkins ) 189 

intervenção antes de sua execução. Esse plugin é uma mão na roda para 
os usuários do Jenkins, pois torna possível usar scripts nos pipelines 
e possibilita o desenvolvimento de pipelines complexos de DevOps 
com várias etapas. 

E Pipeline: esse plugin possibilita a automação do pipeline de CD, exe-
cutando tarefas simples e complexas. Além disso, ele utiliza outros 
plugins tradicionais do Jenkins. 

Plugins de gerenciador de controle de versão (SCM) 

A seguir, são apresentados os principais plugins de gerenciador de controle 
de versão (SCM) e suas funcionalidades. 

m SCM API (API SCM): esse plugin fornece uma API inovadora para 
interagir com sistemas SCM, permitindo notificações, em que é possível 
mostrar alertas aos clientes. 

EB Git: com esse plugin, é possível ter acesso a operações fundamentais 
do Git para projetos Jenkins. Além disso, ele atua como um navegador 
de repositório para muitos provedores. 

E GitHub Integration (integração do GitHub): esse plugin é fun-
damental para integrar os projetos Jenkins aos do GitHub. Com ele, 
é possível agendar o build, submeter o pull do arquivo de código e de 
dados do repositório GitHub e acionar automaticamente cada build 
quando necessário. 

Com base no que foi exposto, percebe-se que a ferramenta Jenkins traz 
grandes ganhos ao processo de desenvolvimento e à equipe responsável por 
ele, pois possibilita a automação de atividades que antes eram feitas de forma 
repetitiva e exaustiva pelos desenvolvedores. Com o Jenkins, é possível criar 
builds, executar testes automatizados, configurar alertas, deploy automá-
tico, além de muitas outras atividades de forma automatizada. Além disso, 
é possível encontrar mais de 1.500 plugins disponíveis para baixar/instalar no 
Jenkins, com o intuito de aumentar suas funcionalidades e auxiliar na solução 
de problemas. Desse modo, o Jenkins se tornou a ferramenta mais utilizada 
para isso, pois, além das funcionalidades aqui citadas, é possível fazer a sua 
integração com quase todas as ferramentas disponíveis para CI/CD. 



190) ( Fundamentos do Jenkins 
Referências 

ANDERSON, M.; JUELL, K. Como instalarJenkins no Ubuntu 18.04. New York: Digita-

o 
Ocean, 2020. Disponível em: https://wwwdigitalocean.com/community/tutorials/ 
how-to-install-jenkins-on-ubuntu-18-04-pt. Acesso em: 8 jun. 2020. 

BALLOT, J. Criando pipelines no Jenkins com Jenkinsfile. São Paulo: 4Linux, 2019. Dispo-
nível em: https://blog.4linux.com.br/criando-pipelines-no-jenkins-com-jenkinsfile/. 
Acesso em: 27 jul. 2020. 

BERG, A. M. Jenkins continuous integration cookbook. Birmingham: Packt, 2012. 

BOAGLIO, F. Jenkins: automatize tudo sem complicações. São Paulo: Casa do Código, 
2016. 

DUVALL, P. M.; MATYAS, S.; GLOVER, A. Continuous integration: improving software 
quality and reducing risk. Boston: Addison-Wesley, 2007. 

ELLINGWOOD, ). Initial server setup with Ubuntu 18.04. New York: DigitalOcean, 2018. 
Disponível em: https://www digitalocean.com/community/tutorials/initial-server-
-setup-with-ubuntu-18-04. Acesso em: 8 jun. 2020. 

FOWLER, M. Continuous integration. Chicago, 2006. Disponível em: https://martinfowler. 
comyarticles/continuousintegration.html. Acesso em: 24 jul. 2020. 

GURU99. Jenkins pipeline tutorial: JenkinsFile example. [S. /], 2020. Disponível em: https:// 
www.guru99.com/jenkins-pipeline-tutorial.html. Acesso em: 10 jul. 2020. 

LATEEF, Z. Jenkins pipeline tutorial: a beginner's guide to continuous delivery. Bengaluru: 

Eureka!, 2020. Disponível em: https:/Awww.edureka.co/blog/jenkins-pipeline-tutorial-

-continuous-delivery. Acesso em: 10 jul. 2020. 

SMART,J. F. Jenkins:the definitive guide: continuous integration for the masses. Boston: 
O'Reilly Media, 2011. 

SONI, M. Jenkins essentials. Birmingham: Packt, 2015. 

TORRE, J. P. La. 20 Jenkins plugins you can't live without. Irvine: Caylent, 2018. Disponível 
em: https://caylent.com/jenkins-plugins. Acesso em: 7 jul. 2020. 

VLASWINKEL, K. How to install Java with Apt on Ubuntu 18.04. New York: DigitalOcean, 
2020. Disponível em: https://www-.digitalocean.com/community/tutorials/how-to-
-install-java-with-apt-on-ubuntu-18-04ginstalling-specific-versions-of-openjdk. Acesso 
em: 8 jun. 2020. 


Fundamentos do Jentins ) ( 191 

Leituras recomendadas 

LASTER, B. Jenkins 2: up and running: evolve your deployment pipeline for nextgene-
ration automation. Boston: O'Reilly Media, 2018. 
LESZKO, R. Continuous delivery with Docker and Jenkins. Birmingham: Packt, 2017. 
MULI, J; OKOTH, A. Jenkins fundamentals: accelerate deliverables, manage builds, and 

Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-

cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 

d 


Esta página foi deixada em branco intencionalmente. 



Pipeline de integração 
e entrega contínuas 

Va Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

m Descrever a criação de um projeto de integração contínua vinculado 
ao repositório Git de um projeto simples. 
E Interpretar os gráficos e /ogs de integração contínua do Jenkins como 
ferramenta de diagnóstico. 
m Demonstrar a programação de execução de um job por meio do 
trigger build. 

Introdução EN 

Neste capítulo, você vai ver como integrar um projeto a um repositório 
Git, a fim de manter a organização, a rastreabilidade e a recuperabilidade 
do código do projeto. Além disso, você vai verificar como a ferramenta 
Jenkins pode auxiliá-lo no monitoramento do trabalho. Por fim, vai ver 
como agendar atividades ou grupos de atividades para serem executadas 
periodicamente. 

1 Criação de um projeto 

Um dos pontos fundamentais para o bom desenvolvimento de um projeto é a 
sua organização. A equipe de desenvolvedores pode e deve utilizar ferramen-
tas que a auxiliem nessa empreitada, a fim de reduzir trabalhos cansativos, 
repetitivos e enfadonhos. O primeiro aspecto que você deve considerar é a 
simples utilização de um método de versionamento do código-fonte, para ter 
algumas garantias da qualidade do projeto. Contudo, ainda assim você pode 
enfrentar alguns desgastes no processo de trabalho. 



194) [ Pipelme de imegração e entrega contínuas 
Apesar de as práticas de DevOps não serem uma regra para a maioria das 
empresas de desenvolvimento, a integração contínua do software certifica 
que toda alteração realizada pelos programadores ative um build do projeto, 
objetivando reconhecer os impactos causados pelas alterações realizadas 
(DEBROY; MILLER; BRIMBLE, 2018). Para automatizar todo esse processo, 
é preciso escolher algum serviço que implemente o protocolo Git. A maioria 
dos serviços já possui todas as ferramentas necessárias para a aplicação desse 
processo. A seguir, você vai ver como criar o seu projeto. 

A Figura 1 mostra um formulário para a criação de um projeto no GitLab, 
um serviço que pode ser utilizado gratuitamente. Nesse formulário, você 
deve adicionar o nome do projeto e uma descrição para ele (opcional), além 
de selecionar a visibilidade do projeto, que pode ser público ou privado. Se 
você optar por um projeto privado, apenas as pessoas com permissão poderão 
interagir com o conteúdo, e tal interação dependerá do tipo de permissão 
concedida. No caso de um projeto público, diversas pessoas poderão visualizar 
e interagir com o conteúdo (essa opção é muito utilizada para software livre). 

Em um projeto público, as alterações no código-fonte são realizadas apenas 
com a aprovação dos gerentes do projeto. Todavia, é possível que usuários 
clonem o repositório e continuem, paralelamente, o desenvolvimento do mesmo 
software sob outra perspectiva. 

devops 

Project URL Project slug 
htpsilgRlab.com/ — marcosbiao devops 
pace? Creategroup.

a 
Project description (optional) 

Visibility Level 

O Bra 

Opubic 

Egintiatize repository

with a README

menediateyclone this projects repository Skip this you plan to push vp an existing reposton 

Figura 1. Criação de um projeto. 

Fonte: GitLab (2020, documento on-line). 


Pipeline de integração e entrega cont nuas) (195 

Após a criação do projeto e a escolha da visibilidade, você pode adicionar 
os colaboradores. Você pode adicionar membros já presentes na plataforma 
ou enviar convites para que outros desenvolvedores se cadastrem nela. Você 
também deve adicionar o tipo de permissão que o colaborador terá. 

Depois de adicionar os desenvolvedores que participarão do projeto e 
de fazer o planejamento, você pode dividir o trabalho por meio das issues 
(em uma tradução literal, “questões”). Ao criar uma issue, descreva a ativi-
dade que será realizada. Quanto melhor for a descrição, mais fácil será para 

o programador desenvolver o seu trabalho. Nesse momento, você também 
pode cadastrar o responsável por aquela atividade, os labels (cada projeto tem 
a sua forma de organização) e a data limite para a entrega. Uma boa prática 
que todos os envolvidos no projeto devem realizar é utilizar o espaço da issue 
para esclarecer dúvidas sobre a atividade específica, além de discutir outras 
orientações, como imagens e exemplos. 
A branch principal do projeto, a master, não deve sofrer qualquer alteração 
diretamente. Sempre é necessário que uma nova branch seja criada pelo de-
senvolvedor. É comum que os programadores clonem a master, realizem seus 
trabalhos em suas estações, criem uma branch aleatória e realizem o push do 
projeto. As boas práticas pedem que a nova branch seja criada com base na 
issue, assim a rastreabilidade é realizada de forma mais adequada. Logo em 
seguida, o sistema deve realizar o build automático do projeto, verificar os 
conflitos existentes e esperar a autorização do gerente, que realiza o merge 
da branch para a master. 

Outra boa prática é realizar um pull do projeto antes do push. Assim, você 
identifica os conflitos existentes e faz as modificações necessárias antes 
de subir o projeto para o servidor. Quando não existem mais conflitos e o 
trabalho realizado está dentro dos padrões esperados, você deve incorporar 
as modificações à branch master e excluir aquela branch. 

No GitLab, toda a configuração do build é realizada por meio de um 
arquivo chamado “gitlab-ciyml”. Esse arquivo utiliza a sintaxe de configu-
ração do YAML Ain't Markup Language (YAML) para definir as ações que 
devem ser realizadas e em qual ordem elas acontecerão. As ferramentas de 
build devem oferecer suporte à atividades rápidas para melhorar o fluxo de 
trabalho (EBERT, 2016). 



196) (Pipetme de integração e entrega contínuas 
Em linhas gerais, aqui você vai ver como construir um script para a compi-
lação do projeto, que varia de acordo com a linguagem que o projeto emprega 
(PLETSCH; PORTO, 2017). Além disso, você deve preparar as permissões 
do arquivo gerado para permitir a sua execução. Você pode adicionar testes 
automatizados que verificarão aspectos que não são vistos por meio da ob-
servação direta do código. 

Para configurar o arquivo gitlab-ciyml, inicialmente, você deve criá-lo na 

pasta raiz do projeto. Assim, o GitLab consegue identificar o arquivo e realizar 

os procedimentos. Esse arquivo deve conter as informações elencadas a seguir. 

E Imagem: a ideia é especificar uma imagem no Docker e uma lista de 
serviços que podem ser utilizados. É possível baixar uma máquina 
virtual já configurada para rodar a aplicação. 

EB Stages: estágios que podem ser usados por jobs ou definições globais. 
Dessa forma, você pode configurar o estágio de testes, por exemplo. 
Script: comandos em shell que serão executados por um job. 
Artifacts: lista de arquivos e diretórios que devem ser anexados ao fim 
do processo. 

E Paths: caminho que vai armazenar o resultado do processo. 

Fique atento 

As configurações variam de acordo com o sistema que está sendo desenvolvido, por 
exemplo, linguagens ou servidor. Assim, o arquivo gitlab-ciyml será único para cada 
projeto. É importante você lembrar de “commitar” suas alterações para que elas fiquem 
salvas em seu repositório. 

2 Gráficos e logs 

Agora que você já viu como criar um repositório no GitLab e já verificou qual 
é a configuração necessária para um processo de integração contínua, vai ver 
como são apresentados os Jogs do Jenkins. Essa é outra ferramenta bastante 
utilizada em processos de integração contínua. 



Pipeline de integração e entrega continuas ) ( 197 

Gestores e colaboradores devem estar atentos a tudo o que se relaciona 
ao projeto, seja interna ou externamente. Uma das melhores maneiras de 
fazer isso é analisar os resultados gerados nos processos que são executados 
automaticamente. Considerando que os procedimentos são realizados de 
forma sistemática e que os testes são feitos com o intuito de evidenciar erros 
no sistema, a análise dos resultados gerados é importante para auxiliar os 
gestores nos encaminhamentos de novas ações. 

O Jenkins é uma ferramenta muito utilizada para a automação de pro-
cedimentos. Ele também pode monitorar as mudanças realizadas no código 
(LIRA; ZANONI; TALON, 2016). O rastreamento do processo de software do 
produto final entregue pode ser realizado por esse sistema. Ao visitar sua tela 
inicial, você pode ver uma lista de todos os trabalhos que foram configurados 
anteriormente. 

Assim, você pode obter um feedback rápido dos números relacionados aos 
testes realizados. A seguir, veja um exemplo em formato gráfico: a Figura 2 
mostra a quantidade de testes executados (em verde), de testes que obtiveram 
sucesso (em amarelo) e de testes que falharam (em vermelho). Mudanças 
bruscas no gráfico são indicadores de problemas no sistema em construção 
ou no ambiente de teste. 

TE.EK! 
A 
Figura 2. Gráfico com análise das 
Fonte: Lima (2017, documento on-line), 
execuções de testes. 
-, 


158) (ipeine de integração e entrega contínuas 
Existe também a possibilidade de adicionar plug-ins ao Jenkins para analisar 
informações de um modo diferente. Um exemplo é a utilização do JUnit que 
consome dados em XML. Assim, você pode gerar uma tabela que mostra 
todos os testes configurados relacionados às builds do sistema. Essa tabela 
informa sobre o sucesso ou a falha do teste (passed ou failed). 

A partir dessa tabela (Figura 3), você pode fazer uma análise mais minuciosa 
de cada teste, o que não é possível por meio do gráfico anterior (Figura 2). 
Com base na tabela, você consegue verificar se o teste sempre é realizado com 

sucesso, se ele começou a falhar a partir de determinada build ou se existe 
alguma intermitência (ora o teste é realizado com sucesso, ora ele falha). 

4 N 
Lp | | Denon tt SM) | semi tomo 

Figura 3. Tabela gerada pelo JUnit. 

Fonte: [Miro Medium] ((201-7), documento on-line) 
A E, 


Pypelme de integração e entrega contínuas ) [199 

Sem a utilização dessa tabela, seria necessário acessar cada build individu-
almente e verificar quais testes obtiveram sucesso e quais falharam. Um bom 
truque para descobrir o porquê de uma falha é usar a visualização de tabela 
para verificar a hora em que o teste foi aprovado pela última vez e comparar 
a versão aprovada com o resultado da execução com falha. Dessa forma, 
a análise será mais rápida e precisa. 

Uma função nativa do Jenkins é o arquivamento de artefatos. Esse recurso 
permite anexar a cada build qualquer dado gerado durante o processo de exe-
cução dos testes. Dessa forma, você pode guardar gráficos, prints das telas, 
arquivos gerados pelo sistema, etc. Essa função é importante para realizar 
análises de resultados atuais com builds de meses anteriores (SOUZA, 2014; 
LIMA, 2017). 

3 Trigger build 

Um recurso muito comum em diversos projetos é o agendamento de execuções 
automáticas, também conhecidas como “trigger builds”. Você pode progra-
mar uma tarefa para acontecer a qualquer horário do dia (ANSCHAU, 2019). 
Por exemplo, você pode agendar um pacote de testes para ser realizado às 22h; 
assim, ao chegar pela manhã, vocêjá poderá analisar os resultados desse job. 

Em algumas situações, um teste automatizado será executado após o deploy. 
Ele fará parte do pipeline de execução já configurado. Contudo, não são apenas 
testes que se pode automatizar. Na verdade, é possível automatizar qualquer 
ação dentro do projeto que se queira executar todos os dias no mesmo horá 

A ideia por trás dessa técnica é permitir que o trabalho se torne mais fluido. 
Assim, não é necessário lembrar de realizar essas atividades: você padroniza 
a execução da tarefa e obtém resultados mais tabuláveis. 

É possível realizar de forma simples a integração do Jenkins com o GitLab 
ou outro serviço de gerenciamento. Para isso, é necessário que você utilize 
as configurações do GitLab para gerar uma chave de acesso. Siga então para 

o Jenkins a fim de incorporar a chave gerada anteriormente. Dessa forma, 
você pode criar as atividades necessárias para que elas sejam executadas no 
repositório. 

200 ) (pipeine de integração e entrega contínuas 
Ao criar um novo job, você pode configurar o trigger build para ser realizado 

em diversos momentos, como: 

EB periodicamente; 
E quando acontecer um merge; 
E quando surgir um evento; 
E após um build de outro projeto. 

Na Figura 4, a seguir, veja a tela de configurações. 

Trigger de builds. 

Dire bl emotameo (enempo, a pat dos sera) 
Construir aços a construção do cur pros 
Construir periodicamente 

B Bus ven a change is push Gt a GL CI Service URL Ipccaost SOB prjacTasteDe

Emas 
Enatied Gatab tiger push Events 
Merge Reqesteves O 
Rebuld open Merge Foquosts Never 
Commerts 
Commenttor rec a bus lesse rey a puts

Jenkins º 

Figura 4. Configurações do trigger build. 
Fonte: GitLab (2020, documento on-line) 

É importante realizar as configurações de pós-build para que os resultados 
sejam publicados nos commits ou nos merges que chamaram o job. Para tal, 
é necessário marcar a opção “Publish build status to GitLab commit”. Dessa 
forma, sempre que possível, você terá os resultados postados diretamente 
nos comentários. 

Por fim, é necessário adicionar o Webhook ao repositório do GitLab. 
Se qualquer alteração acontecer, as informações necessárias serão enviadas 
ao Jenkins. Esse recurso pode ser encontrado nas configurações do seu pro-
jeto no GitLab. Nele, configure as informações que serão reportadas; assim, 

o Jenkins poderá realizar seus procedimentos. Na Figura 5, a seguir, confira 
esse processo. 
seeos 


Pipeline de integração e entrega contínuas ) (zm 

" 

Figura 5. Webhook. 
Fonte: GitLab (2020, documento on-line) 

N J 

Ao finalizar todas essas configurações, você atinge um estágio no qual 
grande parte das ações pode ser automatizada. Com isso, seu projeto ganha 
muito mais agilidade e confiabilidade. Além disso, os colaboradores tendem 
a apresentar mais disposição para trabalhar, já que não precisam mais realizar 
diversos procedimentos que até então eram realizados manualmente. 

Referências 

ANSCHAU,M. H. Sistema de verificação automática de testes no desenvolvimentode firmwa-
res de inversores de frequência. 2019. 86 f. Monografia (Graduação) -Universidade Federal 
de Santa Catarina, Engenharia de Controle e Automação, Blumenau, 2019. Disponível 
em: https://repositorio.ufsc.br/bitstream/handle/123456789/204114/TCC. Martin. 2020. 
pdf?sequence=3&isAllowed=y. Acesso em: 3 out. 2020. 

DEBROY, V; MILLER, S.; BRIMBLE, L. Building lean continuous integration and delivery 
pipelines by applying DevOps principles: a case study at Varidesk. In: EUROPEAN 
SOFTWARE ENGINEERING CONFERENCE AND SYMPOSIUM ON THE FOUNDATIONS. 
OF SOFTWARE ENGINEERING, 26, 2018. Proceedings [...]. 2018. Disponível em: https:// 
dlacm.org/doi/10.1145/3236024.3275528. Acesso em: 3 out. 2020. 

EBERT,C. etal DevOps. leee Software, v. 33, n. 3, 2016. Disponível em: https://ieeexplore. 
ieee.org/document/7458761./authorstauthors. Acesso em: 3 out. 2020. 

GITLAB. [S. :s. n)), 2020. Disponível em: https://about.gitlab.com/ Acesso em: 3 out. 2020. 


202) (pipeline de integração e entrega contínuas 
LIMA, M. Testes automatizados no Jenkins: recursos, plugins e dicas para aumentar 
a produtividade. In: CWI Software. [S. |:s. n], 2017. Disponível em: https://medium. 
com/cwi-software/testes-automatizados-no-jenkins-recursos-plugins-e-dicas-para-
-aumentar-a-produtividade-1685ffale9db. Acesso em: 3 out. 2020. 

LIRA, F; ZANONI, R; TALON, A. Agilidade no desenvolvimento de software utilizando in-
tegração contínua. Caderno de Estudos Tecnológicos, v. 4, n. 1, 2016. Disponível em: http:// 
wwwfatecbauru.edu.br/ojs/index.php/CET/article/view/205. Acesso em: 3 out. 2020. 

[MIRO MEDIUM]. [S. 1: s. n, 201-2]. Disponível em: https://miro.medium.com/ 

max/700/1"MShedf9-krkOr12LyeGtqQ Acesso em: 3 out. 2020.

jpeg. 

PLETSCH, L. A; PORTO, J. B. Integração contínua no desenvolvimento de software 

com a linguagem ABAP. in: SIMPÓSIO BRASILEIRO DE QUALIDADE DE SOFTWARE, 

16. 2017. Anais [.]. Rio de Janeiro, 2017. Disponível em: https:/Awww.researchgate. 
net/publication/330937324Continua no Desenvolvimento. de. Sof-
Integracao 
tware com a Linguagem Acesso em: 3 out. 2020.

ABAP. 

SOUZA, A. T. Aplicação de integração contínua para viabilizar a rastreabilidadeartefatos

de 
durantes a de software. 2014. 103 f Monografia (Especialização) -

manutenção Universi-
dade Tecnológica Federal do Paraná, Medianeira, 2014. 

Leituras recomendadas 

BOAGLIO, F. Jenkins: automatize tudo sem complicações. 2. ed. [S. 1]: Casa do Código, 

2019. 

KIM, G. etal. DevOps: como obter agilidade, confiabilidade e segurança em organizações 

tecnológicas. Rio de Janeiro: Alta Books, 2018. 

MORAES, G. DevOps: um guia para construção, administração e arquitetura de sistemas 
modernos. [S. ]: Casa do Código, 2015. 

Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-

Cionamento foi comprovado no momento da publicação do material. No entanto, a 

rede é extremamente dinâmica; suas páginas estão constantemente mudando de 

local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 

d 


Identificação e 
planejamento 
da configuração 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

| E Reconhecer a importância do gerenciamento de configuração no 
desenvolvimento de software. 
E Listar itens de configuração. 
E Descrever as etapas de identificação e planejamento da configuração. 

Introdução 

A gerência de configuração de software serve como base para outras 
áreas da engenharia de software. Por isso, é tratada como requisito de 
implementação em níveis iniciais de vários modelos de maturidade de 
processos de desenvolvimento, como CMMI-DEV, SPICE e MPS-Br. 

Segundo Sommerville (2011), a gerência de configuração é um con-
junto de atividades de rastreamento e controle iniciadas quando um 
projeto de engenharia de software começa. Essas atividades terminam 
apenas quando o software sai de operação. 

Neste capítulo, você vai verificar a importância da gerência de con-
figuração aplicada ao desenvolvimento de softwares. Você também 
vai ver como identificar e listar os itens de configuração. Além disso, vai 
conhecer as etapas de identificação e planejamento da configuração. 

Aimportância do gerenciamento 
de configuração 

Para começar, você precisa conhecer o conceito de gerenciamento de con-
figuração aplicado ao desenvolvimento de softwares. Considere o seguinte: 



204 ) (identificação e planejamento dia configuração 
A gestão de configuração de software (software configuration management 

— SCM) é uma atividade do tipo “guarda-chuva”, aplicada através de toda 
a gestão de qualidade. Como as mudanças podem ocorrer em qualquer ins-
tante, as atividades SCM são desenvolvidas para (1) identificar a alteração, 
(2) controlar a alteração, (3) assegurar que a alteração esteja sendo im-
plementada corretamente e (4) relatar as alterações a outros interessados. 
[.] É um conjunto de atividades de rastreamento e controle iniciadas quando 
um projeto de engenharia de software começa e [que] termina apenas quando 
o software sai de operação (SOMMERVILLE, 2011). 
As atividades da gerência de configuração de software incluem: controlar e 
acompanhar as mudanças, registrar todas as evoluções do sistema e estabelecer 
e manter a integridade. Como você sabe, todas as alterações realizadas no 
desenvolvimento de um software devem ser controladas. Esse controle está 
diretamente atrelado à qualidade do software entregue. Assim, a gerência é 
parte essencial da gestão de qualidade (PRESSMAN; MAXIM, 2016). 

2 Itens de configuração 

Como você viu, o gerenciamento de configuração aplicado ao desenvolvimento 
de softwares é baseado em um conjunto de atividades: 

E controle de mudanças; 

E controle de versões; 

E integração contínua. 

Nas próximas seções, você vai conhecer melhor cada uma dessas ativi-

dades, verificando como funcionam e quais são as ferramentas disponíveis 

para executá-las. 

Controle de mudanças 

O controle de mudanças colabora com o controle de versão e tem como foco 
avaliar, aceitar e aplicar as mudanças propostas. As mudanças ocorrem frequen-
temente no decorrer do desenvolvimento do software e devem ser registradas, 
avaliadas e agrupadas de acordo com a prioridade da modificação solicitada. 

Trata-se de um item importante pois, diante das informações geradas 
a partir dos registros, da avaliação e das prioridades, é possível planejar o 



Identificação e planejamento da configuração )(205 

escopo, os prazos e os custos de um projeto, mantendo a rastreabilidade desde 
a solicitação até a implantação e a disponibilização do produto. Entre as fer-
ramentas disponíveis para executar o controle de mudança, estão: BitBucket, 
GitHub, Trac e Bugzilla. 

Controle de versão 

Quando ocorre a implementação de uma mudança, deve-se acrescentar o 
seu registro no histórico do incremento. Esse incremento é correspondente 
a uma configuração (versão), ou seja, ao conjunto de itens que formam o 
software. 

Entre as funcionalidades do controle de versões, duas se destacam: viabilizar 
a edição concorrente nos arquivos, que ocorre quando duas ou mais pessoas 
estão trabalhando no mesmo código-fonte, por exemplo; e criar variações no 
projeto. 

O controle de versão é a principal atividade da gerência de configuração 
de software, pois é a ponte entre o controle de mudanças e a integração con-
tínua. São exemplos de ferramentas utilizadas para o controle de versão: Git, 
Mercurial e SubVersion. 

No controle de versão, existem os branches (“ramos”). Eles permitem que 
execuções simultâneas de atividades diferentes, e até mesmo incompatíveis, 
ocorram. Por exemplo: a codificação em arquivos iguais, testes, etc. Existem 
três tipos de branches. Veja a seguir. 

E Principal, também conhecido como trunk (SubVersion), master (Git) 
e default (Mercurial): trata-se do primeiro e mais importante, pois 
concentra as correções, melhorias e funcionalidades que deverão ser 
disponibilizadas na próxima versão entregue. 
E Dedicado, também conhecido como feature branch: mantém de forma 
isolada as implementações demoradas e complexas, que não podem ser 
realizadas no trunk. 
E Manutenção, também conhecida como branch release: é correspondente 
a uma versão liberada, em produção, que necessita de correção, porém 
sem alterar as funcionalidades. 

A Figura 1 mostra que o branch de manutenção é criado para manter 
a versão de produção. Note que as correções são realizadas no branch de 
manutenção e passadas para o trunk por merges (mesclagens). 



206 ) [ Identificação e planejamento da configuração 
manutenção 

principal 

dedicado 

Figura 1. Tipos de branches. 

Fonte: Dias (2016, documento on-line). 

Integração contínua 

Esse item de gerência de configuração tem como objetivo verificar se o software 
foi desenvolvido de acordo com os elementos registrados na configuração. 
Ou seja, a integração é realizada por meio de scripts que deixam a construção, 
os testes e a coleta de métricas de qualidade automatizados. 

As ferramentas de integração contínua acompanham o controle de versão 
e disparam scripts a cada nova configuração registrada. São exemplos de 
ferramentas de integração contínua: Jenkins, Drone.io, CodeShip, entre outras. 

3 Etapas de identificação e planejamento 
da configuração 

A gerência de configuração de software é fundamental para desenvolver 
softwares de qualidade. Para executá-la corretamente, você deve entender os 
conceitos, definir os processos, realizar a integração e utilizar as ferramentas 
disponíveis. 

Realizar o planejamento da configuração de software é de suma impor-
tância, pois a maioria das rotinas são repetitivas. Para fazer um planejamento 
adequado, você precisa considerar alguns itens. Veja a seguir. 

E Definição de atividades: é necessário determinar as atividades que 
serão executadas e a maneira como serão realizadas. 



Identificação e planejamento da configuração ) ( 207 

mB Termos utilizados: sem exceção, todos os envolvidos no planejamento 
devem entender tudo o que for relacionado à terminologia do conteúdo. 
Geralmente, é indicado incluir um glossário ao projeto para facilitar 

o entendimento. 
m Recursos envolvidos: deve-se definir com antecedência as pessoas 
envolvidas, o tipo de hardware, o espaço e os sistemas utilizados. 
m Cronograma: é preciso definir os prazos para cada etapa do planeja-
mento inicial. 

Dentro da identificação e do planejamento da configuração, há algumas 
etapas a serem cumpridas para que o sucesso seja garantido. É o que você 
pode ver na Figura 2. 

a 

Repositório 

Controle de mudança e 
configuração 

Estimativa de status de 

configuração 

Identificação de marcos 

Treinamentos e recursos 

Figura 2. Fluxograma das etapas de 
identificação e planejamento da gerên-
cia de configuração. 


208 ) (identificação e planejamento da configuração 
A partir do plano de configuração, o programa de gerenciamento de confi-
guração pode ser implantado. Para isso, deve-se seguir um escopo do processo, 
que é necessário para definir o que será feito no projeto. Existem métodos de 
identificação que conterão o detalhamento para a nomeação dos artefatos de 
pastas do produto de software. Tais métodos serão detalhados no documento 
do projeto. 

Um plano de gestão de configuração de software define a estratégia do 
projeto para a gestão de alterações. Por sua vez, o processo de controle de 
alterações produz requisições de alterações de software, relatórios e ordens de 
alteração de engenharia (PRESSMAN; MAXIM, 2016). Ele é composto por: 

documento de arquitetura; 
documento de implantação; 
plano de gerenciamento de configuração; 
documento de permissões de pastas e acessos por perfil; 
documento de controle de baselines; 
documento de negócio; 
plano do projeto; 
planilha de contagem de ponto de função; 
documento de processo de negócio; 
checklist de revisão técnica; 
relatório de revisão técnica; 
plano de teste; 
plano de resultado de teste; 
roteiros de teste; 
especificação de caso de uso. 

As baselines do projeto são definidas a cada mudança de fase do projeto, 
e uma delas é utilizada para o encerramento. As baselines tem quatro fases 
mais o encerramento. Veja: 

1. Apresentação dos seguintes itens de configuração: documento de ar-
quitetura, documento de implantação e plano de gerenciamento de 
configuração. 
2. Apresentação dos seguintes itens: documento de permissões de pastas 
e acessos por perfil, documento de controle de baselines e documento 
de negócio. 
3. Apresentação dos seguintes documentos: plano do projeto, planilha 
de contagem de ponto de função e documento de processo de negócio. 

Identificação e planejamento da configuração | [ 209 

4. Apresentação dos seguintes documentos: checklist de revisão técnica, 
relatório de revisão técnica e plano de teste. 
5. Encerramento: apresentação de todos os itens gerados anteriormente 
e do termo de encerramento. 
Em gerência de configuração de software, repositório é um conjunto de 
mecanismos e estruturas de dados que possibilita que a equipe de software 
gerencie as alterações. O detalhamento da estrutura do repositório é a etapa 
seguinte à das baselines. Nela, o documento do projeto é esmiuçado e dispo-
nibilizado na pasta de gerência de configuração. 

No controle de configuração e mudança, são apresentados: 

E o processo de solicitação de mudança, em que são descritos e submetidos 
os problemas e as mudanças, assim como a revisão e a disposição. 

E o comitê de controle de mudança, com a descrição dos membros do 
comitê e dos procedimentos necessários para processar as solicitações 
de mudanças e aprovações. 

Na etapa de estimativa do status de configuração, existem as tarefas 
listadas a seguir. 

m Processo de armazenamento e liberação do projeto: descrição das 
políticas de retenção e dos planos de backup, assim como de erros 
irreversíveis e recuperação. Nesse item, também é descrito o modo 
como a mídia deve ser mantida (se on-line ou off-line), o tipo de mídia 
e o formato. Esse processo também descreve o conteúdo do release, 
a quem é destinado e se há problemas conhecidos ou ainda instruções 
de instalação. 

m Relatórios e auditorias: os relatórios são utilizados para avaliar a quali-
dade do software em qualquer fase do seu ciclo de vida. São elaborados 
relatórios sobre os defeitos com base nas solicitações de mudança, 
fornecendo indicadores com a finalidade de alertar a administração e 
os desenvolvedores em relação a áreas prioritárias do desenvolvimento. 

Na etapa de identificação dos marcos, os internos devem ser percebidos. 
Os marcos do fornecedor são relacionados ao esforço da gerência de confi-
guração do software ou projeto. Eles incluem detalhes sobre o momento em 
que o plano de gestão de configuração deve ser atualizado. 



210) ( Identificação e planejamento da configuração 
Os treinamentos e recursos fazem parte da etapa seguinte, em que são 
descritas ferramentas de software, pessoal e treinamento. Essas três catego-
rias de recursos são requisitos para a implementação e a especificação das 
atividades de gerência de configuração. 

Referências 

DIAS, A. F. Tipos de ramos do controle de versão. 2016. Disponível em: https://blog.pronus. 
io/posts/tipos-de-ramos-do-controle-de-versao/ Acesso em: 27 jun. 2019. 

PRESSMAN, R. S.; MAXIM, B. R. Engenharia de software: uma abordagem profissional. 
8.ed. Porto Alegre: AMGH, 2016. 

SOMMERVILLE, |. Engenharia de software. 9. ed. São Paulo: Pearson, 2011. 

Leituras recomendadas 

GERENCIAMENTO de configuração de software. [2010]. Disponível em: http://www. 
alexandresmcampos.adm.br/informatica/outros/gcs.ntm. Acesso em: 27 jun. 2019. 

REIS, C. Caracterização de um processo de software para projetos de software livre. 2003. 
Dissertação (Mestrado em Ciências da Computação) -Instituto de Ciências Matemática 
e Computação, São Carlos, 2003. 

SCHACH, S. R. Engenharia de software: os paradigmas clássicos & orientado a objetos. 

7. ed. Porto Alegre: AMGH, 2009. 
SCHWARTZ, J. |. Construction of software. In: HOROWITZ, E. Practical strategies for 
developing large systems. Menlo Park: Addison-Wesley, 1975. 

Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-

cionamento foi comprovado no momento da 
rede é extremamente dinâmica; suas páginas 
local e conteúdo. Assim, os editores declaram 
sobre qualidade, precisão ou integralidade das 

publicação do material. No entanto, a 
estão constantemente mudando de 
não ter qualquer responsabilidade 

informações referidas em tais links. 


Gerenciamento de entregas 

Introdução A 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

m Comparar integração contínua, entrega contínua e implantação 

contínua. 
E Explicar o que é blue-green deployment. 
m Descrever o provisionamento em ambientes distribuídos. 

Introdução ———— — 

A integração contínua tem como objetivo facilitar a implantação de 
softwares desenvolvidos de forma que o sistema fique inoperante pelo 
menor tempo possível ou que não fique inoperante em momento algum. 
Como você vai ver, a técnica blue-green deployment pode colaborar para 
isso e para a implantação contínua de desenvolvimento. 

Neste capítulo, você vai estudar a integração, a entrega e a implan-
tação contínuas. Além disso, vai se familiarizar com a técnica blue-green 
deployment e com o provisionamento em ambientes distribuídos. 

1 Integração, entrega e implantação contínuas 

A integração contínua é uma prática de desenvolvimento de software por 
meio da qual os desenvolvedores frequentemente disponibilizam suas 
alterações de código em um repositório central. Ela se refere ao estágio 
de criação ou integração do processo de lançamento de software, em que 
também se originam um componente de automação e um componente 
cultural. Os principais objetivos da integração contínua são: encontrar e 
investigar bugs de forma mais rápida, melhorar a qualidade do software 
e reduzir o tempo que se leva para validar e lançar novas atualizações de 
um software. 



212 )[ Gerenciamento de entregas 

A integração contínua é necessária para facilitar e agilizar a convergência 
de alterações realizadas por diferentes desenvolvedores, minimizando os 
erros. Dessa forma, a atualização para os clientes é mais rápida e eficiente. 
A integração contínua é a primeira etapa da entrega contínua, na qual cada 
desenvolvedor mantém o seu trabalho em andamento continuamente integrado 
ao trabalho de todos os outros desenvolvedores. 

Fique atento 

A diferença entre entrega contínua e implantação contínua está na decisão de negó-
cios referente à implantação em produção. A entrega contínua consiste em manter 
a aplicação em um estado em que sempre possa ser implantada em produção. Já 
a implantação contínua implanta todas as alterações em produção todos os dias, 
ou com maior frequência. 

O funcionamento da integração contínua ocorre da seguinte forma: 

E os desenvolvedores utilizam um repositório de forma comparti-
lhada por meio de um sistema de controle de versão, como o Git e 

o Bitbucket; 
m antes que ocorra cada confirmação por parte dos desenvolvedores, 
eles podem optar por executar testes de unidade localmente em seus 
códigos, como uma verificação extra que se dá antes da integração; 

Bm o serviço de integração contínua cria e executa de forma automática 
os testes de unidade nas novas alterações de código, apresentando de 
imediato todos os erros (caso existam); 

m a cada revisão confirmada pelo desenvolvedor, a integração contínua 
cria o teste automatizado, executando a distribuição contínua. 

Por meio da distribuição contínua, as alterações de códigos são criadas, 
testadas e preparadas de forma automática para que sejam liberadas em 
produção. Dessa forma, a distribuição contínua expande-se com base na 
integração contínua quando realiza a implantação de todas as alterações 
de código em um ambiente de teste ou produção. É o que você pode ver na 
Figura 1. 



Gerenciamento de entregas )( 213 

RE! 

Po e O 

Sourer Conreos Bono Sracno 
Figura 1. Integração contínua, entrega contínua e implementação contínua. 

Fonte:O que... (2019) 

Entre os benefícios da integração contínua, estão os listados a seguir. 

E Melhora na produtividade do desenvolvedor: a integração contínua 
ajuda a equipe a ter maior produtividade ao liberar desenvolvedores 
de tarefas manuais, o que também reduz erros e bugs para os clientes. 
E Investigação mais rápida e fácil de bugs: com a maior frequência de 
testes, a possibilidade de descobrir e investigar bugs mais cedo é maior, 
impedindo ou minimizando grandes problemas no futuro. 
E Distribuição mais rápida de atualizações: a equipe consegue distri-
buir atualizações para os clientes de forma mais rápida e com maior 
frequência. 

Blue-green deployment 

A blue-green deployment é uma técnica utilizada para reduzir o risco de 
implementações. Considere que o ambiente de produção atual está associado 
à cor azul; a técnica consiste em introduzir um ambiente paralelo verde, 
no qual estará a nova versão do software. Assim, quando tudo estiver testado 
e de acordo com o esperado para iniciar a operação, é necessário apenas 
direcionar os usuários para que passem a acessar o ambiente verde. 

Essa técnica é muito importante para implementar a entrega contínua, 
pois reduz o risco, permitindo a realização de testes antes que seja lançada 
a nova versão para produção. Além disso, ela permite implantações com 
tempo inoperacional quase zero e uma forma rápida de reverter o processo 
caso algo dê errado. A ideia fundamental é ter dois ambientes facilmente 



214) ( Gerenciamento de entregas 
alternáveis para intercalar entre eles. Há muitas maneiras de variar os de-
talhes. Um projeto pode fazer a troca saltando o servidor web em vez de 

trabalhar no roteador. Outra variação seria usar o mesmo banco de dados, 
fazendo os switches azul e verde para as camadas da web e do domínio, 
como você pode ver na Figura 2. 

Web App 
Server Server 

a 

& > 

Figura 2. Blue-green deployment. 

Fonte: Fowler (2010, documento on-line) 
A J 

A implantação azul-verde oferece uma maneira rápida de reverter o processo 

— se algo der errado, basta alternar o roteador de volta para o ambiente azul. 
Ainda há a questão de lidar com transações perdidas enquanto o ambiente 
verde estiver ativo. Contudo, dependendo do projeto, é possível alimentar 
transações para ambos os ambientes de forma a manter o ambiente azul como 
backup quando o verde estiver ativo. 
Uma vez que você tenha colocado o ambiente verde como ativo e ele 
estiver estável, você pode utilizar o ambiente azul como ambiente de prepa-
ração para a etapa final de teste da próxima implementação. Quando você 
estiver pronto para o seu próximo lançamento, pode mudar de verde para 
azul da mesma forma. Assim, os ambientes verde e azul alternam regular-
mente entre a versão anterior ativa (para retrocesso) e o armazenamento 
da próxima versão. 

Uma vantagem dessa abordagem é que ela põe em cena o mesmo mecanismo 
básico necessário para um trabalho hot-standby. Portanto, isso permite que se-
jam testados os procedimentos de recuperação de erros graves em cada versão. 



Gerenciamento de entregas ) (eis 

3 Provisionamento em ambientes distribuídos 

Diante do alto crescimento de informações das empresas, aumenta a quantidade 
de servidores e, consequentemente, ampliam-se os custos gerenciais e a comple-
xidade do ambiente. Para solucionar essas questões, é indicada a virtualização 
de servidores, que oferece uma otimização da infraestrutura de tecnologia de 
informação (TI). Entre as vantagens da virtualização, considere as seguintes: 

redução do espaço físico necessário para armazenamento; 
redução do consumo de energia por equipamentos; 
redução da dissipação de calor e da necessidade de refrigeração; 
redução de conexões de cabos de rede; 
redução de tomadas e cabos de energia; 
redução de switches. 

Ou seja, caso seja necessário um novo servidor, basta criar uma máquina 
virtual nova, que vai reduzir o tempo de provisionamento de novos servidores. 
As máquinas virtuais são arquivos, e é muito simples criar máquinas que se 
tornam novos servidores em minutos. Esses novos servidores não significam 
gastos com a aquisição de equipamentos físicos nem aumento da complexidade. 
Quando um servidor não aguentar mais a adição de novas máquinas virtuais, 
basta adicionar mais um novo servidor físico à sua rede. 

O provisionamento garante que servidores sejam preparados para receber 
a implantação de novos projetos e colaborar durante as operações de rotina 
de manutenção. As etapas do provisionamento de servidores consistem em: 

m selecionar servidores (um ou mais) entre os que estão disponíveis; 
E instalar softwares, entre eles sistema operacional, aplicações, middleware 
e drivers; 
m definir e configurar recursos do servidor, como endereço de Internet 
Protocol (IP), espaço em disco, entre outros. 

O provisionamento é um processo que exige tempo de gestores e técnicos 
de TI envolvidos. Existem ferramentas de automação que podem ser utiliza-
das para agilizar esse processo. Entre as ferramentas, estão Chef e Puppet, 
que são open source. Outra forma de realizar essa automação é por meio de 
scripts desenvolvidos pela própria equipe, ou de uma combinação de soluções 
externas, internas e processos manuais, articulados a fim de que o processo 
seja agilizado. 



216) ( Gerenciamento de entregas 
Referências 

FOWLER, M. BlueGreenDeployment. In: MARTIN Fowler. [5. :s. nJ, 2015. Disponível em: 
https:/martinfowlercom/bliki/BlueGreenDeployment.html. Acesso em: 30 jun. 2019. 

O QUE significa integração contínua? In: AMAZON. [S. |:s. nJ, 2019. Disponível em: 
https://aws.amazon.com/pt/devops/continuous-integration/. Acesso em: 30jun. 2019. 

Leituras recomendadas 

BLUE-GREEN deployment. In: INFOO Brasil. [S. [.:s. n., 2019]. Disponível em: https:// 
www.nfoq.com/br/presentations/blue-green-deployment. Acesso em: 30 jun. 2019. 

HUMBLE, J.; FARLEY, D. Continuous delivery: anatomy of the deployment pipeline. In: 

INFORMIT. Hoboken: [5. n], 2010. Disponível em: http://www.informit.com/articles/ 

articleaspx?p=1621865. Acesso em: 30 jun. 2019. 

PRESSMAN, R.; MAXIM, B. Engenharia de software: uma abordagem profissional. 8. ed. 
Porto Alegre: AMGH, 2016. 

ROUSE, M. Software process improvement and capability determination (SPICE). In: 
TECHTARGET. [S. [:s. n), 2008. Disponível em: https://searchsoftwarequality.techtarget. 
com/definition/Software-Process-Improvement-and-Capability-dEtermination. Acesso 
em: 30 jun. 2019. 

SCHACH, S. R. Engenharia de software: os paradigmas clássicos & orientado a objetos. 
Porto Alegre: AMGH, 2010. 

VIRTUALIZAÇÃO: o que é gerenciamento de virtualização? In: RED HAT. [S. :s. n., 2019]. 

Disponível em: https://www.redhat.com/pt-br/topics/virtualization/what-is-virtualiza-

tion-managementk. Acesso em: 30 jun. 2019. 

Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 

d 


Ferramentas de 
gerenciamento de 
configuração de software 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Identificar a função das ferramentas de gerenciamento de configu-
ração de software. 
E Listar as principais ferramentas de gerenciamento de configuração 
de software. 
E Utilizar ferramentas de gerenciamento de configuração de software. 

Introdução 

Nas últimas décadas, com as inovações tecnológicas e os novos com-
portamentos sociais, as equipes de desenvolvimento de software têm 
se tornado multidisciplinares e independentes geograficamente. Nesse 
cenário, as ferramentas de gerenciamento de configuração de software 
(GCSs) se destacam, pois automatizam o processo de desenvolvimento 
de softwares. As GCSs estabelecem e mantêm a integridade e a rastreabi-
lidade de todo o processo de mudanças, contribuindo no monitoramento 
da qualidade e do ciclo de vida do software. 

Neste capítulo, você vai verificar como o uso adequado de ferramentas 
de GCS agiliza, automatiza e garante a qualidade em todo o processo de 
desenvolvimento de softwares. 

Função 

Você já deve ter ouvido falar sobre ferramentas de configuração de software. 
Elas aparecem muitas vezes em contextos profissionais, mas é comum que não 
haja muita referência ao modo como se integram e se organizam conceitual



218 ) ( Ferramentas de gerenciamento de configuração de software 
mente. De acordo com o Swebok elaborado pela IEEE Computer Society, a GCS 
é bastante alinhada à garantia da qualidade de software (GQS), produzindo 
conhecimento e informações importantes para todo o gerenciamento do ciclo 
de vida do software, bem como para a aplicação de melhorias continuadas 
(BOURQUE; FAIRLEY, 2014). Atingir as metas da GQS exige a manipulação 
de um volume considerável de dados, que podem ser suportados por meio do 
uso de ferramentas de GCS, capazes de tornar a tarefa mais ágil. 

Saiba mais 

O Instituto de Engenheiros Eletricistas e Eletrônicos (IEEE) é uma organização profissional 
sem fins lucrativos que promove o conhecimento nas áreas de engenharia elétrica, 
eletrônica e computação. Ele foi fundado nos Estados Unidos. 

Por sua vez, o Swebok (Guide to the Software Engineering Body of Knowledge) 
é um documento criado pelo IEEE que serve como referência para a comunidade de 
engenharia de software (BOURQUE; FAIRLEY, 2014). 

O Swebok define a divisão dos tópicos da GCS conforme a Figura 1. 
As atividades são organizadas em: gestão dos processos de GCS, identificação 
de configuração de software (CS), controle de CS, medições e correções de 
CS, auditoria de CS, gestão de controle de versões e distribuição e a própria 
gestão das ferramentas de GCS (BOURQUE; FAIRLEY, 2014). 

Gerenciamento de 
configuração de software 

Lol

Preasão = Verê

See | |memtcação | | comie | | Mitos | | mustota || Ne

ottio dentição 

Figura 1. Organização das áreas de GCS. 
Fonte: Adaptada de Bourque e Fairley (2014) 


Ferramentas de gerenciamento de configuração de software ) (219 

À medida que a complexidade do desenvolvimento de softwares aumenta, 
faz-se cada vez mais necessária a implementação tanto de processos de GCS 
quanto de ferramentas que automatizem os processos. Com essa implemen-
tação, cada alteração no software fica vinculada a registros dentro das ferra-
mentas de GCS, garantindo a rastreabilidade de todas as mudanças e maior 
qualidade nos serviços e produtos entregues (PRESSMAN; MAXIM, 2016). 
Assim, a GCS é importante para a garantia da qualidade de software, a sua 
verificação e a sua validação, bem como para as revisões e auditorias. 

O uso de ferramentas de GCS não é recente. Essas ferramentas são utilizadas 
para controle de revisão há muito tempo no ambiente UNIX, com funções 
mais limitadas e simplificadas (LEON, 2015). Seu foco baseava-se na gestão 
dos códigos, mas, com o passar do tempo, as ferramentas se tornaram mais 
completas, gerenciando várias funcionalidades do processo de desenvolvimento 
de softwares. 

O aumento na busca por ferramentas de GCS pode ser atribuído a alguns 
fatores. Por exemplo: redução do tempo de desenvolvimento do software, 
melhoria na produtividade, agilidade no rastreamento de problemas, agilidade 
na liberação de novos softwares, redução de erros e maior integração das 
informações para auditorias (LEON, 2015). 

Todas as características e funcionalidades existentes e a serem implemen-
tadas em ferramentas de GCS serão inúteis se as equipes de desenvolvimento 
não garantirem a atualização e o uso correto das ferramentas. A ideia de que 

o desenvolvimento de softwares pode ser feito por um único programador 
ficou no passado. E todas as vezes em que duas ou mais pessoas estão en-
volvidas num mesmo trabalho, um fator crucial para o bom desempenho das 
atividades é a comunicação interpessoal, que é gerenciada pela maioria das 
ferramentas de GCS. 
Como você sabe, processos repetitivos que envolvem documentação, 
modificações, rastreamento, revisões e correções de problemas consomem 
muito tempo dos desenvolvedores. É por isso que, se bem utilizada, a ferra-
menta de GCS agiliza os trabalhos e garante a produtividade (LEON, 2015). 

Automatização 

As ferramentas que automatizam os processos de GCS se baseiam na organi-
zação proposta na Figura 1. Ou seja, cada uma das atividades é composta por 
um conjunto de tarefas que definem e manipulam os itens de configuração 



220) ( Ferramentas de gerenciamento de configuração de software 
(TCs). Os ICs podem ser documentos de quaisquer tipos, casos de testes, 
códigos de programas, componentes, bibliotecas, frameworks, modelos de 
bancos de dados, entre tantos outros artefatos próprios ao desenvolvimento de 
softwares (PRESSMAN; MAXIM, 2016). A seguir, você vai conhecer mais 
sobre a estruturação da GCS (BOURQUE; FAIRLEY, 2014). 

Gestão de processos de GCS: descreve padrões e procedimentos 
usados na definição da configuração do software (como o que será 
gerenciado), identifica os atores responsáveis por cada atividade, idea-
liza o cronograma, define a estrutura de repositório, estabelece as 
políticas de trabalho, determina as linhas de base e a sua estrutura e 
determina quais ferramentas são importantes para o auxílio de toda a 
gestão. É responsável pela organização do contexto, pela definição de 
regras e guias para todo o processo de GCS, pelo planejamento e pela 
supervisão da GCS. 
Identificação de GCS: controla e administra os itens de configuração 
de software de acordo com a especificação a seguir. 
= Identificação de cada item de configuração. 
= Relacionamento entre os itens. 
= Versões de software. 
= Linha de base e aquisição de artefatos de configuração. 
Controle de CS: realiza a gestão durante o ciclo de vida, determinando 
quais mudanças serão realizadas. Autoriza e aprova as alterações, 
apoiando a implementação das mudanças. 
Medições de correções de CS: controlam a correção de defeitos e erros 

durante o processo de desenvolvimento do software. 
Auditoria de CS: avalia a conformidade dos produtos de software, 
bem como os processos de regulamentação deles. 
Gestão de versões e distribuição: é a gestão do controle de versões de 

todos os artefatos, em especial do software desenvolvido, garantindo 
que os artefatos sejam liberados e documentados com confiabilidade. 
Inclui também o empacotamento das versões para instalação, bem 
como seus testes. 
Gestão de ferramentas de configuração: é responsável pelas definições 
e pela administração do uso das ferramentas mais adequadas para apoio 
a todo o processo de gestão de configuração. 



Ferramentas de gerenciamento de configuração de sofmare ) ( 221 

N 
Fique atento 

Há três situações diferentes em relação à adoção de ferramentas de GCS: 
E aquisição de ferramenta proprietária; 
E adoção de ferramenta de código aberto (open source); 
E implementação conforme os requisitos particulares do projeto. 

Há certa variedade de ferramentas de GCS tanto proprietárias quanto de código 
aberto. Um dos aspectos importantes da análise e da definição de ferramentas é per-
ceber o que é mais adequado a cada projeto. Muitas empresas optam por desenvolver 
uma metodologia de configuração por conta da complexidade e do alto custo do 
processo de GCS. Outras resolvem comprar ferramentas de empresas, muitas vezes 
consolidadas e que geram certa segurança no momento de treinamento e suporte. 
Finalmente, há a opção pelas ferramentas de código aberto, que são baixadas gratui-
tamente e que podem contar com suporte e treinamento ou não. 

Antes de adotar ferramentas de GCS, é importante compreender todos os 
processos e atividades que devem ser automatizados no projeto de desenvol-
vimento de softwares. Não é indicado adotar uma alternativa simplesmente 
porque o mercado a utiliza. 

Ferramentas, repositório e equipe de trabalho 

Além de todos os requisitos necessários para a adoção de ferramentas de con-
figuração, as próprias ferramentas devem estar sob a gestão de configuração 
— ou seja, é necessário atentar a ambientes de desenvolvimento integrado 
(integrated development environment — IDEs), compiladores, navegadores, 
pacotes, bibliotecas e outros ICs. Deve-se propor o congelamento de versões 
no repositório de configuração para que se estabeleça um conjunto de itens 
que estejam em pleno funcionamento (PRESSMAN; MAXIM, 2016). 

Assim, os repositórios de configuração de software devem também con-
templar o gerenciamento das ferramentas de desenvolvimento e os testes em 
suas versões compatíveis. Isso é importante para o bom andamento do ciclo de 
desenvolvimento do software e do processo de configuração. Uma alternativa 
interessante para que haja cobertura de tais dependências é a comunicação 
eletrônica padronizada, por meio de e-mails, wikis, gerenciadores de conteúdos 



222) (Ferramentas de gerenciamento de configuração de software 
e mídias sociais internas (LEON, 2015). O repositório de GCS centraliza a 

integração das ferramentas, dos artefatos e de toda a comunicação, gerando 

um centro de fluxo do processo e das informações em formato uniforme e 

padronizado. 

Fique atento 

De nada adiantam esforço, padronização, documentação e comunicação se a equipe 
não estiver alinhada às atividades de configuração para garantir que cada alteração 
seja devidamente adicionada no repositório de CS. 

2 Ferramentas, cenários, planejamento 
e funcionalidades 

Um processo de configuração pode ser burocrático e exigir muito trabalho 
em paralelo ao próprio projeto de desenvolvimento de softwares. É por isso 
que as empresas planejam a configuração antes da escolha das ferramentas. 
Há ferramentas para todos os tamanhos de negócios e necessidades. Em alguns 
casos, devido à complexidade da ferramenta e de seu destaque no mercado 
de configuração de software, há treinamentos e certificações dos produtos. 

A seguir, você vai ver um panorama sobre questões mais práticas. Primeira-
mente, define-se um cenário por meio de uma simples pergunta: quando se deve 
de fato implementar um sistema de gerenciamento de configuração? Não existe 
uma resposta única e exata, porém um bom momento é quando se agregam 
várias pessoas à equipe, que por vezes trabalham dispersas geograficamente. 

Assim, várias questões surgem, envolvendo autoria, políticas, orçamentos, 

cronogramas, a própria coordenação do time e dos processos e, finalmente, 

os custos envolvidos diante das constantes mudanças (LEON, 2015). 
No roteiro de questionamentos importantes para o planejamento, podem 

constar as perguntas listadas a seguir (BOURQUE; FAIRLEY, 2014). 

1. Organização: quais são as motivações para a aquisição das ferramentas? 
2. Ferramentas: serão adquiridas ou desenvolvidas? 
3. Ambiente: quais regras organizacionais e técnicas estarão no contexto 
do uso das ferramentas? 
4. Legado: como os projetos futuros usarão ou não as ferramentas? 

Ferramentas de gerenciamento de configuração de software ) (2as 

5. Financeiro: quem pagará pela aquisição, pela manutenção, pelo treina-
mento e pela customização das ferramentas? 
6. Escopo: como as ferramentas serão implantadas em toda a organização 
e em cada projeto específico? 
7. Responsabilidades: quem será responsável pela introdução de novas 
ferramentas? 
8. Futuro: quais são os planos para as ferramentas no futuro? 
9. Mudanças: quão adaptáveis são as ferramentas? 
10. Divisões e fusões: as ferramentas são compatíveis com os planejamentos 
de divisões organizacionais e as estratégias de fusão? 
11. Integração: já há alguma ferramenta em uso? As ferramentas escolhidas, 
bem como as existentes, possuem mecanismos de integração? 
12.Migração: é possível fazer a migração dos repositórios existentes para 
outras ferramentas enquanto ocorre a manutenção completa de todo o 
histórico de registros? 

Um detalhe não menos importante é o tamanho atual da equipe e as pros-
pecções futuras de crescimento. Ao serem respondidas algumas questões, 
é indicado o início do planejamento, mesmo que minimalista, seguindo um 
padrão público ou uma metodologia própria de gestão de mudanças. Em linhas 
gerais, definem-se as pessoas envolvidas e as suas devidas responsabilidades. 
Também é importante estabelecer uma linha de base com especificações de 
tempo, fixação de políticas de controle de versões, definição das ferramentas 
básicas e do que será atualizado no repositório de configuração (LEON, 2015). 

Um passo importante para complementar a base do planejamento é a 
formulação de um comparativo de ferramentas prontas, propondo um plano 
de aderência aos processos de GCS que serão implantados. Nesse caso, 

o Quadro 1 serve de modelo, considerando ferramentas proprietárias e de 
código aberto. Esse quadro traz algumas das funcionalidades mais importantes 
da automação do GCS das ferramentas elencadas. 
O primeiro ponto a ser considerado é a questão de código aberto ou pro-
prietário. Isso faz diferença principalmente para pequenas empresas, que 
possuem um orçamento enxuto. Empresas de médio a grande portejá podem 
considerar o pagamento de ferramentas, dando preferência à adoção de uma 
opção mais completa, por exemplo, o que pode simplificar o processo de 
treinamento e implantação. 

Solucionado esse ponto, a segunda consideração é relativa ao alcance da 
ferramenta no mercado. Por exemplo, grande parcela de desenvolvedores possui 
experiência com a ferramenta GIT, utilizando-a geralmente em associação 



224) ( Ferramentas de gerenciamento de configuração de software 
à plataforma GitHub. Quanto mais usuários proficientes numa determinada 
ferramenta existem, mais agilidade há na equipe de trabalho. 
O Quadro 1, a seguir, não contém todas as ferramentas existentes, porém 
considera as mais utilizadas e que estão em destaque no mercado. 

dá D 
Quadro 1. Comparativo de ferramentas de configuração de software 

Ferramenta Funcionalidades 

1ºo E) 
s|g sis s 

Slelels|S|8] E

ilsjêjalê|8/8|5 

Ansible Configuration 
Tool (ANSIBLE, 2019) Y Y ú v ? 

CFEngine Configuration 
Tool (CFEngine, 2019) VIU ido 

CHEF Configuration 
Tool (CHEF, 2019) Y Y Y Y Y Y Y E 

ConfigHub (2018) VIVI || |oY 

GIT (20190) 

IBM Rational ClearCase 
(IBM, 2019) 

JIRA (ATLASSIAN, 2019) VIVI |o lá 

mono [Miro 

Octopus Deploy (2019) VivVv|vIovIo / Pp 

Puppet Labs (PUPPET 2019) | / | V | 

RUDDER (2019) VIv|vlll|ovIo 

Eopre RA A A RA RARA RAE 

Subversion (2018) V o 
TeamCity Configuration y y y y J y o 
Tool (KHALUSOVA, 2015) 


Ferramentas de gerenciamento de configuração de software ) [225 

Fique atento 

Para conhecer melhor as ferramentas apresentadas no Quadro 1, você pode conferir 
as páginas de cada uma delas na internet. Nelas, você vai encontrar documentação. 
e tutoriais de instalação e uso. 

3 Estudo de caso 

A seguir, você vai conhecer melhor três ferramentas de gerenciamento de 
configuração: Git/GitHub, Puppet e TeamCity. O Git/GitHub é responsável 
pelo controle de versões de códigos. Já o Puppet ocupa-se do gerenciamento 
da infraestrutura. Finalmente, o TeamCity automatiza o processo de empa-
cotamento e distribuição do software. 

Os exemplos a seguir consideram um cenário hipotético de uma pequena 
empresa. A ideia é que você perceba como a implantação e a integração das 
ferramentas podem ser planejadas. Durante a apresentação do estudo de caso, 
você vai conhecer algumas funcionalidades das três ferramentas. 

Git e GitHub: controle de versões e trabalho 
colaborativo 

O GitHub é uma plataforma que simplifica o controle de versões de código 
e facilita a colaboração entre equipes de desenvolvimento de software. Ela é 
associada à ferramenta Git e ajuda na construção de um repositório centra-
lizado, no qual todos os desenvolvedores podem trabalhar simultaneamente 
baixando, editando e gerenciando códigos de programas. O GitHub documenta 
quando e o que cada pessoa atualizou no mesmo código, simultaneamente ou 
não, ajudando no rastreamento das alterações por pessoas e por data (GIT, 
2019a, 2019b, 2019c). 

Seja para trabalhar com um grupo de desenvolvedores ou para criar um 
repositório com seu portfólio profissional, o GitHub é uma ferramenta interes-
sante. Ele permite que seus repositórios estejam abertos à colaboração geral 
ou fechados para que somente você e seus colegas de projeto os enxerguem. 

Fazendo uma busca rápida em páginas de empresas de recursos humanos 
na internet, você vai ver que várias empresas de tecnologia solicitam que 

o profissional tenha o seu próprio repositório, compondo um portfólio de 
trabalhos feitos. Assim, o GitHub acaba se tornando uma grande rede social 

226 ) [ Ferramentas de gerenciamento de configuração de software 
de profissionais ligados ao desenvolvimento de software e que prezam pela 
colaboração. 

Você pode criar o seu perfil e os seus repositórios nessa ferramenta a 
qualquer momento, o que pode alavancar a sua carreira profissional. Além 
disso, com custo zero, pequenas empresas de desenvolvimento de software 
podem iniciar a automatização de processos de controle de versões e implantar 
Os primeiros passos do gerenciamento de configuração de software. 

Exemplo 

A empresa Algoritmos Integrados Associados iniciou suas atividades há 
menos de um ano e já conta com uma pequena equipe multidisciplinar de 
10 profissionais. Como a estrutura é pequena e inicial, a organização pretende 
trilhar caminhos mais automatizados no que diz respeito à qualidade e à 
gerência de configuração de software. 

Depois de algumas reuniões para decisões sobre vários assuntos, mas prin-
cipalmente focando na certeza de que precisam utilizar ferramentas de código 
aberto, os membros da equipe esperam desenhar uma trajetória de sucesso nos 
processos e aplicações. Para isso, eles optaram pela ferramenta de controle de 
versões Git, acompanhada da plataforma GitHub. Tal decisão se deu pelo fato 
de que esse processo de controle de versões é reconhecidamente o mais difícil 
de se gerir quando a equipe e a complexidade dos requisitos aumentam. Havia 
outras ferramentas disponíveis, como a SubVersion, porém, após a análise 
de uma tabela que comparava todas as ferramentas de controle de versões, 
a empresa optou pelo Git. A oferta de mão de obra que já tem experiência e 
utiliza a ferramenta para a colaboração em códigos foi um fator decisivo para 
que a Algoritmos Integrados Associados finalizasse a sua decisão. 

A equipe definiu e estabeleceu os seus repositórios no GitHub. Antes de 
tudo, todos participaram de estudos e treinamentos, tentando aumentar a 
produtividade com o uso adequado das ferramentas. Toda documentação, 
os tutoriais e as demais informações técnicas foram extraídas das próprias 
páginas do Git e do GitHub. A ferramenta Git conta com um livro eletrônico 
considerando os primeiros passos e demais funcionalidades (GIT, 2019a, 
2019b, 2019c). Também há muitas plataformas de ensino a distância oferecendo 
cursos básicos gratuitamente e com os custos mais variados. Não foi difícil 
optar pelos melhores cursos, uma vez que a Algoritmos Integrados possui um 
forte foco em estudos colaborativos entre os profissionais da equipe. Como 
você sabe, é imprescindível para empresas de desenvolvimento de softwares 

o estudo contínuo de ferramentas, tecnologias e metodologias de trabalho. 

Ferramentas de gerenciamento de configuração de software ) ( 227 

Como tarefa inicial, após os treinamentos, todos os profissionais criaram 
os seus perfis e se propuseram a colaborar com códigos abertos e pequenos 
tutoriais para fomentar a área de desenvolvimento de software. Os repositórios 
proprietários foram definidos como privados para garantir o sigilo dos trabalhos 
da empresa. Além de tudo, os membros da equipe se propuseram a conectar 
as suas IDEs padrões (Visual Studio Code e Sublime Text) à ferramenta Git. 

Puppet: arquitetura e integração 

O Puppet é uma ferramenta de código aberto direcionada à administração 
e à configuração de servidores, bem como à infraestrutura da tecnologia da 
informação (TI). Sua estrutura básica é composta por recursos e seus relaciona-
mentos; estes são comparados com o estado atual do sistema e fazem alterações 
automaticamente, em conformidade com um catálogo preestabelecido. Essa 
ferramenta simula mudanças de configuração, faz o rastreamento do histórico 
do sistema em todo o seu ciclo de vida e identifica se um código ainda produz 

o mesmo estado do software. 
Sua arquitetura é representada por três camadas (PUPPET, 2019): 
E linguagem de configuração; 
E camada de transação; 
E camada de abstração de recursos. 

A camada de linguagem de configuração é uma interface com linguagem 
natural que o ser humano pode compreender, e não uma linguagem de pro-
gramação ou de modelo de dados. Ela foi elaborada para facilitar a interação. 

Já a camada de transação é considerada o motor da ferramenta. Ela é 
responsável por interpretar e compilar a configuração, enviá-la para o agente, 
aplicá-la ao nó e relatar os resultados para o servidor. Tudo isso é feito por 
meio de um grafo que contém a lista de todos os recursos e de todos os seus 
relacionamentos. O grafo permite que a decisão seja ordenada para a aplicação 
da configuração com base nos relacionamentos configurados pelo adminis-
trador da ferramenta (PUPPET, 2019). 

Finalmente, a camada de abstração de recursos simplifica o trabalho do 
administrador do sistema, que muitas vezes precisa lidar com diversos sistemas 
operacionais. Lembre-se de que cada sistema operacional gerencia arquivos, 
pacotes, processos e serviços em execução, programas, contas de usuários, 
grupos, entre outros elementos. A ferramenta Puppet compreende todos esses 
elementos como recursos. 



228) ( Ferramentas de gerenciamento de configuração de software 
Exemplo 

Alguns meses se passaram na Algoritmos Integrados Associados e o controle 
de versões começou a ter bons resultados. A equipe aumentava e sentia-se a 
necessidade de contratar um profissional para DevOps. Novamente, depois 
de reuniões, foi contratado um profissional com experiência na ferramenta 
Puppet. Sua função era trabalhar em tudo o que se referisse à infraestrutura 
de TI da Algoritmos Integrados. 

Um dos critérios utilizados para a implantação do Puppet foi a sua inte-
gração com ferramentas de controle de versões, entre elas o Git. A equipe 
elaborou então um esquema básico (Figura 2) de como a integração se daria 
e do que seria prioritário no gerenciamento pelo novo integrante da equipe. 

= 

mm = 

comede

vesiesêiatosendo a 
Senidor rogo

enaasprão deite de 

ir jo e

ms OR configurações

distribui paraos nós 

Administradoras

faz

definiçõesda

intaesuturava

código (Puppen. No com agente 

E emas 
Puppe Chen) 
»
(Puppet “Q 

jmcom agente He

(Popper chan. (Puppet clemt 

Figura 2. Funcionalidades básicas do Puppet integrado a um gerenciador de versões Git 
ou SubVersion 

Fonte: Adaptada de Puppet (2019). 

A J 

O Puppet é geralmente usado como cliente/servidor, de acordo com o ciclo 
de operação a seguir. 

E Cada cliente, também conhecido como nó, tem um aplicativo (agente) 
instalado e em execução, que se conecta com o servidor periodicamente. 
O tempo é configurado, mas o padrão é que a conexão ocorra de 30 
em 30 minutos. 



Ferramentas de gerenciamento de configuração de software ) [229 

E Uma vez que atinja o tempo de sincronização, o nó envia a sua confi-
guração atual compilada para o servidor. 
Cada configuração compilada é conhecida como “chamada de catálogo”. 
E O resultado no ato da sincronização é relatado ao servidor, demonstrando 
se há ou não divergências entre ele e o cliente. 
Em A configuração do código do Puppet é armazenada num sistema de 
controle de versão como Git ou SubVersion. 

Um aspecto que a empresa considerou ao optar pelo Puppet foi a vasta lite-
ratura, os tutoriais e principalmente o fato de saber que havia uma comunidade 
brasileira (Puppet-BR) engajada em disseminar conteúdos sobre a ferramenta. 

TeamcCity: distribuição e integração 

A ferramenta TeamCity tem por finalidade gerenciar a distribuição da constru-
ção de testes e a integração contínua. Ela possui compatibilidade com várias 
tecnologias, como Java, .NET, Docker, SubVersion, Git, JIRA, Octopus, entre 
outras. Também possui plug-ins de fácil configuração para serviços de nuvem, 
como Aws, Azure, entre outros. 

Diferentemente do Git/GitHub e do Puppet, essa ferramenta possui dois 
tipos de licenças de uso: profissional (gratuita) e enterprise (paga/proprietária). 
Empresas pequenas podem iniciar a utilização pela versão profissional, uma 
vez que esta dá direito a 10 configurações de build. A persistência dos dados 
é feita por meio de um gerenciador de banco de dados MySQL, PostgreSQL, 
Oracle ou MS SQL. A TeamCity possui ainda uma wiki com muitos detalhes 
sobre todas as possibilidades de integração com outras ferramentas, seja por 
meio de APIs, XML ou JSON (KHALUSOVA, 2015). 

Um conceito muito importante para a ferramenta é o build, uma vez que o 
seu foco principal é justamente o gerenciamento das entregas das aplicações. 
Ao empacotar o software para distribuição por meio do TeamCity, você pode 
acionar o gerenciamento de disparo de comunicação para os integrantes da 
equipe por meio de e-mail, Slack, Telegram e outros plug-ins. 

À primeira vista, utilizar uma ferramenta de distribuição parece gerar um 
trabalho duplicado, porque o processo manual de implantação de software 
parece ser bem mais simples. No entanto, conforme a equipe de desenvolvi-
mento torna-se maior, o TeamCity auxilia na distribuição de versões corretas 
e garante a rastreabilidade por meio de relatórios e métricas. A partir disso, 
é possível identificar problemas em testes e no próprio empacotamento do 
software para distribuição. 



230) (Ferramentas de gerenciamento de configuração de software 
Exemplo 

Depois do sucesso alcançado com a implantação do Puppet e do Git como 
ferramentas de GCS na Algoritmos Integrados Associados, novamente, após 
exaustivas reuniões e definições, a equipe optou por implantar uma ferramenta 
para cuidar do empacotamento e da distribuição de seus aplicativos. 

A TeamCity foi a ferramenta escolhida depois de uma seleção minuciosa. 
O número de clientes aumentava e, consequentemente, cresciam os trabalhos 
de distribuição das aplicações, de acordo com todos os requisitos de entrega 
necessários. Embora a TeamCity se torne paga depois de determinado número 
de projetos, ainda sim a escolha foi vantajosa de acordo com a lista de requisitos. 

Logo, a equipe identificou que seria necessário elaborar um plano de boas 
práticas para o uso correto da ferramenta. Então, criou um pequeno manual 
de boas práticas, com os itens listados a seguir. 

E Após a instalação do TeamCity, o administrador de configurações de 
software criará um repositório que conterá um projeto. 
E O projeto poderá ser acessado e configurado num repositório manual 
ou em outro repositório, como no GitHub. 
m Todo projeto receberá um Project ID preenchido automaticamente. Ele 
será um identificador para as configurações da própria ferramenta e 
gerará uma URL apontando para tal Project ID. 
m Uma implantação futura será o Container Docker, pois ambos trabalham 
muito bem em conjunto. 
m Sempre é necessária a realização de backups e a configuração de noti-
ficações de builds para a geração de relatórios de controle. 
m Outra parametrização importante na ferramenta é a implementação de 
medidas para a resolução de falhas nos builds. 
Bm A integração com um sistema de controle de versão gera maior confia-
bilidade nos artefatos gerados em cada build; nesse caso, o Git. 
m Para maiores informações sobre conceitos, instalação e uso do TeamCity, siga 
corretamente os manuais e tutoriais disponibilizados pelo desenvolvedor. 

Mais agilidade nos processos de controle de versões, infraestrutura e 
distribuição garantiram resultados importantes à empresa, como maior produ-
tividade dos desenvolvedores e maior confiabilidade na liberação de versões 
e aplicativos para seus clientes. Agora, a Algoritmos Integrados almeja alçar 
novos voos. Para isso, está estudando outros recursos, bibliotecas e frameworks 
que possam colaborar na automatização de suas ferramentas de GCS. 



Ferramentas de gerenciamento de configuração de software ) (um 

Saiba mais 

O Docker é uma plataforma (de código aberto) criada pelo Google para facilitar o 
processo de virtualização. Ele facilita a criação e a administração de ambientes para 
a disponibilização mais rápida de programas para os clientes. O Docker cria, testa e 
implementa aplicações num ambiente chamado container. 

O desenvolvedor empacota o seu aplicativo de forma padronizada e o disponibiliza 
para execução. Empacotar prevê colocar no containeros códigos, as bibliotecas, algum 
runtime (se houver) e outras ferramentas necessárias para a execução do software. 
Saiba mais no linka seguir. 

https://grgo.page.link/ihbs4 

Como você viu, o mundo do gerenciamento de configuração de software 
é extremamente amplo. Assim, é importante que as equipes de desenvolvi-
mento de software busquem periodicamente apoio e informações sobre novas 
plataformas, frameworks, bibliotecas e tudo mais que automatize o processo 
de configuração. 

Referências 

ANSIBLE. [Site]. 2019. Disponível em: https:/Avww.ansible.com/, Acesso em: 26 jun. 2019. 

ATLASSIAN. Jira software. C2019. Disponível em: https:/Awww.atlassian.com/software/ 
jira/ Acesso em: 27 jun. 2019. 

BOURQUE, P; FAIRLEY, R. E. D. (ed). Swebok v3.0: guide to the software engineering body 
of knowledge. Piscataway: IEEE, 2014. Disponível em: https://ieeecs-media.computer. 
org/media/education/swebok/swebok-v3.pdf. Acesso em: 26 jun. 2019. 

CANONICAL LTD. Juju. 2018. Disponível em: https://jujucharms.com/, Acesso em: 
27 jun. 2019. 

CFENGINE. [Site]. c2019. Disponível em: https://cfengine.com/. Acesso em: 26 jun. 2019. 

CHEF. [Site]. [2019]. Disponível em: https:/Awww.chef.io/. Acesso em: 26 jun. 2019. 

CONFIGHUSB. [Site]. c2018. Disponível em: https:/Avww.confighub.com/. Acesso em: 
26 jun. 2019. 

GIT. [Site]. [2019c]. Disponível em: https://git-scm.com/, Acesso em: 26 jun. 2019. 


232) (Ferementas de gerenciamento de configuração de software 
GIT. Primeiros passos: sobre controle de versão. [2019]. Disponível em: https://git-scm. 

com/book/pt-br/vl/Primeiros-passos-Sobre-Controle-de-Vers%WC3%A30. Acesso em: 

26 jun. 2019. 

GITHUSB. [Site] c2019b. Disponível em: https://github.com/puppet-br/apostila-puppet/ 
releases. Acesso em: 26jun. 2019. 

IBM. [Site]. [2019]. Disponível em: https://wwwlbm.com/us-en/marketplace/rational-
-Clearcase. Acesso em: 26 jun. 2019. 

KHALUSOVA, M. Getting started with TeamCity. 2015. Disponível em: https://confluence. 

jetbrains.com/display/TCD9/Getting+Started+with+TeamCity. Acesso em: 26 jun. 2019. 

LEON, A. Software configuration management handbook. 3rd ed. Norwood: Artech 
House, 2015. 

OCTOPUS DEPLOY. [Site]. c2019. Disponível em: https://octopus.com. Acesso em: 
27 jun. 2019. 

PRESSMAN, R. S.; MAXIM, B. R. Engenharia de software: uma abordagem profissional. 

8. ed. Porto Alegre: AMGH, 2016. 
PUPPET. [Site]. [2019]. Disponível em: http://puppet-br.org/ Acesso em: 26 jun. 2019. 

RUDDER. [Site]. 2019. Disponível em: http:/Awww.rudder-project.org/. Acesso em: 
27 jun. 2019. 

SALTSTACK. [Site]. c2018. Disponível em: https://www.saltstack.com/. Acesso em: 
27 jun. 2019. 

SUBVERSION. Apache” Subversion*. c2018. Disponível em: https://subversion.apache. 
org/. Acesso em: 27 jun. 2019. 

Leituras recomendadas 

FERNANDES, J. M. Metodologia para a implantação de gerência de configuração de software 

em empresas de médio porte. 2011. Dissertação (Mestrado em Computação Aplicada) — 
Centro de Ciências e Tecnologia, Universidade Estadual do Ceará, Rio de Janeiro, 2011. 

MUNDO DOCKER. [Site]. c2019. Disponível em: https:/Awww.mundodocker.com.br/ 
tag/docker-brasil/. Acesso em: 27 jun. 2019. 

SOMMERVILLE, |. Engenharia de software. 9. ed. São Paulo: Pearson, 2011. 

THE ITIL Open Guide. [2019]. Disponível em: https:/Awww.itlibrary.org/. Acesso em: 
27 jun. 2019. 


Ferramentas de gerenciamento de configuração de software ) (233 

Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 


Esta página foi deixada em branco intencionalmente. 



Comunicação e colaboração 
na cultura DevOps 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Listar formas de como manter a equipe alinhada durante todo o 

processo. 

Apontar a importância da cultura DevOps estabelecida pelo time. 

E Reconhecer a comunicação como pilar do desenvolvimento e da 
operacionalização de um negócio. 

Introdução 

A prática de DevOps busca melhorar a eficiência do ambiente de tecnolo-
gia da informação (TI) por meio, principalmente, de fatores que envolvem 
a comunicação, de modo que vem sendo amplamente utilizada pelas 
empresas de tecnologia que desejam otimizar o seu ambiente. Utilizando 
tecnologias habilitadoras, como é o caso da computação em nuvem, 
os grupos de desenvolvimento e operações buscam beneficiar-se de 
novos paradigmas que forneçam melhores ferramentas e ambientes, 
a fim de tornar as suas tarefas mais econômicas e ágeis. 

Neste capítulo, você conhecerá os principais fatores que podem 
manter a equipe de desenvolvimento alinhada e concisa durante o seu 
trabalho. Além disso, conhecerá a importância que a cultura de DevOps 
possui. Por fim, verá como a comunicação torna a integração de DevOps 
possível. 



236 ) ( Comunicação e colaboração na cultura DevOps 
1 Mantendo a equipe alinhada durante 

o processo 
Com o surgimento de desafios tecnológicos contínuos e a crescente necessi-
dade de melhoria dos produtos, torna-se imperativo que a agilidade faça parte 
desse processo. Dessa forma, as empresas que buscam se destacar no ramo 
tecnológico utilizam um conjunto de práticas e ferramentas para acelerar a 
capacidade de organização, a fim de as metas sejam atingidas nos tempos 
planejados. Essas práticas resumem o significado de uma cultura DevOps. 

De acordo com Sharma e Coyne (2015), a cultura DevOps baseia-se em um 

processo contínuo de integração, implantação e feedback. Ou seja, ao contrário 
do modelo tradicional utilizado pelas empresas, conhecido como modelo 
cascata, no qual os clientes apenas têm o conhecimento do projeto em si na 
parte de finalização, o modelo DevOps trabalha na forma como as equipes de 
desenvolvimento e operações de TI interagirão, tornando o processo mais ágil 
e cooperativo entre as equipes, de forma que possíveis correções, ou mesmo 
melhorias, possam ser feitas em qualquer etapa, reduzindo significativamente 
os custos e melhorando a eficiência dos grupos. 

Para saber como alinhar operação e desenvolvimento com o intuito de aplicar uma 
cultura inovadora ao projeto de software, acesse o site DevMedia. 

O DevOps não possui uma estrutura organizacional ideal. Como tudo em 
tecnologia, a resposta certa relativa à estrutura da empresa depende de sua 
situação: equipe atual, planos de crescimento, tamanho da equipe, conjunto 
de habilidades disponíveis, produto, e assim por diante. Alinhar a visão da 
equipe de DevOps deve ser o primeiro objetivo. Somente depois de remover o 
atrito entre as pessoas é que as equipes devem ser organizadas. Ainda assim, 
permitir alguma flexibilidade é importante. Além disso, para realizar essa 
organização, pode-se escolher uma abordagem e permitir que ela evolua a partir 
daí, de modo que ela não necessariamente precisa ser permanente ou imóvel. 



Comunicação e colaboração na cultura DevOps ) (237 

A Figura 1, a seguir, apresenta as diferenças entre os modelos cascata, 
ágil e DevOps. No modelo cascata, os requisitos do software são claros e 
definidos com antecedência. Nesse sentido, é possível projetar, desenvolver, 
testar e implantar sem muitas interações e modificações. Já o modelo ágil 
pode ser utilizado quando muitas mudanças nos requisitos acontecem, de modo 
que é possível adicionar recursos com facilidade e corrigir erros encontrados. 
O ponto fraco dessa abordagem é que só os desenvolvedores reagem com 
rapidez, e a equipe operacional, não. Nesse ponto, surge o DevOps, no qual 
tanto os desenvolvedores quanto a equipe operacional projetam, desenvolvem, 
testam e implantam com rapidez. 

CASCATA 

ES TT ES ES 
ÁGL 

DEVOPS 

Figura 1. Diferenças entre os modelos cascata, ágil e DevOps. 
Fonte: Adaptada de Scott (2018). 

O DevOps se concentra na iteração rápida e na melhoria contínua, e esse é 

o principal benefício dessa metodologia. A seguir, são descritos os principais 
fatores que se destacam para uma melhor integração da equipe: 
= Constante parceria e comunicação: um dos principais fatores que 
impactam no alinhamento da equipe é a parceria/interação, assim como 
a comunicação. A comunicação reforça a ideia de parceria e concede 
aos membros da equipe uma melhor aproximação, principalmente em 
discussões internas, que, por sua vez, auxiliam na agilidade e execução 



238 ) (c omunicação e colaboração na cultura DevOps 
do projeto em si. Além disso, a comunicação, quando realizada dire-
tamente com o cliente, revela necessidades específicas e problemas, 
que, tendo sido descobertos pela equipe, podem ser atendidos com 
maior eficiência. Dessa forma, o produto desenvolvido pode atender e 
satisfazer a todos os requisitos do cliente. 

m Colaboração entre equipes: as equipes de desenvolvimento e operações 
normalmente utilizam ferramentas diferentes para gerenciar os seus 
projetos. Para colaborar melhor entre os limites organizacionais, essas 
equipes devem começar a utilizar as mesmas ferramentas de gerencia-
mento de alterações e gerenciamento de itens de trabalho, ou mesmo 
ferramentas integradas. Isso permite uma maior visibilidade contínua 
das ferramentas e rastreabilidade entre as respectivas solicitações de 
mudança. A colaboração em tempo real utilizando-se uma ferramenta 
comum é obviamente o melhor cenário. 

m Objetivos pontuais: a definição desses objetivos é de fundamental 
importância, uma vez que permite que a equipe esteja alinhada para 
atingi-los. Por exemplo, imagine que cada colaborador tenha entendido o 
objetivo de uma forma: como seria a cooperação entre eles e, principal-
mente, como seria o resultado? Dessa forma, por meio da comunicação 
com os gestores, as equipes precisam ter os objetivos bem descritos e 
definidos, para que os trabalhos possam ser somados e o produto possa 
ser desenvolvido em menor tempo e com maior qualidade. 

m Entrega e testes contínuos: quando um sofiware é desenvolvido, 
a entrega em pequenas partes e de forma constante ajuda a integrar 
melhor a equipe. Isso acontece porque, quando esse processo é realizado, 
a cada entrega, a equipe envolve-se na testagem da parte entregue, 
de modo que pode refletir se esse software soluciona/atende às expec-
tativas desejadas e opinar sobre os próximos passos do produto. Além 
disso, essas práticas tendem a otimizar o tempo, reduzir riscos de falhas 
e reforçar a comunicação e a cooperação. 

E Padronização: essa etapa é fundamental para o prosseguimento do 
projeto, assim como dos objetivos e metas. Quando um padrão é de-
finido, os colaboradores seguem normas, que normalmente buscam 
integrar e alinhar o trabalho das equipes. Além disso, a padronização 
evita burocracias, como eventuais conversões, e aumenta o trabalho 
em conjunto. 



Comunicação e colaboração na cultura Devops | ( 239 

Fique atento 

Os princípios citados anteriormente foram criados para promover o alinhamento do 
grupo de desenvolvimento e de operações utilizando uma perspectiva de equipe. 
No entanto, eles não substituem a necessidade de uma boa formação de equipe. Seja 
em encontros presenciais ou nos mundos distribuídos de hoje, por meio de reuniões 
virtuais regulares e colaboração on-line em tempo real, as melhores equipes são aquelas 
em que as pessoas se conhecem, confiam umas nas outras, se cuidam e, quando há 
desafios, sabem com quem conversar. 

2 Importância da cultura DevOps 

A cultura DevOps está diretamente relacionada às pessoas. Uma organização 
pode adotar os processos e as ferramentas automatizadas mais eficientes, mas 
estes se tornarão sem propósito sem as pessoas que executarão esses processos 
e utilizarão essas ferramentas. Uma cultura DevOps é caracterizada por um 
alto nível de colaboração, foco nos negócios, confiança e um alto valor atri-
buído à aprendizagem por meio da experimentação. Criar uma cultura não é 
como adotar um processo ou uma ferramenta, pois requer a engenharia social 
das pessoas que formam as equipes, cada uma com predisposições únicas e 
experiências diversificadas. Essa diversidade pode tornar a construção da 
cultura desafiadora (SHARMA; COY NE, 2015). 

A criação de uma cultura DevOps requer que os líderes da organização 
trabalhem com seus times para criar um ambiente colaborativo e de compar-
tilhamento. Além disso, os líderes são responsáveis por eliminar as barreiras 
que impossibilitam a cooperação. As avaliações comumente utilizadas, como a 
de remuneração de equipes operacionais por tempo de atividade e estabilidade 
dos sistemas, assim como a de remuneração de desenvolvedores utilizando 
como base a entrega de novas ferramentas, não são benéficas para a cultura 
criada, pois colocam os grupos uns contra os outros. 



240 ) ( Comunicação e colaboração na cultura DevOps 
Exemplo 

Por exemplo, o grupo operacional sabe que a melhor proteção para que a produção 
seja mantida é não aceitar modificações no processo (a forma como a produção está 
atualmente funcionando deve ser mantida, até que pare de funcionar, não permitindo 
que eventuais otimizações possam ser feitas). Em contrapartida, o grupo de desenvol-
vimento tem pouco incentivo para focar na qualidade que se deseja. 

Para que o ambiente não sofra com essas imposições, faz-se necessário subs-
tituir a avaliação pela atribuição de responsabilidade compartilhada na entrega 
de novas melhorias, de forma rápida e segura. Os líderes das organizações 
precisam encorajar a colaboração por meio de uma melhoria na visibilidade. 
Assim, estabelecer um conjunto comum de ferramentas de colaboração é 
essencial, principalmente quando os grupos estão geograficamente separados 
e, devido a esse fator, não podem trabalhar juntos fisicamente. Portanto, prover 
toda a visibilidade dos objetivos e o status de um projeto para os stakeholders 
é crucial para a criação de uma cultura DevOps baseada na confiança e na 
colaboração. 

De acordo com Walls (2013), algumas características culturais funda-
mentais são: 

E Comunicação aberta: uma cultura DevOps é criada por meio de muita 
discussão e debate. As equipes técnicas, tradicionalmente isoladas, 
interagem por meio de sistemas complexos, que podem exigir interven-
ção dos líderes. Uma equipe que adota uma abordagem DevOps fala 
sobre o produto ao longo de seu ciclo de vida, discutindo requisitos, 
recursos, agendas e o que mais possa surgir. O foco está no produto, 
e não na construção de grupos isolados com suas próprias opiniões. 
As métricas de produção e construção estão disponíveis para todos e 
são exibidas com destaque. 

E Incentivo e responsabilidade: o incentivo principal de uma equipe 
de DevOps deve ser o de criar um produto incrível para seus clientes, 
qualquer que seja esse produto. O grupo de desenvolvimento não é 
recompensado por escrever muito código, assim como o grupo de 
operações não é punido quando o código não é executado conforme o 
esperado na produção. A equipe é recompensada quando o produto é 



Comunicação e colaboração na cultura Devops | ( 241 

incrível e compartilha o processo de melhoria quando o produto pode 

ser mais incrível. 

m Respeito: todos os membros da equipe devem se respeitar. Na ver-
dade, nem todos precisam gostar um do outro, mas todos precisam 
reconhecer as contribuições dos demais e tratar bem os membros da 
equipe. Discussões respeitosas e ouvir as opiniões de outras pessoas 
são uma experiência de aprendizado para todos. Contudo, nenhum 
membro da equipe deve ter receio de falar por medo de ser demitido. 
Um dos motivos que mais geram atritos nos grupos é justamente não 
saber ouvir os colegas. 

m Confiança: a confiança é um componente importante para alcançar 
uma cultura DevOps. O grupo de operações deve confiar que o grupo 
de desenvolvimento está realizando um procedimento seguindo um 
determinado caminho pelo fato de ser o melhor plano para o sucesso 
do produto. Da mesma forma, o grupo de desenvolvimento deve confiar 
que o grupo de controle de qualidade não existe apenas para sabotar os 
seus sucessos. O gerente de produto confia que o grupo de operações 
fornecerá métricas e feedback objetivos após a próxima implantação. 
Se alguma parte da equipe não confiar em outra parte, as suas ferra-
mentas não serão importantes. 

Além disso, a cultura DevOps trouxe uma série de benefícios para as 
empresas que a adotam, sendo os principais listados a seguir: 

E Integração entre áreas: como o próprio nome sugere, a cultura DevOps 
busca a união de grupos de áreas de TI distintas. Para além disso, ela 
promove uma cooperação entre diversas áreas, enfatizando a comuni-
cação para a otimização dos negócios. 

Bm Simplificação de processos: uma prática explorada pelo DevOps é a 
modularização de software e sua consequente reutilização. Isso permite 
que os projetos sejam executados de forma mais ágil e adaptativa, 
de acordo com cada objetivo/projeto. 

m Automação: a automação também passa a ser uma prática explorada, 
pois, por meio dela, os grupos de trabalho, que antes concentravam 
esforços em desenvolver novas funcionalidades, agora focam-se em 
documentar, examinar erros e suas possíveis soluções, assim como no 
autoaprendizado. Em suma, a automação tem a função de automatizar 
processos que se tornam repetitivos. 



242 ) [ Comunicação e colaboração na cultura DevOps 
m Racionalização: nessa etapa, ocorre a revisão das práticas de TI uti-
lizadas, a fim de torná-las mais eficientes e racionais. 
mB Atualização da TI: utilizando a seu favor novos paradigmas compu-
tacionais, como é o caso da computação em nuvem, o DevOps otimiza 
significativamente diversos aspectos de TI, como, por exemplo, aqui-
sição de infraestrutura, licenças de software e compartilhamento de 
arquivos. Dessa forma, a computação em nuvem torna-se uma tecnologia 
fundamental para a otimização de processos DevOps. 
EB Colaboração: ainda utilizando as nuvens computacionais como exem-
plo, estas buscam estimular a colaboração das equipes, uma vez que 
várias pessoas podem trabalhar em uma etapa do projeto ao mesmo 
tempo. Dessa forma, profissionais de diferentes grupos podem planejar 
uma solução e compartilhá-la com todos os demais envolvidos, poten-
cializando uma análise mais apurada e aumentado, assim, a agilidade. 
E Elasticidade e escalabi! ade: duas características que foram trazi-
das da computação em nuvem e são exploradas pelo DevOps. Com a 
elasticidade e a escalabilidade dos ambientes, a empresa paga apenas 
pelo que usa, tendo um ambiente que se expande ou se retrai conforme 
a necessidade. 

Um caso que pode servir como exemplo de como a cultura DevOps pode melhorar o 
negócio é a história da atual gigante em computação em nuvem, a Amazon. Na época 
em que a Amazon utilizava servidores dedicados, era um desafio constante prever 
quanto equipamento comprar para atender às demandas de tráfego e estimar como 
acomodar perfeitamente os picos de tráfego imprevistos. Como resultado, cerca de 
40% da capacidade dos servidores da Amazon foi desperdiçada. Durante a temporada 
de compras de Natal, quando o tráfego podia triplicar, mais de três quartos podiam 
ser deixados sem uso, junto ao dinheiro gasto para comprá-lo. 

Depois que o varejista on-line passou para a nuvem Amazon Web Services (AWS), 
foi possível que os engenheiros aumentassem ou diminuíssem a capacidade gra-
dualmente. Isso não apenas reduziu os gastos com capacidade do servidor, como 
também promoveu uma transição para um processo de implantação contínua, que 
permite a qualquer desenvolvedor implantar o seu próprio código nos servidores 
de que precisa, sempre que desejar. Após um ano da mudança da Amazon para a 
AWS, os engenheiros estavam implantando código a cada 11,7 segundos, em média. 
Além disso, a abordagem ágil reduziu o número e a duração das interrupções, resultando 
em aumento da receita. 


Comunicação e colaboração na cultura DevOps ) ( 243 

Dessa forma, a utilização de uma cultura de DevOps dentro de uma empresa 
permite realizar diversas otimizações, que tornam o negócio mais eficiente e 
rentável, além de beneficiar também os grupos de TI, que são incentivados a 
melhorar os seus conhecimentos e a compartilhar informações (cooperações) 
para soluções de problemas. 

3 Comunicação como pilar estratégico 

Como o principal estilo de negociação entre uma equipe ou local de trabalho, 
grande parte da colaboração se resume à comunicação. Além de simples-
mente responder a uma pergunta ou dizer a alguém no que trabalhar a seguir, 
há muitas razões diferentes pelas quais as pessoas se comunicam. As quatro 
razões principais são: compreensão, influência, reconhecimento e construção 
da comunidade. 

Segundo Davis e Daniels (2016), uma grande parte da comunicação é 
projetada para aumentar a compreensão, o que pode indicar uma compreensão 
mais clara do que alguém espera de nós, ou uma compreensão mais profunda 
de um tópico técnico. Os programas formais de orientação são uma ótima 
maneira de se aumentar a compreensão, mas, mesmo sem eles, há muitas 
oportunidades para aumentar a compreensão das pessoas em seu ambiente 
com sucesso. Uma reunião estabelecida da comunidade de prática, seja na 
forma de palestras regulares, hackathon em equipe ou sessões de correção 
de bugs, é uma ótima oportunidade para mostrar às novas pessoas o conjunto 
de expectativas implícitas no ambiente. Essa é uma forma de entendimento 
implícito, captando ideias, normas e costumes sociais por meio da observação, 
em vez do entendimento explícito resultante de uma sessão de orientação ou 
de uma palestra formal. 

Adotar uma cultura de aprendizagem e incentivar o engajamento social 
em torno do compartilhamento de conhecimento fornece pistas contextuais 
apropriadas para o entendimento que geralmente não está presente por meio da 
autoaprendizagem de um assunto. Em vez de examinar os ambientes e tentar 
descrever tudo em listas de verificação ou outros documentos, faz-se neces-
sário reconhecer a importância da construção da comunidade nos ambientes. 

Quando um único colaborador se torna responsável por grandes quantidades 
de sistemas e processos, ele destina uma grande quantidade de conhecimento 
por meio da situação e da aplicação do aprendizado. Sem a disseminação ativa 
desse conhecimento para outras pessoas, criam-se ilhas de conhecimento 
vulneráveis a eventos externos na organização. Além disso, ter apenas algumas 



244) (c omunicação e colaboração na cultura DevOps 
pessoas que entendem um determinado tópico aumenta a pressão sobre esses 
colaboradores, o que pode aumentar o estresse e a probabilidade de esgota-
mento. A comunicação para compartilhar e disseminar o entendimento é uma 
Ótima maneira de se aumentar as habilidades e a robustez da organização. 

Muitas vezes, o entendimento inclui um aspecto da perspectiva histórica. 
Dados os sistemas complexos com os quais se trabalha e a maneira como eles 
crescem e evoluem ao longo do tempo, nem sempre é óbvio para alguém novo 
em uma equipe ou em um projeto por que as coisas são do jeito que são. Esse 
tipo de contexto é muito importante para poder compreender e contribuir 
totalmente para algo. Isso é especialmente verdadeiro para as equipes de 
operações, encarregadas de decidir se algo é atípico ou não: “esse alerta foi 
um alarme falso ou há um problema real que precisa ser investigado?”. Ser 
capaz de comunicar os contextos históricos permite que novos membros da 
equipe cresçam e desenvolvam os seus conhecimentos e entendimentos muito 
mais rapidamente. 

A comunicação também pode ser projetada para influenciar as pessoas. 
O exemplo mais comum disso, em um contexto de trabalho, é tentar convencer 
alguém a ficar “do seu lado” ou desistir do seu ponto de vista quando há uma 
discordância sobre como fazer algo. 

Por exemplo, se um colaborador quiser utilizar um repositório de dados NoSQL para 
O próximo projeto da equipe, porém outro indivíduo achar que o MySQL seria mais 
adequado, ambos tentarão influenciar as pessoas ao seu redor. 

Existem diferentes métodos de influência, alguns mais positivos ou colabo-
rativos que os outros. Certamente, pode-se influenciar os outros interrompendo 
quem discorda de si, vendo quem discute mais alto, ou usando algum tipo de 
coerção. No entanto, nada disso funciona bem para uma dinâmica de equipe 
saudável ou empática, pois, embora a influência possa ter sido alcançada, 
é provável que todo mundo se sinta ressentido. A maneira mais eficaz de se 
influenciar os outros é encontrar um terreno comum o suficiente para que 
eles não apenas façam o que se deseja, mas também desejem o que é feito. 



Comunicação e colaboração na cultura Devops ) ( 245 

Dar reconhecimento é outro motivo comum pelo qual as pessoas se comu-
nicam, pois pode melhorar o moral, já que as pessoas, obviamente, querem 
sentir que seu trabalho e suas realizações são notados e apreciados. Isso pode 
aumentar a cooperação entre os funcionários, pois eles se veem com maior 
contribuição e mais generosos, além de ajudar a reforçar comportamentos 
desejados no trabalho. O reconhecimento geralmente tem duas partes: a iden-
tificação ou realização de algo que deve ser reconhecido e a comunicação real 
desse sentimento. 

A identificação de oportunidades de reconhecimento é uma habilidade que 
leva tempo, pois, se um colaborador não estiver na mentalidade certa (p. ex., 
estar com humor negativo, estressado devido a uma carga de trabalho pesada 
ou estar em um ambiente de equipe incrivelmente competitivo e individualista), 
será mais difícil perceber momentos em que elogios ou reconhecimento seriam 
apropriados. Comunicar o reconhecimento é outra habilidade. Algumas pessoas 
se sentem menos à vontade para elogiar outras, principalmente se não houver 
muito reconhecimento no local de trabalho anteriormente. 

Por fim, a comunicação pode ser utilizada para construir comunidades. 
As equipes com maior empatia e comunicação mais igualitária são mais 
criativas e produtivas, e a construção da comunidade anda de mãos dadas com 
esses fatores. As equipes nas quais as pessoas falam regularmente sobre coisas 
além de assuntos estritamente relacionados ao trabalho têm níveis mais altos 
de confiança e empatia, são capazes de ser mais produtivas e lidam melhor 
com os momentos estressantes como um grupo. As pessoas geralmente intera-
gem melhor no âmbito individual quando conseguem se ver como indivíduos 
completos, e não apenas como endereços de e-mail ou entradas no diretório 
de funcionários da empresa. 

No entanto, não deve haver expectativa de que os funcionários se tornem 
melhores amigos fora do trabalho, e há uma linha tênue entre conhecer alguém 
como pessoa e se tornar muito pessoal. Algumas pessoas estão mais dispostas 
a compartilhar partes pessoais de suas vidas do que outras, e não existe pro-
blema nisso. A chave não é forçar a construção de comunicação interpessoal 
na comunidade, mas sim criar oportunidades para isso, incentivar e permitir 
que isso aconteça naturalmente. Construir relacionamentos e comunidades 
leva tempo, pois isso não acontece da noite para o dia e não pode ser forçado. 
Algo como coffee breaks, almoços compartilhados por tempo suficiente para 
comer e conversar e atividades de inclusão para pessoas com interesses em 
comum podem contribuir bastante para a construção de comunidades fortes. 



246) ( Comunicação e colaboração na cultura DevOps 
Para ter a melhor colaboração/participação, faz-se extremamente necessário 
que ambas as equipes tenham um entendimento comum das tarefas, dos fluxos 
de trabalho, etc., maximizando sistematicamente a capacidade e a produtividade 
das equipes. A comunicação é a chave para isso, de modo que acertar a cultura 
da comunicação é crucial para um ambiente de DevOps. Além disso, deve-se 
estabelecer um protocolo de comunicação claro e preciso, pois essa é uma das 
melhores maneiras de capacitar as equipes de desenvolvimento e operações. 
Caso esses pontos não estejam devidamente definidos, os funcionários, bem 
como os gestores, podem acabar prejudicando todo o grupo, modificando a 
forma como a organização é dirigida. 

Criando uma comunicação sólida dentro 
da cultura DevOps 

Uma grande mudança de paradigma ocorreu devido ao surgimento da cultura 
DevOps, a qual levou à adoção de diversos procedimentos de comunicação 
pelas equipes que trabalham em colaboração. Uma prática que se consolidou 
foi a adoção de ferramentas de comunicação digital, como, por exemplo, 
as ferramentas de mensagens instantâneas e bate-papo, que conquistaram 
grande parte do mercado. No celular, é muito fácil ter acesso a aplicações, 
mantendo as equipes de trabalho em constante comunicação entre si. 

Exemplo 

O Slack é um dos aplicativos de mensagens mais populares utilizados pelas orga-
nizações. Ele aumenta a transparência, com trocas frequentes de dados entre os 
membros das equipes de DevOps. Além disso, o Slack é considerado uma das melhores 
ferramentas, devido à sua estrutura, que se molda facilmente para a criação de grupos. 

Nos últimos anos, as videoconferências e outros meios de comunicação 
digital têm sido um importante canal para facilitar as reuniões dos membros 
da equipe que trabalham localmente ou em todo o mundo. Os software de 
videochamada e de bate-papo em grupo ajudaram a obter sessões on-line mais 
interativas, bem como possibilitaram o compartilhamento fácil de documentos 
e muitos outros benefícios. A melhor parte é que isso pode ser feito utilizando-
-se diversos tipos de dispositivos. 



Comunicação e colaboração na cultura Dev 00s ) (2a7 

Semelhante à estrutura ágil, as equipes em uma cultura DevOps precisam 
realizar reuniões com frequência para oferecer as melhores soluções para os 
clientes. Essas reuniões regulares são consideradas a espinha dorsal da comu-
nicação do DevOps. De preferência, as reuniões devem ocorrer diariamente. 
A reunião de segunda-feira ou da manhã, por exemplo, pode ser utilizada para 
atribuir tarefas a cada membro da equipe, ao passo que a reunião de sexta-feira 
ou do final do dia pode ser utilizada para avaliar o trabalho concluído e para 

o planejamento futuro. 
Uma tarefa tão importante quanto as reuniões regulares é a organização 
destas. Os gerentes de cronograma precisam deixar tempo de trabalho suficiente 
para que cada membro da equipe possa, efetivamente, trabalhar em suas tare-
fas. Muitas interrupções podem quebrar o processo de trabalho, dificultando 

o foco dos funcionários. Assim, o gerenciamento eficaz do tempo depende 
das necessidades individuais e da equipe, bem como da flexibilidade para se 
trabalhar da maneira mais eficiente possível. 
Outra forma de melhorar a comunicação é aprimorar as habilidades sociais, 
pois estas permitem que os membros da equipe colaborem de maneira mais 
eficaz e desenvolvam fortes relações de trabalho. Desse modo, a primeira tarefa 
de todo gerente é encontrar as habilidades sociais presentes em sua equipe. 
Isso dará ao grupo a chance de encontrar as melhores oportunidades para um 
maior desenvolvimento. Após a avaliação inicial das habilidades, o gerente 
pode inscrever os membros em cursos para comunicação assertiva, negociação 
e solução de conflitos, de acordo com suas necessidades e capacidades atuais. 

Por fim, outra estratégia que pode ser utilizada para beneficiar a comu-
nicação é a utilização de dados de projetos anteriores ou atuais. A constante 
adaptação e as mudanças nos processos de trabalho são a base para a imple-
mentação bem-sucedida do DevOps. Esse ideal só pode ser alcançado por 
meio da comunicação eficaz entre os membros da equipe. A comunicação 
exige tempo, portanto, a única maneira de se garantir um trabalho eficaz do 
DevOps é combinar habilidades estratégicas, desenvolvimento de processos, 
excelente gerenciamento de tempo e informações inteligentes sobre dados. 

Como visto, comunicação, colaboração e integração são os três princípios 
da cultura DevOps. Para adotar uma cultura bem-sucedida, é muito impor-
tante trazer melhorias nos feedbacks e nas comunicações entre as equipes e 
as partes interessadas. Ter um terreno comum no entendimento e no uso de 
um protocolo de comunicação permitirá que os funcionários façam uso dos 
meios de comunicação disponíveis. Além disso, melhorar a comunicação 
com o gerenciamento legítimo do tempo maximizará a produtividade da 
organização como um todo. 



Referências 

Fique atento 

J 


Cloud computing e DevOps 

Objetivos de aprendizagem 

Ao final deste texto, você deve apresentar os seguintes aprendizados: 

E Descrever os conceitos básicos relativos a cloud computing. 
E Identificar soluções de DevOps na nuvem. 
E Comparar as soluções de DevOps disponíveis nos provedores de 
cloud computing. 

Introdução 

A cloud computing (computação em nuvem) é uma tecnologia habilitadora 
das metodologias de DevOps. Por meio dela, são fornecidas as principais 
ferramentas para o desenvolvimento de software e colaboração das 
equipes de tecnologia da informação (TI), bem como para o provisiona-
mento de recursos computacionais. Além disso, as práticas de DevOps se 
beneficiam das principais características da nuvem, como escalabilidade, 
acesso por meio da internet e pagamento sob demanda. 

Neste capítulo, você conhecerá o conceito de cloud computing, bem 
como suas principais características, modelos de implantação e modelos 
de serviço. Além disso, conhecerá as soluções de DevOps dos provedores 
de nuvem pública AWS (Amazon Web Services) e Azure. Por fim, conhe-
cerá a relação entre as funções que esses serviços possuem. 

Cloud computing 

A cloud computing (computação em nuvem) é um paradigma recente e con-
siderado disruptivo dentro do seu propósito. Ela foi desenvolvida por meio da 
combinação e evolução da computação distribuída (distributed computing) 
e da virtualização, com fortes contribuições da computação em grade (grid 
computing) e da computação paralela (parallel computing) (BUYYA et al., 



250 ) (ciouacomputia e Devops 
2009). Nesse paradigma, os dois principais atores são o usuário e o provedor. 
O usuário é definido como consumidor e pode ser uma pessoa ou uma or-
ganização inteira. Já o provedor é uma organização que fornece os serviços 
ao usuário. 

De acordo com a definição do National Institute of Standards and Techno-
logy (NIST), a cloud computing é um modelo que fornece acesso sob demanda 
a um conjunto compartilhado de recursos configuráveis de computação, que 
podem ser provisionados e liberados rapidamente sem grandes esforços de 
gerenciamento e interação com o provedor de serviços (MELL; GRANCE, 
2011). 

Saiba mais 

O NIST é uma agência governamental não reguladora que desenvolve tecnologia, 
métricas e padrões para impulsionar a inovação e a competitividade econômica em 
organizações dos Estados Unidos no setor de ciência e tecnologia. Como parte desse 
esforço, o NIST produz padrões e diretrizes para ajudar as agências federais a atender 
aos requisitos da Lei Federal de Gerenciamento de Segurança da Informação (FISMA). 

Essa definição é constituída por cinco características essenciais, três mode-
los de serviço e quatro modelos de implementação. O serviço de computação 
em nuvem apresenta as seguintes características: 

E Atendimento sob demanda: é possível alocar e desalocar recursos com-
putacionais sem a necessidade de interação com a equipe do fornecedor. 

E Amplo acesso à rede: todos os serviços oferecidos pelo provedor devem 
estar acessíveis em uma rede LAN (/ocal area network; ou rede de área 
local, em português) ou pela Internet, e o acesso precisa estar disponível 
por meio de mecanismos padrões, como navegadores. 

E Pool de recursos: o provedor possui e gerencia um conjunto de recursos 
físicos de computação e atende a vários usuários de acordo com a sua 
demanda. 



Cloud computing e Devops ) ( 251 

m Rápida elasticidade: o provedor permite ao usuário alocar e desalocar 
recursos em qualquer quantidade, a qualquer momento. Para o usuário, 
Os recursos disponíveis parecem não ter limites. 

Bm Serviço medido: o provedor é responsável por controlar o uso e a 
disponibilidade dos recursos, uma vez que essas informações precisam 
ser disponibilizadas ao usuário para garantir o SLA (service level 
agreement; ou contrato de nível de serviço, em português). Além disso, 
ele é utilizado para fins de cobrança. 

Em suma, a cloud computing precisa ser acessível pela internet e deve for-
necer elasticidade dos recursos em um modelo de cobrança por uso. Os serviços 
fornecidos por um provedor de nuvem são categorizados em três modelos de 
serviço: infraestrutura como serviço (laaS), plataforma como serviço (PaaS) 
e software como serviço (SaaS). Esses serviços são apresentados na Figura 1, 
em que as respectivas camadas dos serviços possuem suas responsabilidades 
divididas entre o gerenciamento feito pelo cliente que contratou o serviço e 
pelo provedor de nuvem. 

A o 
Infraestrutura Plataforma Software 
(como serviço) (como serviço) (como serviço) 

g s—

Ê 2 J / 
E 2

mens seno |,

É) é g é

E RM NEN RR 

3 Ê 8

) so (É DE >;

L 1. 7ê 

E

Virtualização 3 Virtualização 8 Virtualização í 
ê $

Hardnare ã Hardnare ê Hardnare 8 

= a] 
Armazenamento $ Armazenamento Armazenamento 

Rede ê Rede Rede

8 ) ) 
Figura 1. Modelos de serviços da nuvem e suas divisões de responsabilidade entre cliente 
e provedor da nuvem. 
Fonte: Adaptada de IBGP (2017) 


252 ) ( Cloud computing e DevOps 
A infraestrutura como modelo de serviço (IaaS) fornece acesso a recursos 
de TI virtualizados para computação, armazenamento e rede. Em geral, esses 
recursos são máquinas virtuais (VM, virtual machine), também chamadas de 
instâncias, armazenamento e configurações de rede. O usuário desse serviço 
é um administrador do sistema, o qual interage com o provedor para alocar, 
configurar e desalocar os recursos, os quais podem ser disponibilizados para o 
usuário final. O provedor é responsável por manter os serviços de baixo nível, 
como máquinas físicas, rede e virtualizador, além de manter um conjunto 
atualizado de imagens do sistema operacional e definir as características das 
instâncias de VMs disponíveis e a alocação de recursos no hardware físico. 
O fornecedor, por sua vez, precisa manter a correção de todo o ambiente básico. 
O usuário precisa seguir as opções de configuração definidas, como usar 
uma das imagens de sistemas operacionais disponíveis ou criar a sua própria 
imagem, de acordo com os procedimentos do provedor. Quando o usuário tem 
uma VM em funcionamento, é sua responsabilidade administrá-la. O principal 
benefício da laaS é que o usuário tem total controle sobre os recursos, pois 
possui acesso administrativo às VMs. 

A plataforma como modelo de serviço (PaaS) fornece um Kit de fer-
ramentas completo que permite ao usuário desenvolver e gerenciar as suas 
aplicações. O utilizador desse modelo de serviço normalmente é um usuário 
técnico, como administradores de aplicações, desenvolvedores e testadores. 
O utilizador também pode ser um usuário final de uma aplicação, porém, 
do ponto de vista do usuário, o modelo aplicável, nesse caso, é o SaaS. 
O provedor define e suporta um conjunto de ferramentas de desenvolvimento. 
Essa definição abrange as linguagens de programação suportadas e suas 
respectivas interfaces de programação de aplicações (API, application pro-
gramming interface). Todos os recursos físicos são controlados pelo provedor, 
e é responsabilidade dele manter toda a infraestrutura do serviço. Além disso, 
ele é responsável por manter atualizados todos os componentes da plataforma, 
aplicando patches de segurança ou novas versões de determinadas bibliotecas, 
por exemplo. 

Por fim, o software como serviço (SaaS) fornece ao usuário a capacidade de 
usar uma aplicação, visto que ele tem acesso a um software implantado como 
um serviço da web pela internet. O utilizador do SaaS pode ser uma pessoa que 
deseja ter acesso a determinado software ou organização que fornece acesso 
aos seus membros. As principais responsabilidades do fornecedor são garantir 
que o software fornecido seja suportado e testado. Além disso, o provedor é 
responsável por garantir a confidencialidade dos dados, de modo que os dados 
de determinado usuário não possam ser acessados por outros. O controle de 



Cloud computing e Devops | ( 253 

segurança do acesso do usuário, os sites e procedimentos de recuperação de 
desastres e a verificação de conformidade também são de responsabilidade 
do fornecedor, bem como problemas com o sistema operacional, dispositivos 
de hardware e opções de configuração. 

A definição de nuvem do NIST lista quatro modelos de implementação: 
privado, comunitário, público e híbrido. Cada um desses modelos possui 
particularidades em relação a aspectos como dependência de rede, segurança, 
quantidade de recursos, entre outros. No modelo público, o usuário aluga os 
recursos de um provedor. Já os modelos privado e comunitário podem ser 
utilizados em duas configurações: terceirizadas, em que o usuário aluga 
recursos exclusivos de um provedor; e no local, em que os recursos pertencem 
ao usuário. Por fim, o modelo híbrido é uma combinação entre qualquer um 
dos outros três modelos. Os recursos são controlados por uma plataforma em 
nuvem que representa uma abstração de todos os componentes que formam 
uma arquitetura de provedor em nuvem. 

Uma nuvem privada é um modelo em que os recursos físicos são con-
trolados pelo usuário ou por uma organização. Os cenários mais comuns de 
uso desse modelo são quando uma organização decide virtualizar seu data 
center (configuração local) ou quando deseja alugar todos os recursos de 
um provedor (cenário terceirizado); em ambos os cenários, o usuário tem 
exclusividade sobre os recursos físicos. Já uma nuvem comunitária pode 
ser considerada como uma derivação de uma nuvem privada. As principais 
preocupações relacionadas a ambas as nuvens são as mesmas, como políticas 
de segurança e gerenciamento de recursos. A diferença entre os dois modelos é 
que os recursos da nuvem comunitária são compartilhados. Em outras palavras, 
os usuários de uma nuvem comunitária são um grupo de organizações, 
conhecido como membros, os quais geralmente compartilham interesses. 
Esses membros podem ser empresas diferentes pertencentes ao mesmo grupo 
econômico ou departamentos diferentes de uma instituição pública. 

Nas nuvens públicas, os recursos sempre pertencem a um provedor e 
são compartilhados com os seus vários usuários. O provedor define as suas 
políticas e preços, e os usuários precisam concordar com eles para ter acesso 
aos recursos. O usuário desse modelo pode ser uma organização inteira ou 
apenas uma pessoa, uma vez que é possível alocar apenas uma máquina 
pequena e barata. Em teoria, uma nuvem híbrida é um modelo que combina 
dois ou mais modelos privados, públicos e comunitários. No entanto, esses 
resultados apresentam uma complexidade significativa na configuração, pois 
podem mudar à medida que os fornecedores entram e saem da nuvem. 



254 ) (c loud computing e DevOps 
2 Soluções DevOps na nuvem 

Segundo Freeman (2019), o DevOps e as nuvens computacionais possuem 
metodologias complementares e oferecem facilidades um para o outro. 
À medida que as organizações adotam a nuvem, a proposta em relação à parte 
financeira da nuvem para hospedar/migrar uma carga de trabalho do DevOps 
se torna evidente. A flexibilidade, a resiliência, a agilidade e os serviços ofe-
recidos por uma plataforma em nuvem permitem otimizar a linha de entrega 
de aplicações hospedadas na nuvem. Os ambientes, desde o desenvolvimento 
até os testes e a produção, podem ser provisionados e configurados conforme 
necessário. Esse processo minimiza os gargalos relacionados com o ambiente 
na etapa de entrega. 

As organizações buscam utilizar plataformas em nuvem para reduzir o custo 
de ambientes de desenvolvimento e teste ou para fornecer uma experiência 
moderna e simplificada para os desenvolvedores. Isso cria um ambiente de 
negócios extremamente atraente para a adoção da nuvem com e para DevOps. 
A seguir, serão descritos os serviços de DevOps na nuvem oferecidos por 
dois provedores renomados de nuvens públicas: Amazon e Microsoft Azure. 

Amazon: serviços de DevOps oferecidos 

O provedor de nuvem pública Amazon disponibiliza um grande conjunto de 
serviços que ajudam no estabelecimento do DevOps para empresas. Essas 
ferramentas buscam realizar automatizações em trabalhos manuais e geren-
ciamento de ambientes complexos, com pessoas especializadas no controle das 
operações (AMAZON WEB SERVICES, c2020). As soluções oferecidas são 
classificadas em: integração e entrega contínua, microsserviços, infraestrutura 
como código, monitoramento e registro em /og, plataforma como serviço e 
controle de versões. Os principais serviços oferecidos pela integração e entrega 
contínua são listados a seguir: 

E AWS CodePipeline: serviço de integração e entrega contínua para 
atualizações rápidas e confiáveis de aplicações e infraestruturas. 
O CodePipeline cria, testa e implanta o código sempre que ocorrer uma 
alteração de código, de acordo com os modelos de processo de lança-
mento definidos. Isso permite disponibilizar recursos e atualizações 
de forma rápida e confiável. 



Cloud computing e DevOps )[ 255 

m AWS CodeBuild: serviço totalmente gerenciado que compila o código-
-fonte, roda testes e produz pacotes de software prontos para implanta-
ção. Com o AWS CodeBuild, não é necessário provisionar, gerenciar e 
escalar os próprios servidores. Além disso, ele escala continuamente e 
processa múltiplas compilações ao mesmo tempo, o que evita que elas 
fiquem esperando em uma fila. 
m AWS CodeDeploy: automatiza as implantações de código para qualquer 
instância, inclusive instâncias do Amazon EC2 e servidores locais. 
O AWS CodeDeploy facilita o provisionamento rápido de novos recursos, 
ajuda a evitar tempo de inatividade durante a implantação de aplicações 
e lida com a complexidade de atualizá-las. 
m AWS CodeStar: permite desenvolver, criar e implantar rapidamente 
aplicações na AWS. O AWS CodeStar disponibiliza uma interface de 
usuário unificada, permitindo que se gerencie facilmente atividades 
de desenvolvimento de software em um só lugar. Com ele, é possível 
configurar toda a cadeia de ferramentas de entrega contínua em alguns 
minutos, possibilitando agilizar o lançamento de código. 

Os microsserviços apresentam as seguintes soluções: 

m Amazon Elastic Container Service: serviço de gerenciamento de 
contêineres altamente escalável e de alto desempenho com suporte 
a contêineres do Docker. Esse serviço permite executar facilmente 
aplicações em um cluster gerenciado de instâncias do Amazon EC2. 

m AWS Lambda: permite executar códigos sem provisionar ou gerenciar 
servidores. Com o Lambda, pode-se executar o código para praticamente 
qualquer tipo de aplicação ou serviço de back-end, tudo sem precisar 
de administração. Após carregar o código, o Lambda toma conta de 
tudo o que for necessário para executar e alterar a escala do código 
com alta disponibilidade. 

Na categoria de infraestrutura como código, destacam-se os seguintes 
serviços: 

m AWS CloudFormation: oferece aos desenvolvedores e administradores 
de sistemas uma maneira fácil de criar e gerenciar um grupo de recursos 
relacionados com a AWS, provisionando e atualizando-os de forma 
organizada e previsível. É possível utilizar os exemplos de modelos do 
AWS CloudFormation ou criar modelos próprios. 



256 ) ( Cloud computing e DevOps 
m AWS OpsWorks: serviço de gerenciamento de configurações que 
usa o Chef, uma plataforma de automação que trata configurações de 
servidor como código. O OpsWorks usa o Chef para automatizar a forma 
como os servidores são configurados, implantados e gerenciados em 
instâncias do Amazon Elastic Compute Cloud (Amazon EC2) ou em 
ambientes de computação locais. O OpsWorks oferece dois serviços: 

o AWS OpsWorks for Chef Automate e o AWS OpsWorks Stacks. 
m AWS Systems Manager: serviço de gerenciamento que ajuda a coletar 
o inventário de software, aplicar patches em sistemas operacionais, criar 
imagens de sistemas e configurar os sistemas operacionais Windows e 
Linux, tudo isso de forma automática. Esses recursos ajudam a definir 
e rastrear configurações do sistema, evitar desvios e manter a confor-
midade do software em configurações do EC2 e locais. 
m AWS Config: serviço totalmente gerenciado que oferece inventário 
de recursos, histórico de configuração e notificações de alteração de 
configuração da AWS para proporcionar segurança e governança. 
O Config Rules permite criar regras que verificam automaticamente a 
configuração de recursos da AWS gravada pelo AWS Config. 

Os serviços oferecidos pela categoria de monitoramento e registro em /og 
são os seguintes: 

m Amazon CloudWatch: serviço de monitoramento para recursos em 
nuvem da AWS e as aplicações executadas nela. É possível utilizar o 
Amazon CloudWatch para coletar e rastrear métricas, coletar e monitorar 
arquivos de /og, definir alarmes e reagir automaticamente a alterações 
nos recursos da AWS. 

m AWS X-Ray: ajuda os desenvolvedores a analisar e depurar aplicações 
distribuídas de produção, como as criadas utilizando uma arquitetura 
de microsserviços. Com o X-Ray, é possível entender o desempenho da 
aplicação e dos seus serviços subjacentes para identificar e solucionar 
a causa raiz de problemas e erros de desempenho. 

m AWS CloudTrail: serviço da web que registra as chamadas de APIs 
da AWS para a conta e envia os arquivos de Jog para o cliente do ser-
viço. As informações registradas incluem a identidade do chamador 
da API, a hora da chamada da API, o endereço IP (Internet Protocol; 
ou Protocolo de Internet, em português) de origem do chamador da 
API, os parâmetros da solicitação e os elementos de resposta retornados 
pelo serviço da AWS. 



Cloud computing e DevOps ) (257 

A categoria de plataforma como um serviço oferece o seguinte serviço: 

m AWS Elastic Beanstalk: serviço de fácil utilização para implantação e 
escalabilidade de aplicações e serviços da web desenvolvidos com Java, 
NET, PHP, Node,js, Python, Ruby, Go e Docker em servidores familia-
res, como Apache, Nginx, Passenger e IIS. Ao fazer o upload do código, 

o Elastic Beanstalk se encarrega automaticamente da implementação, 
desde o provisionamento de capacidade, o balanceamento de carga e a 
escalabilidade automática até o monitoramento da saúde da aplicação. 
Ao mesmo tempo, ele mantém total controle sobre os recursos da AWS 
que sustentam uma aplicação e pode acessar os recursos subjacentes 
a qualquer momento. 
Por fim, na última categoria, denominada controle de versões, a AWS 
oferece o seguinte serviço: 

m AWS CodeCommit: serviço de controle de fonte totalmente gerenciado 
que permite que as empresas hospedem com facilidade repositórios Git 
privados, seguros e altamente escaláveis. É possível utilizar o CodeCom-
mit para armazenar com segurança qualquer coisa, desde código-fonte 
até arquivos binários. Além disso, ele funciona perfeitamente com 
outras ferramentas Git atuais. 

Saiba mais 

Chef é uma ferramenta pioneira de DevOps que trouxe o conceito de infraestrutura 
como código para empresas em todo o mundo. O software Chef está em uso em alguns 
dos ambientes de TI mais exigentes, visando a ajudaras organizações a fornecerem 
infraestrutura e aplicações rapidamente, mantendo a segurança. O Chef Desktop aplica 
as lições aprendidas das práticas ágeis e de DevOps ao gerenciamento de recursos 
de TI, permitindo novos níveis de eficiência, escala e gerenciamento de riscos. Em vez 
de apontar e clicar (configuração manual), o estado desejado é definido no código e 
pode ser aplicado de forma confiável e repetida na frota. 


258 ) (ciouacomputma e DevOps 
Microsoft Azure: serviços de DevOps oferecidos 

A Microsoft Azure fornece soluções com as quais as equipes podem implemen-
tar práticas de DevOps durante o planejamento, o desenvolvimento, a entrega 
e as operações de aplicativo (MICROSOFT AZURE, c2020). Os principais 
serviços de DevOps fornecidos por esse provedor são listados a seguir: 

Azure Artifacts: permite criar e compartilhar feeds de pacotes Maven, 

npm e NuGet de fontes públicas e privadas com equipes de qualquer 

tamanho. Além disso, é possível adicionar um gerenciamento de paco-

tes totalmente integrado aos pipelines de integração/entrega contínua 

(CI/CD) com um único clique. 

Azure Pipelines: serviço de nuvem utilizado para criar e testar auto-

maticamente o projeto de código e disponibilizá-lo para outros usuá-

rios. Esse serviço funciona com praticamente qualquer linguagem de 

programação ou tipo de projeto. O Azure Pipelines combina integração 

contínua (CI) e entrega contínua (CD) para testar e criar constante e 

consistentemente o código e enviá-lo para qualquer destino. 

Azure Boards: com o serviço web do Azure Boards, as equipes podem 

gerenciar os seus projetos de software. Ele fornece um conjunto de 

recursos, que incluem suporte nativo para Serum e Kanban, painéis 

personalizáveis e relatórios integrados. Essas ferramentas podem ser 

dimensionadas à medida que a empresa cresce. Além disso, é possível 

começar com rapidez e facilidade o rastreamento de históricos de usu-

ários, itens de lista de pendências, tarefas, recursos e bugs associados 

ao projeto. O trabalho é controlado adicionando-se itens com base nos 

tipos de processo disponíveis para o projeto. 

Azure Repos: conjunto de ferramentas de controle de versão que podem 

ser utilizadas para gerenciar o código. Tanto em um projeto de software 

grande como em um pequeno, utilizar o controle de versão o mais 

rápido possível é uma prática que agiliza e melhora a criação do projeto. 

Azure Monitor: serviço para monitorar os recursos do Azure que 

permite coletar dados granulares de desempenho e utilização, logs 

de atividades e diagnósticos e notificações dos recursos do Azure de 

maneira consistente. 

Azure DevTest: fornece aos desenvolvedores e testadores um ambiente 

de autoatendimento para criar rapidamente ambientes de desenvolvi-

mento e teste, minimizando o desperdício e controlando os custos. 



Cloud computing e DevOps ) [259 

Bm Azure Test Plans: fornece ferramentas para que os membros da equipe 
possam aumentar a qualidade e a colaboração em todo o processo 
de desenvolvimento. A solução de gerenciamento de teste é de fácil 
utilização e fornece todos os recursos necessários para teste manual 
planejado, teste de aceitação do usuário, teste exploratório e coleta de 

feedback das partes interessadas. 

3 Comparando soluções DevOps na nuvem 

Os dois principais provedores de nuvens públicas possuem muitos recursos, com 
ampla cobertura. Dessa forma, muitas organizações optam por utilizar ambos 
os provedores, buscando experimentar os melhores serviços. O Quadro 1, 
a seguir, apresenta uma comparação das ferramentas das nuvens Amazon 
AWS e Microsoft Azure em relação a seus propósitos. Vale ressaltar que estão 
listadas apenas as soluções que possuem funções semelhantes, ou seja, nem 
todos os serviços de determinado provedor podem ser comparados. 

” 
Quadro 1. Comparação dos serviços disponibilizados pelos provedores AWS e Azure 

Serviços AWS Serviços Azure Descrição 

CloudWatch Monitor Solução abrangente para 

X-Ray coletar, analisar e agir na 
telemetria de ambientes 
locais e de nuvem. 

CodeDeploy DevOps (constituído Serviço de nuvem 
CodeCommit pelas soluções: Azure para colaboração no 
CodePipeline Boards, Azure Pipelines, | desenvolvimento de código. 
Azure Repos, Azure Test 
Plans e Azure Artifacts) 

Ferramentas para Ferramentas para Coleção de ferramentas para 
desenvolvedores desenvolvedores compilar, depurar, implantar, 

diagnosticar e gerenciar 

aplicativos e serviços 
escalonáveis multiplataforma. 

(Continua) 



260 ) (ciouacomputia e DevOps 
(Continuação) 

Quadro 1. Comparação dos serviços disponibilizados pelos provedores AWS e Azure 

Serviços AWS Serviços Azure Descrição 

CodeBuild DevOps Serviço de compilação 
totalmente gerenciado 
que oferece suporte à 
implantação/integração 
contínua. 

Interface de linha cu Com base na API REST nativa 

de comando PowerShell em todos os serviços de 
nuvem, diversos wrappers de 
linguagens de programação 
específicas a um idioma 
fornecem maneiras mais 
fáceis de criar soluções. 

OpsWorks Automação Configura e opera aplicações 
de todas as formas e 
tamanhos, fornecendo 
modelos para criar e 
gerenciar uma coleção de 
recursos. 

CloudFormation Gerenciadorde recursos | Oferece uma maneira para 
Extensões de VM os usuários automatizarem 
Automação do Azure as tarefas de TI manuais, de 

longa duração, passíveis 
de erro e frequentemente 
repetidas. 

Fonte: Adaptado de Microsoft (2020). 
a , 

Portanto, percebe-se que tanto a AWS como o Azure possuem uma ampla 
gama de ferramentas que possibilitam a criação de um ambiente de DevOps 
na nuvem. Dessa forma, determinar qual serviço é o ideal para determinado 
negócio torna-se complexo, uma vez que ambos os provedores possuem inú-
meros cases bem-sucedidos e funções atrativas. A melhor forma de buscar 
pelas soluções que melhor se adequem a um negócio é utilizar as ferramentas 
de ambos os provedores para determinada tarefa, pois, na maioria das vezes, 
estes oferecem tempos experimentais para avaliar os seus produtos. Além 
disso, se a utilização de ambos for mantida, as ferramentas desses provedores 
tendem a trabalhar juntas, para que a solução final desejada seja atingida. 



Cloud computing e Devops ) ( 261 

N 
Referências 

AMAZON WEB SERVICES. Desenvolvimento e operações e AWS. [5. |]: AWS, c2020. Dispo-

nível em: https://aws.amazon.com/pt/devops/. Acesso em: 8 jul. 2020. 

BUYYA, R. et al. Cloud computing and emerging IT platforms: vision, hype, and rea-
lity for delivering computing as the Sth utility. Future Gener. Comput. Syst, v. 25, n. 6, 
P. 599-616, 2009. 

FREEMAN, E. DevOps for dummies. Hoboken: John Wiley & Sons, 2019. 

IBGP [Divisão de responsabilidade). Brasília, DF: IBGP, 2017. Disponível em: https://forum. 
ibgp.net.br/wp-content/uploads/2017/05/Divisao-de-Responsabilidades.jpg. Acesso 
em: 27 jul. 2020. 

MELL, P; GRANCE, T. The NIST definition of cloud computing. Gaithersburg: NIST, 2011, 

MICROSOFT. Comparação entre os serviços do AWS e do Azure. [S. |]: Microsoft, 2020. 
Disponível em: https://docs.microsoft.com/pt-br/azure/architecture/aws-professional/ 
services. Acesso em: 8 jul. 2020. 

MICROSOFT AZURE. Produtos do Azure. [5. |): Microsoft, c2020. Disponível em: https:// 
azure.microsoft.com/pt-br/services/fdevops. Acesso em: 8 jul. 2020. 

AU , 

Fique atento 

Os links para sites da web fornecidos neste capítulo foram todos testados, e seu fun-
cionamento foi comprovado no momento da publicação do material. No entanto, a 
rede é extremamente dinâmica; suas páginas estão constantemente mudando de 
local e conteúdo. Assim, os editores declaram não ter qualquer responsabilidade 
sobre qualidade, precisão ou integralidade das informações referidas em tais links. 

J 


Quadro 1. Tipos de ramos egrs estabelecidas no Glow 

Correção de um bug Oqueiráparaa | Umanoa 
produção. | pode produção “| implementação,para

Tam-cítico,quenão | próxima release 
bémconhecido | pela próxima próximapara

esperar a ou 
portronco trunk) | reease uma relesefutura 

hotéix-* release-* -Livre, exceto 
master, develop, 
release-*, 
hotfix-* 

Aéqueestejapronto | Atéqueesteapronto | Infinito Enquantoestiverem 
paraserlberadopara | para

paraserliberado desenvolvimento 
produção produção 

master develop master develop 

(Continua) 

Votar para a página63, 



Quadro 1. Tipos de ramos e regras estabelecidas no Gitfow 

master develope | masteredevelop | feature, 
release develope 
release 

lt us IP) --

Cada fusão gerará | Dexe serfundido primeiro | Deveserfundido -Um amo para cada 
umanova versão e então, | com

| commaster primeiro novaimplementação 
comdevelop raster e então, com leatur 
develop 

Fonte: Adaptado de Driessen (010 

Voltar para a página64. 



(o 
Quadro 1. Exemplo de execução do comando Dockercontaner!s 

blbchaafis | couchbase | "/entrypoint.sh cone.” | shousago | Exted()3houisogo d 
Câbdeds3dado | postges | "docker-entrypoint.s.” | 2housago | UnZhour Sep | po 
STIcalistt | eds "docker-entrypoint.s.” | 4housago | Up4hous Gp | cache 
CSefódsóglba | nglx “ngina-g daemon oÉ." | Zhousago | UpZhous Bco | web 

E, 


Voltar a página 116,

para 


(Oo 
Quadro5, Sida do comando kubect 1 get. pods -o ide após aescala do deploydo Nginkno cluster 

meu-nginx-648497EBfd-2969 [1/1 |Running |O 19 10,32,0,2 kube-03 
meu-nginx-64f49fBfd-6r5gs [1/1 |Running [0 19s 10,40,0.3 kube-02 
meu-nginx-64€497f8fd-bnggá 1/1 |Ruming |O 195 10.32,0.3 knbe-03 
meu-ngink-648497f8fd-htdn9 1/1 [Running |O 19s 10,40,0,1 kube-02 

O, 

Voar para a página167. 



